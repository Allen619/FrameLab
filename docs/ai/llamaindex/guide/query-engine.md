---
title: æŸ¥è¯¢å¼•æ“
description: æŒæ¡ LlamaIndex çš„ Query Engine è°ƒä¼˜æŠ€å·§ï¼Œæå‡æ£€ç´¢ç²¾åº¦å’Œå›ç­”è´¨é‡
---

# æŸ¥è¯¢å¼•æ“

## æ¦‚è¿°

æœ¬ç« å°†å¸®åŠ©ä½ æ·±å…¥ç†è§£ **Query Engineï¼ˆæŸ¥è¯¢å¼•æ“ï¼‰** çš„å·¥ä½œåŸç†å’Œä¼˜åŒ–æŠ€å·§ã€‚å®Œæˆæœ¬ç« åï¼Œä½ å°†èƒ½å¤Ÿï¼š

- ç†è§£ Query Engine çš„æ ¸å¿ƒå‚æ•°
- é…ç½®ä¸åŒçš„å“åº”æ¨¡å¼
- ä½¿ç”¨é‡æ’åºæå‡æ£€ç´¢ç²¾åº¦
- å®ç°æµå¼å“åº”å’Œå¼•ç”¨æ¥æº

## æ ¸å¿ƒæ¦‚å¿µ

### ä»€ä¹ˆæ˜¯ Query Engineï¼Ÿ

[ğŸ”— Query Engine å®˜æ–¹æ–‡æ¡£](https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/){target="_blank" rel="noopener"}

**Query Engine** å°±åƒä¸€ä¸ª**æ™ºèƒ½é—®ç­”åŠ©æ‰‹**ã€‚å½“ä½ æå‡ºé—®é¢˜æ—¶ï¼ŒåŠ©æ‰‹ä¼šï¼š

1. ç†è§£ä½ çš„é—®é¢˜
2. ä»çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç›¸å…³å†…å®¹
3. ç»„ç»‡ä¿¡æ¯å¹¶ç”Ÿæˆå›ç­”

```mermaid
graph TD
    A[ç”¨æˆ·é—®é¢˜] --> B[Query Engine]
    B --> C[Retriever æ£€ç´¢å™¨]
    C --> D[è·å–ç›¸å…³ Nodes]
    D --> E[Response Synthesizer]
    E --> F[ç”Ÿæˆæœ€ç»ˆå›ç­”]
```

**å›¾è¡¨è¯´æ˜**: Query Engine åè°ƒ Retriever å’Œ Response Synthesizerï¼Œå®Œæˆä»é—®é¢˜åˆ°ç­”æ¡ˆçš„å®Œæ•´æµç¨‹ã€‚

### æ ¸å¿ƒç»„ä»¶

| ç»„ä»¶ | ä½œç”¨ | ç±»æ¯” |
|------|------|------|
| Retriever | æ£€ç´¢ç›¸å…³å†…å®¹ | å›¾ä¹¦ç®¡ç†å‘˜æ‰¾ä¹¦ |
| Response Synthesizer | ç”Ÿæˆå›ç­” | ä½œè€…å†™æ€»ç»“ |
| Node Postprocessor | åå¤„ç†æ£€ç´¢ç»“æœ | ç¼–è¾‘ç²¾é€‰å†…å®¹ |

## ä»£ç ç¤ºä¾‹

### ç¤ºä¾‹ 1: åŸºç¡€é…ç½®

> é€‚ç”¨ç‰ˆæœ¬: LlamaIndex 0.10.x+

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

# åŠ è½½æ–‡æ¡£å¹¶åˆ›å»ºç´¢å¼•
documents = SimpleDirectoryReader("./data/").load_data()
index = VectorStoreIndex.from_documents(documents)

# åˆ›å»ºæŸ¥è¯¢å¼•æ“ï¼ˆä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
query_engine = index.as_query_engine()

# æ‰§è¡ŒæŸ¥è¯¢
response = query_engine.query("è¿™ä»½æ–‡æ¡£çš„ä¸»è¦è§‚ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ")
print(response)
```

**è¯´æ˜**: é»˜è®¤é…ç½®é€‚ç”¨äºå¤§å¤šæ•°åœºæ™¯ï¼Œä½†äº†è§£å¯è°ƒå‚æ•°èƒ½å¸®åŠ©ä½ ä¼˜åŒ–æ•ˆæœã€‚

### ç¤ºä¾‹ 2: é…ç½®æ£€ç´¢å‚æ•°

> é€‚ç”¨ç‰ˆæœ¬: LlamaIndex 0.10.x+

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

documents = SimpleDirectoryReader("./data/").load_data()
index = VectorStoreIndex.from_documents(documents)

# é…ç½®æ£€ç´¢å‚æ•°
query_engine = index.as_query_engine(
    similarity_top_k=5,           # æ£€ç´¢æœ€ç›¸å…³çš„ 5 ä¸ª chunks
    response_mode="compact",      # å“åº”æ¨¡å¼
    verbose=True,                 # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
)

response = query_engine.query("æ–‡æ¡£ä¸­æåˆ°äº†å“ªäº›å…³é”®æŠ€æœ¯ï¼Ÿ")
print(response)
```

**è¯´æ˜**: `similarity_top_k` æ§åˆ¶æ£€ç´¢æ•°é‡ï¼Œå¤ªå°å¯èƒ½é—æ¼ä¿¡æ¯ï¼Œå¤ªå¤§å¯èƒ½å¼•å…¥å™ªéŸ³ã€‚

## Response Mode è¯¦è§£

[ğŸ”— Response Synthesizer æ¨¡å¼è¯¦è§£](https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/response_modes/){target="_blank" rel="noopener"}

### å“åº”æ¨¡å¼å¯¹æ¯”

| æ¨¡å¼ | è¯´æ˜ | é€‚ç”¨åœºæ™¯ | Token æ¶ˆè€— |
|------|------|----------|------------|
| `compact` | å‹ç¼©æ‰€æœ‰ chunks åç”Ÿæˆå›ç­” | é€šç”¨åœºæ™¯ | ä¸­ |
| `refine` | é€ä¸ª chunk è¿­ä»£ä¼˜åŒ–å›ç­” | éœ€è¦ç²¾ç¡®ç­”æ¡ˆ | é«˜ |
| `tree_summarize` | å±‚çº§æ€»ç»“åç”Ÿæˆå›ç­” | æ–‡æ¡£æ€»ç»“ | é«˜ |
| `simple_summarize` | ç®€å•æ‹¼æ¥åæ€»ç»“ | å¿«é€Ÿå“åº” | ä½ |
| `no_text` | åªè¿”å›æ£€ç´¢ç»“æœä¸ç”Ÿæˆå›ç­” | è°ƒè¯•æ£€ç´¢ | æ—  |

### ç¤ºä¾‹ 3: ä½¿ç”¨ä¸åŒå“åº”æ¨¡å¼

> é€‚ç”¨ç‰ˆæœ¬: LlamaIndex 0.10.x+

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

documents = SimpleDirectoryReader("./data/").load_data()
index = VectorStoreIndex.from_documents(documents)

# æ¨¡å¼ 1: compactï¼ˆé»˜è®¤ï¼‰- å¹³è¡¡è´¨é‡å’Œé€Ÿåº¦
compact_engine = index.as_query_engine(
    response_mode="compact",
    similarity_top_k=3,
)

# æ¨¡å¼ 2: refine - è¿­ä»£ä¼˜åŒ–ï¼Œè´¨é‡æ›´é«˜
refine_engine = index.as_query_engine(
    response_mode="refine",
    similarity_top_k=5,
)

# æ¨¡å¼ 3: tree_summarize - é€‚åˆæ€»ç»“ç±»é—®é¢˜
summarize_engine = index.as_query_engine(
    response_mode="tree_summarize",
    similarity_top_k=10,
)

# å¯¹æ¯”æ•ˆæœ
question = "è¯·æ€»ç»“è¿™ä»½æ–‡æ¡£çš„æ ¸å¿ƒå†…å®¹"
print("Compact æ¨¡å¼:", compact_engine.query(question))
print("Refine æ¨¡å¼:", refine_engine.query(question))
print("Summarize æ¨¡å¼:", summarize_engine.query(question))
```

**è¯´æ˜**: æ ¹æ®é—®é¢˜ç±»å‹é€‰æ‹©åˆé€‚çš„å“åº”æ¨¡å¼ï¼Œæ€»ç»“ç±»é—®é¢˜æ¨è `tree_summarize`ã€‚

## é‡æ’åºä¼˜åŒ–

### ä¸ºä»€ä¹ˆéœ€è¦é‡æ’åºï¼Ÿ

å‘é‡ç›¸ä¼¼åº¦æœç´¢å¯èƒ½è¿”å›è¯­ä¹‰ç›¸ä¼¼ä½†ä¸ç›¸å…³çš„ç»“æœã€‚é‡æ’åºï¼ˆRerankerï¼‰å¯ä»¥ï¼š

1. ä½¿ç”¨æ›´ç²¾ç¡®çš„æ¨¡å‹é‡æ–°è¯„åˆ†
2. è¿‡æ»¤æ‰ä¸ç›¸å…³çš„ç»“æœ
3. æå‡æœ€ç»ˆå›ç­”è´¨é‡

```mermaid
graph TD
    A[ç”¨æˆ·é—®é¢˜] --> B[å‘é‡æ£€ç´¢]
    B --> C[Top-10 ç»“æœ]
    C --> D[Reranker é‡æ’åº]
    D --> E[Top-3 ç²¾é€‰ç»“æœ]
    E --> F[ç”Ÿæˆé«˜è´¨é‡å›ç­”]
```

**å›¾è¡¨è¯´æ˜**: é‡æ’åºåœ¨å‘é‡æ£€ç´¢åè¿›è¡ŒäºŒæ¬¡ç­›é€‰ï¼Œæå‡ç»“æœç›¸å…³æ€§ã€‚

### ç¤ºä¾‹ 4: ä½¿ç”¨ Cohere Reranker

> é€‚ç”¨ç‰ˆæœ¬: LlamaIndex 0.10.x+

```python
# å®‰è£…: pip install llama-index-postprocessor-cohere-rerank
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.postprocessor.cohere_rerank import CohereRerank

# è®¾ç½® Cohere API Key
import os
os.environ["COHERE_API_KEY"] = "your-cohere-api-key"

documents = SimpleDirectoryReader("./data/").load_data()
index = VectorStoreIndex.from_documents(documents)

# åˆ›å»ºé‡æ’åºå™¨
cohere_rerank = CohereRerank(
    api_key=os.environ["COHERE_API_KEY"],
    top_n=3,  # é‡æ’åºåä¿ç•™ top 3
)

# é…ç½®æŸ¥è¯¢å¼•æ“ä½¿ç”¨é‡æ’åº
query_engine = index.as_query_engine(
    similarity_top_k=10,  # å…ˆæ£€ç´¢ 10 ä¸ª
    node_postprocessors=[cohere_rerank],  # åº”ç”¨é‡æ’åº
)

response = query_engine.query("æ–‡æ¡£çš„å…³é”®ç»“è®ºæ˜¯ä»€ä¹ˆï¼Ÿ")
print(response)
```

**è¯´æ˜**: å…ˆæ£€ç´¢è¾ƒå¤šç»“æœï¼Œå†ç”¨ Reranker ç²¾é€‰ï¼Œå¹³è¡¡å¬å›ç‡å’Œç²¾ç¡®åº¦ã€‚

### ç¤ºä¾‹ 5: ä½¿ç”¨æœ¬åœ°é‡æ’åºæ¨¡å‹

> é€‚ç”¨ç‰ˆæœ¬: LlamaIndex 0.10.x+

```python
# å®‰è£…: pip install llama-index-postprocessor-flag-embedding-reranker
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker

documents = SimpleDirectoryReader("./data/").load_data()
index = VectorStoreIndex.from_documents(documents)

# ä½¿ç”¨æœ¬åœ° BGE Rerankerï¼ˆæ— éœ€ APIï¼‰
reranker = FlagEmbeddingReranker(
    model="BAAI/bge-reranker-base",
    top_n=3,
)

query_engine = index.as_query_engine(
    similarity_top_k=10,
    node_postprocessors=[reranker],
)

response = query_engine.query("é¡¹ç›®çš„æŠ€æœ¯æ¶æ„æ˜¯ä»€ä¹ˆï¼Ÿ")
print(response)
```

**è¯´æ˜**: BGE Reranker å®Œå…¨æœ¬åœ°è¿è¡Œï¼Œæ— éœ€ API è°ƒç”¨ï¼Œé€‚åˆéšç§æ•æ„Ÿåœºæ™¯ã€‚

## æµå¼å“åº”

### ä¸ºä»€ä¹ˆä½¿ç”¨æµå¼å“åº”ï¼Ÿ

ä¼ ç»Ÿæ¨¡å¼éœ€è¦ç­‰å¾…å®Œæ•´å›ç­”ç”Ÿæˆï¼Œç”¨æˆ·ä½“éªŒè¾ƒå·®ã€‚æµå¼å“åº”å¯ä»¥ï¼š

1. å®æ—¶æ˜¾ç¤ºç”Ÿæˆçš„æ–‡å­—
2. æå‡ç”¨æˆ·æ„ŸçŸ¥é€Ÿåº¦
3. ä¾¿äºæ„å»ºäº¤äº’å¼åº”ç”¨

### ç¤ºä¾‹ 6: é…ç½®æµå¼å“åº”

> é€‚ç”¨ç‰ˆæœ¬: LlamaIndex 0.10.x+

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

documents = SimpleDirectoryReader("./data/").load_data()
index = VectorStoreIndex.from_documents(documents)

# å¯ç”¨æµå¼å“åº”
query_engine = index.as_query_engine(streaming=True)

# æ‰§è¡Œæµå¼æŸ¥è¯¢
streaming_response = query_engine.query("è¯·è¯¦ç»†è§£é‡Šæ–‡æ¡£çš„æ ¸å¿ƒæ¦‚å¿µ")

# é€å­—è¾“å‡º
for text in streaming_response.response_gen:
    print(text, end="", flush=True)

print()  # æ¢è¡Œ
```

**è¯´æ˜**: æµå¼å“åº”è®©ç”¨æˆ·åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å°±èƒ½çœ‹åˆ°å†…å®¹ï¼Œæå‡äº¤äº’ä½“éªŒã€‚

## å¼•ç”¨æ¥æº

### ç¤ºä¾‹ 7: æ˜¾ç¤ºå›ç­”æ¥æº

> é€‚ç”¨ç‰ˆæœ¬: LlamaIndex 0.10.x+

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

documents = SimpleDirectoryReader("./data/").load_data()
index = VectorStoreIndex.from_documents(documents)

query_engine = index.as_query_engine(
    similarity_top_k=3,
)

response = query_engine.query("æ–‡æ¡£çš„ä¸»è¦ç»“è®ºæ˜¯ä»€ä¹ˆï¼Ÿ")

# è¾“å‡ºå›ç­”
print("å›ç­”:", response.response)
print("\næ¥æº:")

# è¾“å‡ºæ¥æºä¿¡æ¯
for i, node in enumerate(response.source_nodes, 1):
    print(f"\n[{i}] ç›¸å…³åº¦: {node.score:.4f}")
    print(f"    æ–‡ä»¶: {node.node.metadata.get('file_name', 'unknown')}")
    print(f"    å†…å®¹: {node.node.text[:100]}...")
```

**è¯´æ˜**: é€šè¿‡ `source_nodes` å¯ä»¥è·å–æ¯ä¸ªå›ç­”ç‰‡æ®µçš„åŸå§‹æ¥æºï¼Œå¢å¼ºå¯ä¿¡åº¦ã€‚

## Query Engine é…ç½®å®Œæ•´ç¤ºä¾‹

### ç¤ºä¾‹ 8: ç”Ÿäº§çº§é…ç½®

> é€‚ç”¨ç‰ˆæœ¬: LlamaIndex 0.10.x+

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker

# é…ç½®æ¨¡å‹
Settings.llm = OpenAI(model="gpt-4o-mini", temperature=0.1)
Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small")

# åŠ è½½æ–‡æ¡£å¹¶åˆ›å»ºç´¢å¼•
documents = SimpleDirectoryReader("./data/").load_data()
index = VectorStoreIndex.from_documents(documents)

# åˆ›å»ºé‡æ’åºå™¨
reranker = FlagEmbeddingReranker(
    model="BAAI/bge-reranker-base",
    top_n=5,
)

# åˆ›å»ºç”Ÿäº§çº§æŸ¥è¯¢å¼•æ“
query_engine = index.as_query_engine(
    similarity_top_k=10,              # åˆå§‹æ£€ç´¢æ•°é‡
    response_mode="compact",          # å“åº”æ¨¡å¼
    streaming=True,                   # å¯ç”¨æµå¼
    node_postprocessors=[reranker],   # é‡æ’åº
)

# æ‰§è¡ŒæŸ¥è¯¢
response = query_engine.query("è¯·åˆ†ææ–‡æ¡£çš„æ ¸å¿ƒè§‚ç‚¹å’Œå»ºè®®")

# æµå¼è¾“å‡º
for text in response.response_gen:
    print(text, end="", flush=True)
```

**è¯´æ˜**: è¿™ä¸ªé…ç½®ç»“åˆäº†é‡æ’åºã€æµå¼å“åº”å’Œåˆç†çš„æ£€ç´¢å‚æ•°ï¼Œé€‚åˆç”Ÿäº§ç¯å¢ƒã€‚

## é¿å‘æŒ‡å—

### âŒ å¸¸è§é—®é¢˜ 1: å›ç­”ä¸ç›¸å…³

**ç°è±¡**: LLM çš„å›ç­”ä¸é—®é¢˜å…³è”åº¦ä½ï¼Œæˆ–åŒ…å«æ— å…³ä¿¡æ¯ã€‚

**æ ¹å› **: `similarity_top_k` è¿‡å°ï¼Œæˆ–æ£€ç´¢åˆ°çš„ chunks ä¸ç›¸å…³ã€‚

**è§£å†³æ–¹æ¡ˆ**:

```python
# 1. å¢åŠ æ£€ç´¢æ•°é‡
query_engine = index.as_query_engine(similarity_top_k=10)

# 2. æ·»åŠ é‡æ’åº
from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker
reranker = FlagEmbeddingReranker(model="BAAI/bge-reranker-base", top_n=3)
query_engine = index.as_query_engine(
    similarity_top_k=10,
    node_postprocessors=[reranker],
)

# 3. æ£€æŸ¥æ£€ç´¢ç»“æœ
response = query_engine.query("ä½ çš„é—®é¢˜")
for node in response.source_nodes:
    print(f"ç›¸å…³åº¦: {node.score}, å†…å®¹: {node.node.text[:100]}")
```

**é¢„é˜²æªæ–½**: å§‹ç»ˆæ£€æŸ¥ source_nodes ç¡®è®¤æ£€ç´¢è´¨é‡ã€‚

### âŒ å¸¸è§é—®é¢˜ 2: ä¸Šä¸‹æ–‡è¿‡é•¿æŠ¥é”™

**ç°è±¡**:

```
openai.BadRequestError: This model's maximum context length is 8192 tokens
```

**æ ¹å› **: æ£€ç´¢çš„ chunks è¿‡å¤šæˆ–è¿‡é•¿ï¼Œè¶…å‡º LLM ä¸Šä¸‹æ–‡é™åˆ¶ã€‚

**è§£å†³æ–¹æ¡ˆ**:

```python
# 1. å‡å°‘æ£€ç´¢æ•°é‡
query_engine = index.as_query_engine(similarity_top_k=3)

# 2. ä½¿ç”¨ refine æ¨¡å¼ï¼ˆé€ä¸ªå¤„ç†ï¼Œä¸ä¼šè¶…é™ï¼‰
query_engine = index.as_query_engine(
    similarity_top_k=10,
    response_mode="refine",
)

# 3. åœ¨ç´¢å¼•æ—¶å‡å° chunk_size
from llama_index.core.node_parser import SentenceSplitter
splitter = SentenceSplitter(chunk_size=256, chunk_overlap=50)
```

**é¢„é˜²æªæ–½**: ç›‘æ§ token ä½¿ç”¨é‡ï¼Œæ ¹æ®æ¨¡å‹é™åˆ¶è°ƒæ•´å‚æ•°ã€‚

### âŒ å¸¸è§é—®é¢˜ 3: å“åº”è¶…æ—¶

**ç°è±¡**:

```
openai.APITimeoutError: Request timed out
```

**æ ¹å› **: LLM ç”Ÿæˆæ—¶é—´è¿‡é•¿ï¼Œæˆ–ç½‘ç»œä¸ç¨³å®šã€‚

**è§£å†³æ–¹æ¡ˆ**:

```python
from llama_index.llms.openai import OpenAI
from llama_index.core import Settings

# å¢åŠ è¶…æ—¶æ—¶é—´
Settings.llm = OpenAI(
    model="gpt-4o-mini",
    timeout=120,  # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    max_retries=3,  # é‡è¯•æ¬¡æ•°
)

# æˆ–ä½¿ç”¨æµå¼å“åº”é¿å…è¶…æ—¶æ„ŸçŸ¥
query_engine = index.as_query_engine(streaming=True)
```

**é¢„é˜²æªæ–½**: ç”Ÿäº§ç¯å¢ƒè®¾ç½®åˆç†çš„è¶…æ—¶å’Œé‡è¯•ç­–ç•¥ã€‚

### âŒ å¸¸è§é—®é¢˜ 4: å›ç­”ç¼ºä¹æ·±åº¦

**ç°è±¡**: å›ç­”è¿‡äºç®€çŸ­æˆ–è¡¨é¢ï¼Œç¼ºä¹è¯¦ç»†åˆ†æã€‚

**æ ¹å› **: ä½¿ç”¨äº† `simple_summarize` æ¨¡å¼ï¼Œæˆ– LLM temperature è¿‡é«˜ã€‚

**è§£å†³æ–¹æ¡ˆ**:

```python
from llama_index.llms.openai import OpenAI
from llama_index.core import Settings

# ä½¿ç”¨æ›´å¼ºçš„æ¨¡å‹å’Œæ›´ä½çš„ temperature
Settings.llm = OpenAI(model="gpt-4o", temperature=0.1)

# ä½¿ç”¨ refine æ¨¡å¼è·å¾—æ›´è¯¦ç»†çš„å›ç­”
query_engine = index.as_query_engine(
    similarity_top_k=8,
    response_mode="refine",
)
```

**é¢„é˜²æªæ–½**: é‡è¦é—®é¢˜ä½¿ç”¨ refine æ¨¡å¼å’Œé«˜è´¨é‡æ¨¡å‹ã€‚

## ç”Ÿäº§æœ€ä½³å®è·µ

### å‚æ•°æ¨è

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| similarity_top_k | 5-10 | åˆå§‹æ£€ç´¢æ•°é‡ |
| reranker top_n | 3-5 | é‡æ’åºåä¿ç•™æ•°é‡ |
| response_mode | compact | é€šç”¨åœºæ™¯ |
| streaming | true | æå‡ç”¨æˆ·ä½“éªŒ |
| temperature | 0.1-0.3 | é™ä½éšæœºæ€§ |

### æ€§èƒ½ä¼˜åŒ–

```python
# 1. ä½¿ç”¨ç¼“å­˜å‡å°‘é‡å¤æŸ¥è¯¢
from llama_index.core import Settings
Settings.llm = OpenAI(model="gpt-4o-mini")

# 2. æ‰¹é‡æŸ¥è¯¢ä¼˜åŒ–
questions = ["é—®é¢˜1", "é—®é¢˜2", "é—®é¢˜3"]
responses = [query_engine.query(q) for q in questions]

# 3. å¼‚æ­¥æŸ¥è¯¢
import asyncio

async def async_query(engine, question):
    return await engine.aquery(question)

# å¹¶å‘æ‰§è¡Œå¤šä¸ªæŸ¥è¯¢
responses = await asyncio.gather(*[
    async_query(query_engine, q) for q in questions
])
```

## å°ç»“

æœ¬ç« æˆ‘ä»¬å­¦ä¹ äº†ï¼š

1. âœ… **Query Engine æ ¸å¿ƒå‚æ•°**ï¼šsimilarity_top_kã€response_mode
2. âœ… **å“åº”æ¨¡å¼é€‰æ‹©**ï¼šcompactã€refineã€tree_summarize
3. âœ… **é‡æ’åºä¼˜åŒ–**ï¼šCohere Rerankerã€BGE Reranker
4. âœ… **æµå¼å“åº”ä¸å¼•ç”¨æ¥æº**ï¼šæå‡ç”¨æˆ·ä½“éªŒå’Œå¯ä¿¡åº¦

## ä¸‹ä¸€æ­¥

ç°åœ¨ä½ å·²ç»æŒæ¡äº†æŸ¥è¯¢å¼•æ“ä¼˜åŒ–ï¼Œè®©æˆ‘ä»¬ç»§ç»­å­¦ä¹  [Agent åŸºç¡€](/ai/llamaindex/guide/agent-basics)ï¼Œæ„å»ºèƒ½è‡ªä¸»æ€è€ƒå’Œè¡ŒåŠ¨çš„æ™ºèƒ½ä»£ç†ã€‚
