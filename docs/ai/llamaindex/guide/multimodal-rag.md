---
title: å¤šæ¨¡æ€ RAG
description: æŒæ¡ PDF å›¾è¡¨æå–å’Œå›¾ç‰‡ç†è§£ï¼Œæ„å»ºèƒ½å¤„ç†æ–‡æœ¬ä¸è§†è§‰å†…å®¹çš„ RAG ç³»ç»Ÿ
---

# å¤šæ¨¡æ€ RAG

## æ¦‚è¿°

æœ¬ç« å°†å¸®åŠ©ä½ æŒæ¡ **å¤šæ¨¡æ€ RAGï¼ˆMultimodal RAGï¼‰** æŠ€æœ¯ã€‚å®Œæˆæœ¬ç« åï¼Œä½ å°†èƒ½å¤Ÿï¼š

- ç†è§£å¤šæ¨¡æ€ RAG çš„å·¥ä½œåŸç†
- ä» PDF ä¸­æå–è¡¨æ ¼å’Œå›¾è¡¨
- ä½¿ç”¨è§†è§‰ LLM è¿›è¡Œå›¾ç‰‡ç†è§£
- æ„å»ºèƒ½åŒæ—¶å¤„ç†æ–‡æœ¬å’Œå›¾åƒçš„ RAG ç³»ç»Ÿ

## æ ¸å¿ƒæ¦‚å¿µ

### ä»€ä¹ˆæ˜¯å¤šæ¨¡æ€ RAGï¼Ÿ

**å¤šæ¨¡æ€ RAG** å°±åƒä¸€ä¸ª**æ—¢èƒ½é˜…è¯»æ–‡å­—åˆèƒ½çœ‹æ‡‚å›¾ç‰‡çš„åŠ©æ‰‹**ã€‚

æƒ³è±¡ä½ æœ‰ä¸€ä»½åŒ…å«æ–‡å­—è¯´æ˜å’Œæ•°æ®å›¾è¡¨çš„è´¢åŠ¡æŠ¥å‘Šã€‚ä¼ ç»Ÿ RAG åªèƒ½è¯»æ‡‚æ–‡å­—éƒ¨åˆ†ï¼Œå¯¹äºå›¾è¡¨ä¸­çš„è¶‹åŠ¿ã€è¡¨æ ¼ä¸­çš„æ•°å­—æ— èƒ½ä¸ºåŠ›ã€‚å¤šæ¨¡æ€ RAG åˆ™èƒ½ï¼š

- ğŸ“ ç†è§£æ–‡å­—å†…å®¹
- ğŸ“Š è§£è¯»å›¾è¡¨è¶‹åŠ¿
- ğŸ“‹ æå–è¡¨æ ¼æ•°æ®
- ğŸ–¼ï¸ åˆ†æå›¾ç‰‡å«ä¹‰

```mermaid
graph TD
    A[å¤æ‚æ–‡æ¡£] --> B{å†…å®¹ç±»å‹}

    B -->|çº¯æ–‡æœ¬| C[æ–‡æœ¬æå–å™¨]
    B -->|è¡¨æ ¼| D[è¡¨æ ¼æå–å™¨]
    B -->|å›¾è¡¨/å›¾ç‰‡| E[å›¾åƒæå–å™¨]

    C --> F[TextNode]
    D --> G[TextNode ç»“æ„åŒ–è¡¨æ ¼]
    E --> H[ImageNode]

    F --> I[æ–‡æœ¬ Embedding]
    G --> I
    H --> J[å›¾åƒ Embedding]

    I --> K[å¤šæ¨¡æ€ç´¢å¼•]
    J --> K

    K --> L[ç”¨æˆ·æŸ¥è¯¢]
    L --> M{æ£€ç´¢ç»“æœ}
    M --> N[æ–‡æœ¬èŠ‚ç‚¹]
    M --> O[å›¾åƒèŠ‚ç‚¹]

    N --> P[å¤šæ¨¡æ€ LLM]
    O --> P
    P --> Q[ç»¼åˆå›ç­”]
```

**å›¾è¡¨è¯´æ˜**: å¤šæ¨¡æ€ RAG å°†æ–‡æ¡£ä¸­çš„ä¸åŒå†…å®¹ç±»å‹åˆ†åˆ«å¤„ç†ï¼Œç»Ÿä¸€ç´¢å¼•ï¼Œæœ€ç»ˆé€šè¿‡å¤šæ¨¡æ€ LLM ç»¼åˆç”Ÿæˆç­”æ¡ˆã€‚

### TextNode vs ImageNode

| èŠ‚ç‚¹ç±»å‹ | å­˜å‚¨å†…å®¹ | ç”¨é€” |
|---------|---------|------|
| **TextNode** | æ–‡æœ¬å†…å®¹ + å…ƒæ•°æ® | ä¼ ç»Ÿæ–‡æœ¬æ£€ç´¢ |
| **ImageNode** | å›¾ç‰‡è·¯å¾„/Base64 + å…ƒæ•°æ® | å›¾åƒæ£€ç´¢å’Œç†è§£ |

LlamaIndex é€šè¿‡ä¸åŒçš„ Node ç±»å‹æ¥ç»Ÿä¸€ç®¡ç†å¤šæ¨¡æ€æ•°æ®ã€‚

---

## PDF æ–‡æ¡£è§£æ

### è§£æå™¨é€‰å‹

| è§£æå™¨ | é€‚ç”¨åœºæ™¯ | ä¼˜åŠ¿ | åŠ£åŠ¿ |
|--------|---------|------|------|
| **PyPDF / PyMuPDF** | ç®€å•çº¯æ–‡æœ¬ PDF | é€Ÿåº¦å¿«ã€æ— ä¾èµ– | æ— æ³•å¤„ç†å›¾è¡¨ |
| **Unstructured** | å¤æ‚æ–‡æ¡£ï¼ˆè¡¨æ ¼/å›¾è¡¨ï¼‰ | å¼€æºå…è´¹ã€åŠŸèƒ½å¼º | å®‰è£…ç¨å¤æ‚ |
| **LlamaParse** | é«˜ç²¾åº¦éœ€æ±‚ | ç²¾åº¦æœ€é«˜ | æ”¶è´¹æœåŠ¡ |

### ç¤ºä¾‹ 1: ä½¿ç”¨ PyMuPDF è§£æç®€å• PDF

> é€‚ç”¨ç‰ˆæœ¬: LlamaIndex 0.10.x+

```bash
# å®‰è£…ä¾èµ–
pip install llama-index-readers-file pymupdf
```

```python
from llama_index.core import VectorStoreIndex
from llama_index.readers.file import PyMuPDFReader

# åŠ è½½ PDF
reader = PyMuPDFReader()
documents = reader.load_data(file_path="./report.pdf")

print(f"åŠ è½½äº† {len(documents)} ä¸ªé¡µé¢")

# åˆ›å»ºç´¢å¼•
index = VectorStoreIndex.from_documents(documents)

# æŸ¥è¯¢
query_engine = index.as_query_engine()
response = query_engine.query("æŠ¥å‘Šçš„ä¸»è¦ç»“è®ºæ˜¯ä»€ä¹ˆï¼Ÿ")
print(response)
```

**è¯´æ˜**: PyMuPDF é€‚åˆçº¯æ–‡æœ¬ PDFï¼Œé€Ÿåº¦å¿«ä½†æ— æ³•æå–å›¾è¡¨å†…å®¹ã€‚

### ç¤ºä¾‹ 2: ä½¿ç”¨ Unstructured æå–è¡¨æ ¼å’Œå›¾è¡¨

> é€‚ç”¨ç‰ˆæœ¬: LlamaIndex 0.10.x+

Unstructured æ˜¯å¤„ç†å¤æ‚æ–‡æ¡£çš„**å¼€æºåˆ©å™¨**ï¼Œæ”¯æŒæå–è¡¨æ ¼ã€å›¾è¡¨ç­‰ç»“æ„åŒ–å†…å®¹ã€‚

```bash
# å®‰è£…ä¾èµ–
pip install unstructured[pdf]
pip install llama-index-readers-file
# å¦‚æœéœ€è¦ OCR åŠŸèƒ½
pip install pytesseract pdf2image
```

```python
from unstructured.partition.pdf import partition_pdf
from llama_index.core import Document, VectorStoreIndex

# ä½¿ç”¨ Unstructured è§£æ PDF
elements = partition_pdf(
    filename="./financial_report.pdf",
    strategy="auto",  # auto ç­–ç•¥ä¼šæ™ºèƒ½é€‰æ‹©è§£ææ–¹å¼
    extract_images_in_pdf=True,  # æå–å›¾ç‰‡
    infer_table_structure=True,  # æ¨æ–­è¡¨æ ¼ç»“æ„
    chunking_strategy="by_title",  # æŒ‰æ ‡é¢˜åˆ†å—
)

# æŸ¥çœ‹æå–çš„å…ƒç´ ç±»å‹
for element in elements[:10]:
    print(f"ç±»å‹: {type(element).__name__}, å†…å®¹é¢„è§ˆ: {str(element)[:100]}...")

# è½¬æ¢ä¸º LlamaIndex Document
documents = []
for element in elements:
    doc = Document(
        text=str(element),
        metadata={
            "element_type": type(element).__name__,
            "source": "financial_report.pdf",
        }
    )
    documents.append(doc)

print(f"å…±æå– {len(documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")

# åˆ›å»ºç´¢å¼•
index = VectorStoreIndex.from_documents(documents)
```

**è¯´æ˜**: Unstructured ä¼šè‡ªåŠ¨è¯†åˆ«æ–‡æ¡£ä¸­çš„ä¸åŒå…ƒç´ ç±»å‹ï¼ˆæ ‡é¢˜ã€æ®µè½ã€è¡¨æ ¼ç­‰ï¼‰ã€‚

### ç¤ºä¾‹ 3: è¡¨æ ¼æ•°æ®æå–

> é€‚ç”¨ç‰ˆæœ¬: LlamaIndex 0.10.x+

```python
from unstructured.partition.pdf import partition_pdf
from unstructured.documents.elements import Table

# è§£æ PDF å¹¶ä¸“æ³¨äºè¡¨æ ¼
elements = partition_pdf(
    filename="./data_report.pdf",
    strategy="hi_res",  # é«˜ç²¾åº¦æ¨¡å¼ï¼Œæ›´å¥½çš„è¡¨æ ¼è¯†åˆ«
    infer_table_structure=True,
)

# ç­›é€‰å‡ºè¡¨æ ¼å…ƒç´ 
tables = [el for el in elements if isinstance(el, Table)]

print(f"å‘ç° {len(tables)} ä¸ªè¡¨æ ¼")

for i, table in enumerate(tables):
    print(f"\n--- è¡¨æ ¼ {i+1} ---")
    print(table.text)

    # è·å–è¡¨æ ¼çš„ HTML ç»“æ„ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if hasattr(table, 'metadata') and hasattr(table.metadata, 'text_as_html'):
        print("HTML ç»“æ„:")
        print(table.metadata.text_as_html)
```

**è¯´æ˜**: ä½¿ç”¨ `hi_res` ç­–ç•¥å¯ä»¥è·å¾—æ›´å¥½çš„è¡¨æ ¼è¯†åˆ«æ•ˆæœï¼Œä½†å¤„ç†é€Ÿåº¦è¾ƒæ…¢ã€‚

### ç¤ºä¾‹ 4: ä½¿ç”¨ LlamaParseï¼ˆé«˜ç²¾åº¦æ–¹æ¡ˆï¼‰

> é€‚ç”¨ç‰ˆæœ¬: LlamaIndex 0.10.x+

LlamaParse æ˜¯ LlamaIndex å®˜æ–¹çš„äº‘ç«¯è§£ææœåŠ¡ï¼Œç²¾åº¦æœ€é«˜ä½†éœ€è¦ä»˜è´¹ã€‚

```bash
pip install llama-parse
```

```python
import os
from llama_parse import LlamaParse
from llama_index.core import VectorStoreIndex

# è®¾ç½® API Key
os.environ["LLAMA_CLOUD_API_KEY"] = "your-api-key"

# åˆ›å»ºè§£æå™¨
parser = LlamaParse(
    result_type="markdown",  # è¾“å‡ºä¸º Markdown æ ¼å¼
    verbose=True,
)

# è§£æ PDF
documents = parser.load_data("./complex_report.pdf")

print(f"è§£æå®Œæˆï¼Œå…± {len(documents)} ä¸ªæ–‡æ¡£")

# åˆ›å»ºç´¢å¼•å¹¶æŸ¥è¯¢
index = VectorStoreIndex.from_documents(documents)
query_engine = index.as_query_engine()
response = query_engine.query("æŠ¥å‘Šä¸­çš„å…³é”®æ•°æ®æ˜¯ä»€ä¹ˆï¼Ÿ")
print(response)
```

**è¯´æ˜**: LlamaParse æä¾›å…è´¹é¢åº¦ï¼ˆæ¯æœˆ 1000 é¡µï¼‰ï¼Œè¶…å‡ºåæŒ‰ $3/1000 é¡µæ”¶è´¹ã€‚

---

## å›¾ç‰‡ç†è§£

### å¤šæ¨¡æ€ LLM é…ç½®

LlamaIndex æ”¯æŒå¤šç§å¤šæ¨¡æ€ LLMï¼š

| LLM | æä¾›å•† | ç‰¹ç‚¹ |
|-----|--------|------|
| **GPT-4V / GPT-4o** | OpenAI | ç»¼åˆèƒ½åŠ›å¼º |
| **Claude 3 Vision** | Anthropic | é•¿ä¸Šä¸‹æ–‡ã€é«˜è´¨é‡ |
| **Gemini Pro Vision** | Google | æˆæœ¬è¾ƒä½ |
| **LLaVA** | å¼€æº | å¯æœ¬åœ°è¿è¡Œ |

### ç¤ºä¾‹ 5: ä½¿ç”¨ GPT-4V åˆ†æå›¾ç‰‡

> é€‚ç”¨ç‰ˆæœ¬: LlamaIndex 0.10.x+

```bash
pip install llama-index-multi-modal-llms-openai
```

```python
from llama_index.multi_modal_llms.openai import OpenAIMultiModal
from llama_index.core.schema import ImageDocument

# åˆ›å»ºå¤šæ¨¡æ€ LLM
mm_llm = OpenAIMultiModal(
    model="gpt-4o",  # æˆ– "gpt-4-vision-preview"
    max_new_tokens=1000,
)

# åŠ è½½å›¾ç‰‡ï¼ˆæ”¯æŒæœ¬åœ°è·¯å¾„æˆ– URLï¼‰
image_doc = ImageDocument(image_path="./chart.png")

# åˆ†æå›¾ç‰‡
response = mm_llm.complete(
    prompt="è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾è¡¨ï¼ŒåŒ…æ‹¬è¶‹åŠ¿ã€å…³é”®æ•°æ®ç‚¹å’Œå¯èƒ½çš„ç»“è®ºã€‚",
    image_documents=[image_doc],
)

print(response.text)
```

**è¯´æ˜**: GPT-4o å¯¹å›¾è¡¨ã€æµç¨‹å›¾ç­‰æŠ€æœ¯å›¾ç‰‡çš„ç†è§£èƒ½åŠ›å¾ˆå¼ºã€‚

### ç¤ºä¾‹ 6: ä½¿ç”¨ Claude 3 Vision

> é€‚ç”¨ç‰ˆæœ¬: LlamaIndex 0.10.x+

```bash
pip install llama-index-multi-modal-llms-anthropic
```

```python
from llama_index.multi_modal_llms.anthropic import AnthropicMultiModal
from llama_index.core.schema import ImageDocument

# åˆ›å»ºå¤šæ¨¡æ€ LLM
mm_llm = AnthropicMultiModal(
    model="claude-3-sonnet-20240229",  # æˆ– claude-3-opus
    max_tokens=1000,
)

# åŠ è½½å›¾ç‰‡
image_doc = ImageDocument(image_path="./diagram.png")

# åˆ†æå›¾ç‰‡
response = mm_llm.complete(
    prompt="è¿™å¼ å›¾å±•ç¤ºäº†ä»€ä¹ˆï¼Ÿè¯·è§£é‡Šå…¶ä¸­çš„å…³é”®æ¦‚å¿µå’Œæµç¨‹ã€‚",
    image_documents=[image_doc],
)

print(response.text)
```

### ç¤ºä¾‹ 7: ä»å›¾ç‰‡æå–ç»“æ„åŒ–æ•°æ®

> é€‚ç”¨ç‰ˆæœ¬: LlamaIndex 0.10.x+

```python
from llama_index.multi_modal_llms.openai import OpenAIMultiModal
from llama_index.core.schema import ImageDocument
from llama_index.core.program import MultiModalLLMCompletionProgram
from pydantic import BaseModel
from typing import List

# å®šä¹‰è¾“å‡ºç»“æ„
class ChartData(BaseModel):
    """å›¾è¡¨æ•°æ®ç»“æ„"""
    title: str
    chart_type: str  # å¦‚ "bar", "line", "pie"
    data_points: List[dict]
    trend: str
    key_insights: List[str]

# åˆ›å»ºå¤šæ¨¡æ€ LLM
mm_llm = OpenAIMultiModal(model="gpt-4o", max_new_tokens=1000)

# åŠ è½½å›¾ç‰‡
image_doc = ImageDocument(image_path="./sales_chart.png")

# åˆ›å»ºç»“æ„åŒ–æå–ç¨‹åº
program = MultiModalLLMCompletionProgram.from_defaults(
    output_cls=ChartData,
    image_documents=[image_doc],
    prompt_template_str="""
    åˆ†æè¿™å¼ å›¾è¡¨ï¼Œæå–ä»¥ä¸‹ä¿¡æ¯ï¼š
    1. å›¾è¡¨æ ‡é¢˜
    2. å›¾è¡¨ç±»å‹
    3. å…³é”®æ•°æ®ç‚¹
    4. æ•´ä½“è¶‹åŠ¿
    5. ä¸»è¦æ´å¯Ÿ

    è¯·ä»¥ JSON æ ¼å¼è¿”å›ç»“æœã€‚
    """,
    multi_modal_llm=mm_llm,
)

# æ‰§è¡Œæå–
result = program()
print(f"å›¾è¡¨æ ‡é¢˜: {result.title}")
print(f"å›¾è¡¨ç±»å‹: {result.chart_type}")
print(f"è¶‹åŠ¿: {result.trend}")
print(f"å…³é”®æ´å¯Ÿ: {result.key_insights}")
```

**è¯´æ˜**: ç»“åˆ Pydantic æ¨¡å‹ï¼Œå¯ä»¥ä»å›¾ç‰‡ä¸­æå–ç»“æ„åŒ–æ•°æ®ï¼Œä¾¿äºåç»­å¤„ç†ã€‚

---

## å¤šæ¨¡æ€ç´¢å¼•ä¸æ£€ç´¢

### å¤šæ¨¡æ€ RAG æ¶æ„

```mermaid
graph TD
    subgraph æ•°æ®å‡†å¤‡
        A[PDF æ–‡æ¡£] --> B[Unstructured è§£æ]
        B --> C[æ–‡æœ¬å—]
        B --> D[æå–çš„å›¾ç‰‡]
    end

    subgraph ç´¢å¼•æ„å»º
        C --> E[æ–‡æœ¬ Embedding]
        D --> F[å›¾ç‰‡ Embedding]
        E --> G[å¤šæ¨¡æ€å‘é‡å­˜å‚¨]
        F --> G
    end

    subgraph æŸ¥è¯¢æµç¨‹
        H[ç”¨æˆ·é—®é¢˜] --> I[é—®é¢˜ Embedding]
        I --> J[å¤šæ¨¡æ€æ£€ç´¢]
        G --> J
        J --> K[ç›¸å…³æ–‡æœ¬èŠ‚ç‚¹]
        J --> L[ç›¸å…³å›¾ç‰‡èŠ‚ç‚¹]
        K --> M[å¤šæ¨¡æ€ LLM]
        L --> M
        M --> N[ç»¼åˆå›ç­”]
    end
```

### ç¤ºä¾‹ 8: æ„å»ºå¤šæ¨¡æ€ RAG Pipeline

> é€‚ç”¨ç‰ˆæœ¬: LlamaIndex 0.10.x+

```python
import os
from pathlib import Path
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.core.schema import ImageDocument, TextNode
from llama_index.multi_modal_llms.openai import OpenAIMultiModal

# å‡†å¤‡æ•°æ®ç›®å½•
data_dir = Path("./multimodal_data")
text_dir = data_dir / "texts"
image_dir = data_dir / "images"

# åŠ è½½æ–‡æœ¬æ–‡æ¡£
text_documents = SimpleDirectoryReader(str(text_dir)).load_data()

# åŠ è½½å›¾ç‰‡æ–‡æ¡£
image_documents = []
for img_path in image_dir.glob("*.png"):
    image_documents.append(
        ImageDocument(
            image_path=str(img_path),
            metadata={"source": img_path.name}
        )
    )

print(f"åŠ è½½äº† {len(text_documents)} ä¸ªæ–‡æœ¬æ–‡æ¡£")
print(f"åŠ è½½äº† {len(image_documents)} ä¸ªå›¾ç‰‡æ–‡æ¡£")

# åˆå¹¶æ‰€æœ‰æ–‡æ¡£
all_documents = text_documents + image_documents

# åˆ›å»ºç´¢å¼•
index = VectorStoreIndex.from_documents(all_documents)

# é…ç½®å¤šæ¨¡æ€ LLM
mm_llm = OpenAIMultiModal(model="gpt-4o", max_new_tokens=1000)

# åˆ›å»ºå¤šæ¨¡æ€æŸ¥è¯¢å¼•æ“
query_engine = index.as_query_engine(
    multi_modal_llm=mm_llm,
    similarity_top_k=5,  # æ£€ç´¢ top 5 ç›¸å…³å†…å®¹
)

# æ‰§è¡ŒæŸ¥è¯¢
response = query_engine.query("æ ¹æ®æ–‡æ¡£å’Œå›¾è¡¨ï¼Œæ€»ç»“ä¸»è¦å‘ç°")
print(response)
```

### ç¤ºä¾‹ 9: å®Œæ•´ç«¯åˆ°ç«¯ Pipeline

> é€‚ç”¨ç‰ˆæœ¬: LlamaIndex 0.10.x+

```python
from pathlib import Path
from unstructured.partition.pdf import partition_pdf
from llama_index.core import Document, VectorStoreIndex
from llama_index.core.schema import ImageDocument
from llama_index.multi_modal_llms.openai import OpenAIMultiModal

def build_multimodal_index(pdf_path: str, output_dir: str = "./extracted"):
    """
    ä» PDF æ„å»ºå¤šæ¨¡æ€ç´¢å¼•çš„å®Œæ•´æµç¨‹

    Args:
        pdf_path: PDF æ–‡ä»¶è·¯å¾„
        output_dir: æå–å›¾ç‰‡çš„è¾“å‡ºç›®å½•
    """
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)

    # Step 1: è§£æ PDF
    print("æ­£åœ¨è§£æ PDF...")
    elements = partition_pdf(
        filename=pdf_path,
        strategy="hi_res",
        extract_images_in_pdf=True,
        extract_image_block_output_dir=str(output_path),
        infer_table_structure=True,
    )

    # Step 2: æ•´ç†æ–‡æœ¬èŠ‚ç‚¹
    text_documents = []
    for element in elements:
        doc = Document(
            text=str(element),
            metadata={
                "element_type": type(element).__name__,
                "source": pdf_path,
            }
        )
        text_documents.append(doc)

    print(f"æå–äº† {len(text_documents)} ä¸ªæ–‡æœ¬ç‰‡æ®µ")

    # Step 3: æ•´ç†å›¾ç‰‡èŠ‚ç‚¹
    image_documents = []
    for img_path in output_path.glob("*.png"):
        image_documents.append(
            ImageDocument(
                image_path=str(img_path),
                metadata={"source": pdf_path}
            )
        )

    print(f"æå–äº† {len(image_documents)} å¼ å›¾ç‰‡")

    # Step 4: æ„å»ºç´¢å¼•
    all_documents = text_documents + image_documents
    index = VectorStoreIndex.from_documents(all_documents)

    return index


def query_multimodal(index, question: str):
    """æ‰§è¡Œå¤šæ¨¡æ€æŸ¥è¯¢"""
    mm_llm = OpenAIMultiModal(model="gpt-4o", max_new_tokens=1000)

    query_engine = index.as_query_engine(
        multi_modal_llm=mm_llm,
        similarity_top_k=5,
    )

    response = query_engine.query(question)
    return response


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æ„å»ºç´¢å¼•
    index = build_multimodal_index("./annual_report.pdf")

    # æŸ¥è¯¢
    response = query_multimodal(
        index,
        "æ ¹æ®æŠ¥å‘Šä¸­çš„å›¾è¡¨ï¼Œåˆ†æä»Šå¹´çš„ä¸šç»©è¡¨ç°"
    )
    print(response)
```

---

## é¿å‘æŒ‡å—

### âŒ å¸¸è§é—®é¢˜ 1: å›¾ç‰‡åŠ è½½å¤±è´¥

**ç°è±¡**:

```
FileNotFoundError: [Errno 2] No such file or directory
```

**è§£å†³æ–¹æ¡ˆ**:

```python
from pathlib import Path

# ä½¿ç”¨ç»å¯¹è·¯å¾„
image_path = Path("./images/chart.png").resolve()
image_doc = ImageDocument(image_path=str(image_path))

# æˆ–æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
if not image_path.exists():
    raise FileNotFoundError(f"å›¾ç‰‡ä¸å­˜åœ¨: {image_path}")
```

### âŒ å¸¸è§é—®é¢˜ 2: å›¾ç‰‡æ ¼å¼ä¸æ”¯æŒ

**ç°è±¡**: æŸäº›å›¾ç‰‡æ— æ³•è¢«å¤šæ¨¡æ€ LLM å¤„ç†ã€‚

**è§£å†³æ–¹æ¡ˆ**:

```python
from PIL import Image

def convert_to_png(image_path: str) -> str:
    """å°†å›¾ç‰‡è½¬æ¢ä¸º PNG æ ¼å¼"""
    img = Image.open(image_path)
    png_path = image_path.rsplit(".", 1)[0] + ".png"
    img.save(png_path, "PNG")
    return png_path

# æ”¯æŒçš„æ ¼å¼ï¼šPNG, JPEG, GIF, WebP
# å»ºè®®ç»Ÿä¸€ä½¿ç”¨ PNG æ ¼å¼
```

### âŒ å¸¸è§é—®é¢˜ 3: Unstructured å®‰è£…é—®é¢˜

**ç°è±¡**: å®‰è£… Unstructured æ—¶ä¾èµ–æŠ¥é”™ã€‚

**è§£å†³æ–¹æ¡ˆ**:

```bash
# Windows ç”¨æˆ·
pip install "unstructured[pdf]"
# å¯èƒ½éœ€è¦å®‰è£… poppler
# ä¸‹è½½: https://github.com/oschwartz10612/poppler-windows/releases

# macOS ç”¨æˆ·
brew install poppler
pip install "unstructured[pdf]"

# Linux ç”¨æˆ·
sudo apt-get install poppler-utils
pip install "unstructured[pdf]"
```

### âŒ å¸¸è§é—®é¢˜ 4: å¤šæ¨¡æ€ LLM æˆæœ¬è¿‡é«˜

**ç°è±¡**: å¤„ç†å¤§é‡å›¾ç‰‡æ—¶ API è´¹ç”¨æ¿€å¢ã€‚

**ç¼“è§£ç­–ç•¥**:

1. **å›¾ç‰‡å‹ç¼©**: åœ¨å‘é€å‰å‹ç¼©å›¾ç‰‡å°ºå¯¸
2. **æ‰¹é‡å¤„ç†**: å¤šå¼ ç›¸å…³å›¾ç‰‡åˆå¹¶å¤„ç†
3. **ç¼“å­˜ç»“æœ**: å¯¹ç›¸åŒå›¾ç‰‡ç¼“å­˜åˆ†æç»“æœ

```python
from PIL import Image

def resize_image(image_path: str, max_size: int = 1024) -> str:
    """å‹ç¼©å›¾ç‰‡ä»¥å‡å°‘ API æˆæœ¬"""
    img = Image.open(image_path)

    # å¦‚æœå›¾ç‰‡è¿‡å¤§ï¼Œç­‰æ¯”ç¼©æ”¾
    if max(img.size) > max_size:
        ratio = max_size / max(img.size)
        new_size = (int(img.size[0] * ratio), int(img.size[1] * ratio))
        img = img.resize(new_size, Image.Resampling.LANCZOS)

    resized_path = image_path.rsplit(".", 1)[0] + "_resized.png"
    img.save(resized_path, "PNG", optimize=True)
    return resized_path
```

---

## ç”Ÿäº§æœ€ä½³å®è·µ

### è§£æç­–ç•¥é€‰æ‹©

| æ–‡æ¡£ç±»å‹ | æ¨èè§£æå™¨ | è¯´æ˜ |
|---------|-----------|------|
| çº¯æ–‡æœ¬ PDF | PyMuPDF | é€Ÿåº¦å¿«ï¼Œæˆæœ¬ä½ |
| å¸¦è¡¨æ ¼ PDF | Unstructured `hi_res` | è¡¨æ ¼è¯†åˆ«å‡†ç¡® |
| æ‰«æä»¶ PDF | Unstructured + OCR | éœ€è¦ Tesseract |
| é«˜ç²¾åº¦éœ€æ±‚ | LlamaParse | æ”¶è´¹ä½†ç²¾åº¦æœ€é«˜ |

### æ€§èƒ½ä¼˜åŒ–

1. **é¢„å¤„ç†ç¼“å­˜**: å°†è§£æç»“æœç¼“å­˜ï¼Œé¿å…é‡å¤å¤„ç†
2. **å¼‚æ­¥å¤„ç†**: å¤§æ–‡æ¡£ä½¿ç”¨å¼‚æ­¥è§£æ
3. **åˆ†é¡µå¤„ç†**: è¶…å¤§ PDF åˆ†é¡µå¤„ç†ï¼Œé¿å…å†…å­˜æº¢å‡º

```python
# å¼‚æ­¥å¤„ç†ç¤ºä¾‹
import asyncio
from llama_parse import LlamaParse

async def parse_pdf_async(file_path: str):
    parser = LlamaParse(result_type="markdown")
    documents = await parser.aload_data(file_path)
    return documents

# æ‰¹é‡å¼‚æ­¥å¤„ç†
async def parse_multiple_pdfs(file_paths: list):
    tasks = [parse_pdf_async(fp) for fp in file_paths]
    results = await asyncio.gather(*tasks)
    return results
```

---

## å°ç»“

æœ¬ç« æˆ‘ä»¬å­¦ä¹ äº†ï¼š

1. âœ… **å¤šæ¨¡æ€ RAG æ¦‚å¿µ**: ç†è§£å¦‚ä½•å¤„ç†æ–‡æœ¬å’Œå›¾åƒæ··åˆå†…å®¹
2. âœ… **PDF è§£æ**: ä½¿ç”¨ Unstructured å’Œ LlamaParse æå–è¡¨æ ¼å’Œå›¾è¡¨
3. âœ… **å›¾ç‰‡ç†è§£**: é…ç½® GPT-4V/Claude 3 è¿›è¡Œè§†è§‰åˆ†æ
4. âœ… **å¤šæ¨¡æ€ç´¢å¼•**: æ„å»ºèƒ½æ£€ç´¢æ–‡æœ¬å’Œå›¾åƒçš„ RAG ç³»ç»Ÿ

## ä¸‹ä¸€æ­¥

æ­å–œä½ å®Œæˆäº† LlamaIndex è¿›é˜¶æ•™ç¨‹ï¼ä½ ç°åœ¨å¯ä»¥ï¼š

- å›é¡¾ [ç”Ÿäº§éƒ¨ç½²](/ai/llamaindex/guide/production) äº†è§£æ›´å¤šä¼˜åŒ–æŠ€å·§
- æ¢ç´¢ [Agent è¿›é˜¶](/ai/llamaindex/guide/agent-advanced) æ„å»ºæ›´å¤æ‚çš„åº”ç”¨
- å‚è€ƒå®˜æ–¹æ–‡æ¡£è·å–æœ€æ–° API æ›´æ–°
