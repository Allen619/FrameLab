---
title: Streaming æµå¼å¤„ç†
description: æŒæ¡ LangGraph çš„ 6 ç§ stream modeï¼Œä»çŠ¶æ€å¢é‡åˆ° LLM token çº§æµå¼è¾“å‡º
---

# Streaming æµå¼å¤„ç†

> å¯¹å‰ç«¯å¼€å‘è€…æ¥è¯´ï¼Œæµå¼è¾“å‡ºå¹¶ä¸é™Œç”Ÿâ€”â€”SSEã€ReadableStreamã€EventSource éƒ½æ˜¯æ—¥å¸¸å·¥å…·ã€‚LangGraph çš„ streaming æœ¬è´¨ä¸ŠåšåŒä¸€ä»¶äº‹ï¼šè®©ä½ åœ¨å›¾æ‰§è¡Œè¿‡ç¨‹ä¸­**å®æ—¶æ‹¿åˆ°ä¸­é—´æ•°æ®**ï¼Œè€Œä¸æ˜¯ç­‰å…¨éƒ¨å®Œæˆã€‚

## å‰ç«¯ç±»æ¯”ï¼šå…ˆå»ºç«‹ç›´è§‰

| å‰ç«¯æ¦‚å¿µ | LangGraph stream mode | è¯´æ˜ |
|---------|----------------------|------|
| `EventSource` (SSE) | `stream()` æ–¹æ³• | æŒç»­æ¥æ”¶æœåŠ¡ç«¯æ¨é€çš„äº‹ä»¶æµ |
| `response.body.getReader()` | `messages` mode | é€ token è¯»å– LLM è¾“å‡º |
| `ReadableStream` + transform | `custom` mode | è‡ªå®šä¹‰æ•°æ®åœ¨æµä¸­ä¼ é€’ |
| Redux action log | `updates` mode | æ¯ä¸ªèŠ‚ç‚¹æ‰§è¡Œåçš„çŠ¶æ€å˜æ›´ |
| React DevTools profiler | `debug` mode | å®Œæ•´çš„æ‰§è¡Œè¿½è¸ªä¿¡æ¯ |

**LangGraph åŸç”Ÿè¯­ä¹‰**ï¼š`graph.stream()` æ˜¯ LangGraph çš„æµå¼æ‰§è¡Œå…¥å£ã€‚é€šè¿‡ `stream_mode` å‚æ•°ï¼Œä½ å¯ä»¥ç²¾ç¡®æ§åˆ¶æ¥æ”¶å“ªç§ç²’åº¦çš„æ•°æ®â€”â€”ä»å®Œæ•´çŠ¶æ€å¿«ç…§åˆ°å•ä¸ª LLM tokenã€‚

[ğŸ”— Streaming å®˜æ–¹æŒ‡å—](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/){target="_blank" rel="noopener"}

---

## 1. æ”¯æŒçš„ Stream Mode åˆ—è¡¨

LangGraph æ”¯æŒä»¥ä¸‹ 6 ç§ stream modeï¼š

| Mode | è¾“å‡ºå†…å®¹ | å…¸å‹åœºæ™¯ |
|------|---------|---------|
| `values` | æ¯æ­¥åçš„**å®Œæ•´çŠ¶æ€** | éœ€è¦çœ‹åˆ°å…¨é‡ state å˜åŒ– |
| `updates` | æ¯æ­¥çš„**å¢é‡æ›´æ–°** | åªå…³å¿ƒ"å˜äº†ä»€ä¹ˆ" |
| `messages` | LLM çš„ **token çº§è¾“å‡º** | å‰ç«¯å®æ—¶å±•ç¤ºæ‰“å­—æ•ˆæœ |
| `events` | LangChain å›è°ƒäº‹ä»¶ | ä¸ LangSmith é›†æˆçš„è§‚æµ‹ |
| `custom` | èŠ‚ç‚¹å†…è‡ªå®šä¹‰æ•°æ® | è¿›åº¦æ¡ã€ä¸­é—´ç»“æœæ¨é€ |
| `debug` | å®Œæ•´è°ƒè¯•ä¿¡æ¯ | å¼€å‘æ’æŸ¥é—®é¢˜ |

---

## 2. åŸºç¡€ç”¨æ³•

### `values` â€” å®Œæ•´çŠ¶æ€å¿«ç…§

æ¯æ¬¡èŠ‚ç‚¹æ‰§è¡Œå®Œæ¯•åï¼Œè¿”å›å½“å‰çš„**å®Œæ•´çŠ¶æ€**ï¼š

```python
from langgraph.graph import StateGraph, START, END, MessagesState
from langgraph.checkpoint.memory import InMemorySaver

def node_a(state: MessagesState):
    return {"messages": [{"role": "assistant", "content": "ç¬¬ä¸€æ­¥å®Œæˆ"}]}

def node_b(state: MessagesState):
    return {"messages": [{"role": "assistant", "content": "ç¬¬äºŒæ­¥å®Œæˆ"}]}

builder = StateGraph(MessagesState)
builder.add_node("a", node_a)
builder.add_node("b", node_b)
builder.add_edge(START, "a")
builder.add_edge("a", "b")
builder.add_edge("b", END)

graph = builder.compile()

# values modeï¼šæ¯æ¬¡æ‹¿åˆ°å®Œæ•´ state
for chunk in graph.stream(
    {"messages": [{"role": "user", "content": "å¼€å§‹"}]},
    stream_mode="values"
):
    print(f"æ¶ˆæ¯æ•°é‡: {len(chunk['messages'])}")
    print(f"æœ€åä¸€æ¡: {chunk['messages'][-1]}")
    print("---")
```

**å‰ç«¯ç±»æ¯”**ï¼šç±»ä¼¼äº Redux store çš„ `subscribe()` â€” æ¯æ¬¡ state å˜æ›´åä½ æ‹¿åˆ°å®Œæ•´çš„ store snapshotã€‚

### `updates` â€” å¢é‡æ›´æ–°

åªè¿”å›æ¯ä¸ªèŠ‚ç‚¹çš„**è¾“å‡ºå¢é‡**ï¼Œä¸åŒ…å«å®Œæ•´çŠ¶æ€ï¼š

```python
# updates modeï¼šåªçœ‹æ¯æ­¥å˜äº†ä»€ä¹ˆ
for chunk in graph.stream(
    {"messages": [{"role": "user", "content": "å¼€å§‹"}]},
    stream_mode="updates"
):
    # chunk æ˜¯ dictï¼Œkey æ˜¯èŠ‚ç‚¹åï¼Œvalue æ˜¯è¯¥èŠ‚ç‚¹çš„è¾“å‡º
    for node_name, update in chunk.items():
        print(f"èŠ‚ç‚¹ [{node_name}] è¾“å‡º: {update}")
```

**å‰ç«¯ç±»æ¯”**ï¼šç±»ä¼¼äº Redux ä¸­é—´ä»¶åªçœ‹ action payloadï¼Œè€Œä¸æ˜¯æ•´ä¸ª state treeã€‚

è¾“å‡ºç¤ºä¾‹ï¼š
```
èŠ‚ç‚¹ [a] è¾“å‡º: {'messages': [{'role': 'assistant', 'content': 'ç¬¬ä¸€æ­¥å®Œæˆ'}]}
èŠ‚ç‚¹ [b] è¾“å‡º: {'messages': [{'role': 'assistant', 'content': 'ç¬¬äºŒæ­¥å®Œæˆ'}]}
```

---

## 3. å¤šæ¨¡å¼ç»„åˆ Streaming

ä½ å¯ä»¥åŒæ—¶è®¢é˜…å¤šç§ stream modeï¼ŒLangGraph ä¼šç”¨å…ƒç»„æ ‡è®°æ¯æ¡æ•°æ®çš„æ¥æºï¼š

```python
# åŒæ—¶æ¥æ”¶ updates å’Œ messages
for mode, chunk in graph.stream(
    {"messages": [{"role": "user", "content": "è®²ä¸ªç¬‘è¯"}]},
    stream_mode=["updates", "messages"]
):
    if mode == "updates":
        # èŠ‚ç‚¹çº§æ›´æ–°
        print(f"[UPDATE] {chunk}")
    elif mode == "messages":
        # LLM token çº§è¾“å‡º
        msg_chunk, metadata = chunk
        if msg_chunk.content:
            print(f"[TOKEN] {msg_chunk.content}", end="", flush=True)
```

**å‰ç«¯ç±»æ¯”**ï¼šç±»ä¼¼äºåœ¨ä¸€ä¸ª `EventSource` è¿æ¥ä¸­é€šè¿‡ä¸åŒçš„ `event` ç±»å‹åŒºåˆ†æ¶ˆæ¯ï¼š

```javascript
// å‰ç«¯ç±»æ¯”
const source = new EventSource('/stream');
source.addEventListener('update', (e) => { /* èŠ‚ç‚¹æ›´æ–° */ });
source.addEventListener('token', (e) => { /* LLM token */ });
```

---

## 4. Stream Graph State

### æµå¼è·å–çŠ¶æ€ï¼ˆé€‚åˆå‰ç«¯è½®è¯¢å±•ç¤ºï¼‰

```python
from langgraph.graph import StateGraph, START, END
from typing import TypedDict, Annotated
from operator import add

class AnalysisState(TypedDict):
    query: str
    steps: Annotated[list[str], add]
    result: str

def step_research(state: AnalysisState):
    return {"steps": ["research_done"]}

def step_analyze(state: AnalysisState):
    return {"steps": ["analysis_done"]}

def step_summarize(state: AnalysisState):
    return {"result": "æœ€ç»ˆåˆ†æç»“è®º", "steps": ["summary_done"]}

builder = StateGraph(AnalysisState)
builder.add_node("research", step_research)
builder.add_node("analyze", step_analyze)
builder.add_node("summarize", step_summarize)
builder.add_edge(START, "research")
builder.add_edge("research", "analyze")
builder.add_edge("analyze", "summarize")
builder.add_edge("summarize", END)

graph = builder.compile()

# ç”¨ values mode æµå¼è§‚å¯Ÿå®Œæ•´çŠ¶æ€æ¼”å˜
for state_snapshot in graph.stream(
    {"query": "åˆ†æ LangGraph", "steps": [], "result": ""},
    stream_mode="values"
):
    print(f"å·²å®Œæˆæ­¥éª¤: {state_snapshot['steps']}")
    if state_snapshot.get("result"):
        print(f"ç»“æœ: {state_snapshot['result']}")
```

---

## 5. Stream Subgraph Outputs

å½“ä½ çš„å›¾åŒ…å«å­å›¾æ—¶ï¼Œå¯ä»¥é€šè¿‡ `subgraphs=True` å‚æ•°è·å–å­å›¾å†…éƒ¨çš„æ‰§è¡Œæµï¼š

```python
from langgraph.graph import StateGraph, START, END, MessagesState

# å®šä¹‰å­å›¾
def sub_node(state: MessagesState):
    return {"messages": [{"role": "assistant", "content": "å­å›¾å¤„ç†å®Œæˆ"}]}

sub_builder = StateGraph(MessagesState)
sub_builder.add_node("sub_process", sub_node)
sub_builder.add_edge(START, "sub_process")
sub_builder.add_edge("sub_process", END)
sub_graph = sub_builder.compile()

# ä¸»å›¾å¼•ç”¨å­å›¾
def main_node(state: MessagesState):
    return {"messages": [{"role": "assistant", "content": "ä¸»å›¾èŠ‚ç‚¹"}]}

main_builder = StateGraph(MessagesState)
main_builder.add_node("main", main_node)
main_builder.add_node("sub", sub_graph)  # å­å›¾ä½œä¸ºèŠ‚ç‚¹
main_builder.add_edge(START, "main")
main_builder.add_edge("main", "sub")
main_builder.add_edge("sub", END)

graph = main_builder.compile()

# subgraphs=True ä¼šå±•å¼€å­å›¾å†…éƒ¨çš„æ‰§è¡Œæµ
for namespace, mode, chunk in graph.stream(
    {"messages": [{"role": "user", "content": "hello"}]},
    stream_mode="updates",
    subgraphs=True
):
    if namespace:
        print(f"[å­å›¾ {namespace}] {chunk}")
    else:
        print(f"[ä¸»å›¾] {chunk}")
```

### è°ƒè¯•æŠ€å·§

å­å›¾è°ƒè¯•æ—¶ï¼Œ`namespace` å…ƒç»„æ ‡è¯†äº†è°ƒç”¨é“¾è·¯å¾„ï¼š

```python
# namespace ç¤ºä¾‹ï¼š
# () â€” ä¸»å›¾
# ('sub:abc123',) â€” åä¸º "sub" çš„å­å›¾ï¼Œè¿è¡Œ ID ä¸º abc123
# ('sub:abc123', 'nested:def456') â€” å­å›¾ä¸­åµŒå¥—çš„å­å›¾
```

---

## 6. LLM Token Streaming

è¿™æ˜¯å‰ç«¯æœ€å¸¸ç”¨çš„åœºæ™¯â€”â€”é€ token å±•ç¤º LLM è¾“å‡ºï¼Œå®ç°"æ‰“å­—æœºæ•ˆæœ"ã€‚

```python
from dataclasses import dataclass
from langchain.chat_models import init_chat_model
from langgraph.graph import StateGraph, START

@dataclass
class JokeState:
    topic: str
    joke: str = ""

model = init_chat_model(model="gpt-4.1-mini")

def call_model(state: JokeState):
    response = model.invoke(
        [{"role": "user", "content": f"è®²ä¸€ä¸ªå…³äº{state.topic}çš„ç¬‘è¯"}]
    )
    return {"joke": response.content}

graph = (
    StateGraph(JokeState)
    .add_node(call_model)
    .add_edge(START, "call_model")
    .compile()
)

# messages modeï¼šé€ token æµå¼è¾“å‡º
for message_chunk, metadata in graph.stream(
    {"topic": "ç¨‹åºå‘˜"},
    stream_mode="messages"
):
    if message_chunk.content:
        print(message_chunk.content, end="", flush=True)

# metadata åŒ…å«æœ‰ç”¨ä¿¡æ¯ï¼š
# {
#   "langgraph_node": "call_model",
#   "langgraph_triggers": ["start:call_model"],
#   "tags": [...],
#   ...
# }
```

### å¼‚æ­¥ç‰ˆæœ¬ï¼ˆæ›´é€‚åˆ Web æœåŠ¡ï¼‰

```python
async def stream_to_client():
    """åœ¨ FastAPI / asyncio ç¯å¢ƒä¸­ä½¿ç”¨"""
    async for message_chunk, metadata in graph.astream(
        {"topic": "å‰ç«¯å¼€å‘"},
        stream_mode="messages"
    ):
        if message_chunk.content:
            yield message_chunk.content  # å‘é€ç»™å‰ç«¯
```

### æŒ‰æ ‡ç­¾è¿‡æ»¤ LLM è¾“å‡º

å½“ä¸€ä¸ªèŠ‚ç‚¹ä¸­æœ‰å¤šä¸ª LLM è°ƒç”¨æ—¶ï¼Œç”¨æ ‡ç­¾åŒºåˆ†ï¼š

```python
from langchain.chat_models import init_chat_model

joke_model = init_chat_model(model="gpt-4.1-mini", tags=["joke"])
poem_model = init_chat_model(model="gpt-4.1-mini", tags=["poem"])

async def multi_llm_node(state, config):
    joke = await joke_model.ainvoke(
        [{"role": "user", "content": f"è®²ä¸ªå…³äº{state['topic']}çš„ç¬‘è¯"}],
        config
    )
    poem = await poem_model.ainvoke(
        [{"role": "user", "content": f"å†™é¦–å…³äº{state['topic']}çš„çŸ­è¯—"}],
        config
    )
    return {"joke": joke.content, "poem": poem.content}

# æµå¼æ—¶æŒ‰ tag è¿‡æ»¤
async for msg, metadata in graph.astream(
    {"topic": "çŒ«"},
    stream_mode="messages"
):
    if "joke" in metadata.get("tags", []):
        print(f"[ç¬‘è¯] {msg.content}", end="")
    elif "poem" in metadata.get("tags", []):
        print(f"[è¯—æ­Œ] {msg.content}", end="")
```

---

## 7. Stream Custom Dataï¼ˆè‡ªå®šä¹‰æ•°æ®æµï¼‰

å½“ä½ éœ€è¦åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­å‘å®¢æˆ·ç«¯æ¨é€è‡ªå®šä¹‰è¿›åº¦ä¿¡æ¯ã€‚[ğŸ”— StreamWriter ä¸è‡ªå®šä¹‰æ•°æ®æµ](https://langchain-ai.github.io/langgraph/how-tos/streaming-content/){target="_blank" rel="noopener"}

```python
from typing import TypedDict
from langgraph.types import StreamWriter
from langgraph.graph import StateGraph, START, END

class State(TypedDict):
    query: str
    answer: str

def processing_node(state: State, writer: StreamWriter):

    # æ¨é€è¿›åº¦æ›´æ–°
    writer({"progress": 0.3, "stage": "æ­£åœ¨æœç´¢ç›¸å…³æ–‡æ¡£..."})

    # ... æ‰§è¡Œæœç´¢ ...

    writer({"progress": 0.7, "stage": "æ­£åœ¨ç”Ÿæˆå›ç­”..."})

    # ... è°ƒç”¨ LLM ...

    writer({"progress": 1.0, "stage": "å®Œæˆ"})

    return {"answer": "è¿™æ˜¯ç”Ÿæˆçš„å›ç­”"}

graph = (
    StateGraph(State)
    .add_node("process", processing_node)
    .add_edge(START, "process")
    .add_edge("process", END)
    .compile()
)

# custom mode æ¥æ”¶è‡ªå®šä¹‰æ•°æ®
for chunk in graph.stream(
    {"query": "ä»€ä¹ˆæ˜¯ LangGraph?", "answer": ""},
    stream_mode="custom"
):
    print(chunk)
# è¾“å‡ºï¼š
# {'progress': 0.3, 'stage': 'æ­£åœ¨æœç´¢ç›¸å…³æ–‡æ¡£...'}
# {'progress': 0.7, 'stage': 'æ­£åœ¨ç”Ÿæˆå›ç­”...'}
# {'progress': 1.0, 'stage': 'å®Œæˆ'}
```

**å‰ç«¯ç±»æ¯”**ï¼šè¿™å°±åƒåœ¨åç«¯å¤„ç†ä¸­é€šè¿‡ WebSocket æˆ– SSE å‘å‰ç«¯æ¨é€è¿›åº¦æ¡æ•°æ®ï¼š

```javascript
// å‰ç«¯æ¥æ”¶ç«¯ç±»æ¯”
const source = new EventSource('/api/process?query=xxx');
source.onmessage = (e) => {
  const { progress, stage } = JSON.parse(e.data);
  updateProgressBar(progress);
  updateStatusText(stage);
};
```

### ç»„åˆ custom å’Œå…¶ä»– mode

```python
# åŒæ—¶æ¥æ”¶è‡ªå®šä¹‰æ•°æ®å’ŒèŠ‚ç‚¹æ›´æ–°
for mode, chunk in graph.stream(
    {"query": "test", "answer": ""},
    stream_mode=["custom", "updates"]
):
    if mode == "custom":
        print(f"[è¿›åº¦] {chunk}")
    elif mode == "updates":
        print(f"[èŠ‚ç‚¹] {chunk}")
```

---

## 8. ä¸ä»»æ„ LLM é…åˆä½¿ç”¨

å¦‚æœä½ çš„ LLM ä¸æ˜¯ LangChain é›†æˆçš„ï¼ˆæ¯”å¦‚è‡ªç ”æ¨¡å‹æˆ–å°ä¼— APIï¼‰ï¼Œå¯ä»¥ç”¨ `custom` mode æ‰‹åŠ¨æµå¼è¾“å‡ºï¼š

```python
import httpx
from langgraph.types import StreamWriter
from langgraph.graph import StateGraph, START, END
from typing import TypedDict

class State(TypedDict):
    prompt: str
    response: str

def call_custom_llm(state: State, writer: StreamWriter):

    # æ‰‹åŠ¨è°ƒç”¨é LangChain çš„ LLM API
    with httpx.stream(
        "POST",
        "https://your-llm-api.com/generate",
        json={"prompt": state["prompt"]},
    ) as response:
        full_text = ""
        for line in response.iter_lines():
            token = line  # è§£æä½ çš„ API æ ¼å¼
            full_text += token
            # é€šè¿‡ custom stream æ¨é€æ¯ä¸ª token
            writer({"token": token})

    return {"response": full_text}

graph = (
    StateGraph(State)
    .add_node(call_custom_llm)
    .add_edge(START, "call_custom_llm")
    .add_edge("call_custom_llm", END)
    .compile()
)

for chunk in graph.stream(
    {"prompt": "hello", "response": ""},
    stream_mode="custom"
):
    print(chunk["token"], end="", flush=True)
```

---

## 9. ç¦ç”¨ç‰¹å®šæ¨¡å‹çš„ Streaming

æŸäº›åœºæ™¯ä¸‹ä½ å¯èƒ½ä¸å¸Œæœ›æŸä¸ª LLM è°ƒç”¨äº§ç”Ÿæµå¼è¾“å‡ºï¼ˆæ¯”å¦‚å†…éƒ¨åˆ†ç±»å™¨ï¼‰ï¼š

```python
from langchain.chat_models import init_chat_model

# ç¦ç”¨æŸä¸ªæ¨¡å‹å®ä¾‹çš„ streaming
classifier = init_chat_model(model="gpt-4.1-mini", streaming=False)

# æˆ–è€…åœ¨è°ƒç”¨æ—¶ç¦ç”¨
response = model.invoke(
    messages,
    config={"callbacks": []},  # ç§»é™¤å›è°ƒä¹Ÿä¼šé˜»æ­¢ streaming äº‹ä»¶
)
```

åœ¨ `messages` stream mode ä¸‹ï¼Œè¢«ç¦ç”¨ streaming çš„æ¨¡å‹è°ƒç”¨ä¸ä¼šäº§ç”Ÿé€ token è¾“å‡ºï¼Œè€Œæ˜¯åœ¨è°ƒç”¨å®Œæˆåä¸€æ¬¡æ€§è¿”å›å®Œæ•´æ¶ˆæ¯ã€‚

---

## 10. æµå¼è¾“å‡ºçš„å‰åç«¯é›†æˆæ¶æ„

```mermaid
sequenceDiagram
    participant Browser as æµè§ˆå™¨
    participant API as FastAPI åç«¯
    participant LG as LangGraph Runtime
    participant LLM as LLM API

    Browser->>API: POST /chat/stream (SSE)
    API->>LG: graph.astream(input, stream_mode=["messages","custom"])

    loop æ¯ä¸ª token / è‡ªå®šä¹‰äº‹ä»¶
        LG->>LLM: è°ƒç”¨æ¨¡å‹
        LLM-->>LG: token chunk
        LG-->>API: (mode, chunk)
        API-->>Browser: SSE event: {type, data}
    end

    LG-->>API: æ‰§è¡Œå®Œæ¯•
    API-->>Browser: SSE: [DONE]
```

### FastAPI é›†æˆç¤ºä¾‹

```python
from fastapi import FastAPI
from fastapi.responses import StreamingResponse

app = FastAPI()

@app.post("/chat/stream")
async def chat_stream(query: str):
    async def event_generator():
        async for mode, chunk in graph.astream(
            {"messages": [{"role": "user", "content": query}]},
            stream_mode=["messages", "custom"]
        ):
            if mode == "messages":
                msg_chunk, _ = chunk
                if msg_chunk.content:
                    yield f"data: {msg_chunk.content}\n\n"
            elif mode == "custom":
                yield f"event: progress\ndata: {chunk}\n\n"
        yield "data: [DONE]\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream"
    )
```

---

## è¦ç‚¹å›é¡¾

| Stream Mode | ç²’åº¦ | ç”¨é€” |
|------------|------|------|
| `values` | å®Œæ•´çŠ¶æ€ | è§‚å¯Ÿæ¯æ­¥åçš„å…¨é‡ state |
| `updates` | èŠ‚ç‚¹å¢é‡ | åªçœ‹"å˜äº†ä»€ä¹ˆ" |
| `messages` | LLM token | å‰ç«¯æ‰“å­—æœºæ•ˆæœ |
| `events` | å›è°ƒäº‹ä»¶ | ä¸ LangSmith é›†æˆ |
| `custom` | è‡ªå®šä¹‰æ•°æ® | è¿›åº¦æ¡ã€é LangChain LLM |
| `debug` | è°ƒè¯•è¯¦æƒ… | å¼€å‘æ’æŸ¥ |

---

## å…ˆä¿®ä¸ä¸‹ä¸€æ­¥

- **å…ˆä¿®**ï¼š[æŒä¹…åŒ–](/ai/langgraph/guide/persistence) | [Durable Execution](/ai/langgraph/guide/durable-execution)
- **ä¸‹ä¸€æ­¥**ï¼š[Interrupts (HITL)](/ai/langgraph/guide/interrupts) | [Subgraphs å­å›¾](/ai/langgraph/guide/subgraphs)
