---
title: éƒ¨ç½²æŒ‡å—
description: å­¦ä¹ å¦‚ä½•å°† AI SDK åº”ç”¨éƒ¨ç½²åˆ° Vercelã€Node.js ç­‰ç¯å¢ƒï¼ŒæŒæ¡ç¯å¢ƒå˜é‡ç®¡ç†ã€è¶…æ—¶å¤„ç†å’Œ Edge Runtime é…ç½®ã€‚
---

# éƒ¨ç½²æŒ‡å—

æœ¬ç« ä»‹ç»å°† AI SDK åº”ç”¨éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒçš„å…³é”®é…ç½®ï¼ŒåŒ…æ‹¬ Vercel éƒ¨ç½²ã€Node.js ç‹¬ç«‹éƒ¨ç½²ã€è¶…æ—¶å¤„ç†å’Œè¿è¡Œæ—¶é€‰æ‹©ã€‚

[ğŸ”— AI SDK Vercel éƒ¨ç½²æŒ‡å—](https://ai-sdk.dev/docs/advanced/vercel-deployment-guide){target="_blank" rel="noopener"}

## Vercel éƒ¨ç½²ï¼ˆæ¨èï¼‰

Vercel æ˜¯ AI SDK çš„æ¨èéƒ¨ç½²å¹³å°ï¼Œæä¾›äº†å¼€ç®±å³ç”¨çš„ Serverless å’Œ Edge è¿è¡Œæ—¶æ”¯æŒã€‚

### åŸºç¡€éƒ¨ç½²

```bash
# å®‰è£… Vercel CLI
npm install -g vercel

# éƒ¨ç½²
vercel
```

### è¶…æ—¶é…ç½®

Vercel çš„ Fluid Compute é»˜è®¤å‡½æ•°è¶…æ—¶ä¸º **5 åˆ†é’Ÿï¼ˆ300 ç§’ï¼‰**ï¼Œå¯¹å¤§å¤šæ•°æµå¼åº”ç”¨å·²ç»è¶³å¤Ÿã€‚å¦‚éœ€æ›´é•¿è¶…æ—¶ï¼š

**æ–¹å¼ä¸€ï¼šåœ¨ Next.js è·¯ç”±ä¸­é…ç½®**

```typescript
// app/api/chat/route.ts
import { streamText, convertToModelMessages, type UIMessage } from 'ai'

// è®¾ç½®æœ€å¤§æ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰
export const maxDuration = 600 // 10 åˆ†é’Ÿï¼Œéœ€è¦ Pro æˆ– Enterprise è®¡åˆ’

export async function POST(req: Request) {
  const { messages }: { messages: UIMessage[] } = await req.json()

  const result = streamText({
    model: 'openai/gpt-4o',
    messages: await convertToModelMessages(messages),
  })

  return result.toUIMessageStreamResponse()
}
```

**æ–¹å¼äºŒï¼šåœ¨ vercel.json ä¸­é…ç½®**

```json
{
  "functions": {
    "api/chat/route.ts": {
      "maxDuration": 600
    }
  }
}
```

> è¶…è¿‡ 300 ç§’éœ€è¦ Vercel Pro æˆ– Enterprise è®¡åˆ’ã€‚

### æµå¼å“åº”é…ç½®

éƒ¨ç½²åæµå¼ä¸å·¥ä½œæ—¶ï¼Œéœ€è¦æ·»åŠ ä¼ è¾“ç¼–ç å¤´ï¼š

```typescript
export async function POST(req: Request) {
  const { messages } = await req.json()

  const result = streamText({
    model: 'openai/gpt-4o',
    messages: await convertToModelMessages(messages),
  })

  return result.toUIMessageStreamResponse({
    headers: {
      'Transfer-Encoding': 'chunked',
      Connection: 'keep-alive',
    },
  })
}
```

## Node.js ç‹¬ç«‹éƒ¨ç½²

### Express æœåŠ¡å™¨

```typescript
import express from 'express'
import { streamText, convertToModelMessages } from 'ai'

const app = express()
app.use(express.json())

app.post('/api/chat', async (req, res) => {
  const { messages } = req.body

  const result = streamText({
    model: 'openai/gpt-4o',
    messages: await convertToModelMessages(messages),
  })

  // è®¾ç½®æµå¼å“åº”å¤´
  res.setHeader('Content-Type', 'text/event-stream')
  res.setHeader('Cache-Control', 'no-cache')
  res.setHeader('Connection', 'keep-alive')

  // å°† AI SDK çš„æµå†™å…¥å“åº”
  const stream = result.toDataStream()
  const reader = stream.getReader()

  while (true) {
    const { done, value } = await reader.read()
    if (done) break
    res.write(value)
  }

  res.end()
})

app.listen(3000, () => {
  console.log('æœåŠ¡è¿è¡Œåœ¨ http://localhost:3000')
})
```

### Docker éƒ¨ç½²

```dockerfile
FROM node:20-alpine

WORKDIR /app

COPY package*.json ./
RUN npm ci --only=production

COPY . .
RUN npm run build

EXPOSE 3000

CMD ["node", "dist/server.js"]
```

```yaml
# docker-compose.yml
version: '3.8'
services:
  ai-app:
    build: .
    ports:
      - "3000:3000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - NODE_ENV=production
    restart: unless-stopped
```

## ç¯å¢ƒå˜é‡ç®¡ç†

### å¿…è¦çš„ç¯å¢ƒå˜é‡

```bash
# .env.localï¼ˆæœ¬åœ°å¼€å‘ï¼Œä¸æäº¤åˆ° Gitï¼‰

# LLM Provider API Keys
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
DEEPSEEK_API_KEY=sk-...
GOOGLE_API_KEY=AIza...

# ç¼“å­˜ï¼ˆå¯é€‰ï¼‰
KV_URL=https://...
KV_TOKEN=...

# åº”ç”¨é…ç½®
NODE_ENV=production
```

### Vercel ç¯å¢ƒå˜é‡é…ç½®

```bash
# é€šè¿‡ CLI è®¾ç½®
vercel env add OPENAI_API_KEY

# æˆ–åœ¨ Vercel Dashboard â†’ Settings â†’ Environment Variables ä¸­é…ç½®
```

### å®‰å…¨æœ€ä½³å®è·µ

1. **æ°¸è¿œä¸è¦åœ¨å®¢æˆ·ç«¯æš´éœ² API Key** â€” æ‰€æœ‰ LLM è°ƒç”¨å¿…é¡»åœ¨æœåŠ¡ç«¯è¿›è¡Œ
2. **ä½¿ç”¨ç¯å¢ƒå˜é‡** â€” ä¸è¦ç¡¬ç¼–ç  Key
3. **åŒºåˆ†ç¯å¢ƒ** â€” å¼€å‘/é¢„è§ˆ/ç”Ÿäº§ä½¿ç”¨ä¸åŒçš„ Key
4. **Key è½®æ¢** â€” å®šæœŸæ›´æ¢ API Key
5. **æƒé™æœ€å°åŒ–** â€” æ¯ä¸ª Key åªæˆäºˆå¿…è¦çš„æƒé™

```typescript
// æœåŠ¡ç«¯è·¯ç”± â€” å®‰å…¨
export async function POST(req: Request) {
  const result = await generateText({
    model: 'openai/gpt-4o', // API Key ä»ç¯å¢ƒå˜é‡è¯»å–
    prompt: '...',
  })
  return new Response(result.text)
}

// âŒ é”™è¯¯ï¼šæ°¸è¿œä¸è¦åœ¨å®¢æˆ·ç«¯ä»£ç ä¸­ä½¿ç”¨ API Key
// const openai = createOpenAI({ apiKey: 'sk-...' })
```

## Edge Runtime

Edge Runtime è¿è¡Œåœ¨ CDN è¾¹ç¼˜èŠ‚ç‚¹ï¼Œæä¾›æ›´ä½çš„å»¶è¿Ÿï¼š

```typescript
// app/api/chat/route.ts

// ä½¿ç”¨ Edge Runtime
export const runtime = 'edge'

export async function POST(req: Request) {
  const { messages } = await req.json()

  const result = streamText({
    model: 'openai/gpt-4o',
    messages: await convertToModelMessages(messages),
  })

  return result.toUIMessageStreamResponse()
}
```

### Edge vs Node.js Runtime

| ç‰¹æ€§ | Edge Runtime | Node.js Runtime |
|------|-------------|-----------------|
| å†·å¯åŠ¨ | æå¿«ï¼ˆ< 50msï¼‰ | è¾ƒæ…¢ï¼ˆ100-500msï¼‰ |
| æ‰§è¡Œæ—¶é—´é™åˆ¶ | é€šå¸¸ 30 ç§’ | å¯é…ç½®åˆ° 10 åˆ†é’Ÿ |
| Node.js API | éƒ¨åˆ†æ”¯æŒ | å®Œå…¨æ”¯æŒ |
| æ–‡ä»¶ç³»ç»Ÿ | ä¸æ”¯æŒ | æ”¯æŒ |
| æµå¼æ”¯æŒ | åŸç”Ÿæ”¯æŒ | æ”¯æŒ |
| é€‚ç”¨åœºæ™¯ | ç®€å•çš„æµå¼èŠå¤© | å¤æ‚çš„ Agentã€RAG |

**é€‰æ‹©å»ºè®®ï¼š**

- ç®€å•çš„å¯¹è¯å¼ AI â†’ Edge Runtimeï¼ˆä½å»¶è¿Ÿï¼‰
- å¤æ‚çš„ Agent / RAG / æ–‡ä»¶å¤„ç† â†’ Node.js Runtimeï¼ˆåŠŸèƒ½å®Œæ•´ï¼‰
- é•¿æ—¶é—´è¿è¡Œçš„ä»»åŠ¡ â†’ Node.js Runtimeï¼ˆæ›´é•¿è¶…æ—¶ï¼‰

## ç”Ÿäº§ç¯å¢ƒæ£€æŸ¥æ¸…å•

### éƒ¨ç½²å‰

- [ ] æ‰€æœ‰ API Key é…ç½®ä¸ºç¯å¢ƒå˜é‡
- [ ] è®¾ç½®åˆç†çš„ `maxDuration` è¶…æ—¶
- [ ] å®æ–½é€Ÿç‡é™åˆ¶
- [ ] æ·»åŠ é”™è¯¯å¤„ç†å’Œæ—¥å¿—
- [ ] é…ç½®ç¼“å­˜ç­–ç•¥

### éƒ¨ç½²å

- [ ] éªŒè¯æµå¼å“åº”æ­£å¸¸å·¥ä½œ
- [ ] æ£€æŸ¥é”™è¯¯ç›‘æ§å’Œå‘Šè­¦
- [ ] æµ‹è¯•è¶…æ—¶åœºæ™¯
- [ ] ç›‘æ§ Token ä½¿ç”¨é‡å’Œæˆæœ¬
- [ ] éªŒè¯ CORS é…ç½®ï¼ˆå¦‚æœå‰åç«¯åˆ†ç¦»ï¼‰

### æ€§èƒ½ä¼˜åŒ–

```typescript
// 1. å¯ç”¨æµå¼å“åº” â€” é™ä½é¦–å­—èŠ‚æ—¶é—´ï¼ˆTTFBï¼‰
const result = streamText({
  model: 'openai/gpt-4o',
  messages,
})

// 2. é€‰æ‹©åˆé€‚çš„æ¨¡å‹ â€” ç®€å•ä»»åŠ¡ç”¨å°æ¨¡å‹
const model =
  taskComplexity === 'simple' ? 'openai/gpt-4o-mini' : 'openai/gpt-4o'

// 3. è®¾ç½®åˆç†çš„ maxOutputTokens â€” é¿å…æµªè´¹
const result = await generateText({
  model: 'openai/gpt-4o',
  maxOutputTokens: 500, // æŒ‰éœ€è®¾ç½®
  prompt: '...',
})
```

## ä¸‹ä¸€æ­¥

- [ç¼“å­˜ä¸é€Ÿç‡é™åˆ¶](/ai/vercel-ai-sdk/guide/caching-and-limits) â€” æ·±å…¥äº†è§£ç”Ÿäº§ç¯å¢ƒçš„ç¼“å­˜å’Œé™æµ
- [é”™è¯¯å¤„ç†ä¸æµ‹è¯•](/ai/vercel-ai-sdk/guide/error-handling) â€” å®Œå–„é”™è¯¯å¤„ç†æœºåˆ¶
- [Provider é€‰å‹æŒ‡å—](/ai/vercel-ai-sdk/guide/providers) â€” é€‰æ‹©æœ€é€‚åˆéƒ¨ç½²éœ€æ±‚çš„ Provider
