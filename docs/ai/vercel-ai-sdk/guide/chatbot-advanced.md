---
title: èŠå¤©è¿›é˜¶
description: æ¶ˆæ¯æŒä¹…åŒ–ã€æµæ¢å¤ã€å·¥å…·è°ƒç”¨å±•ç¤ºå’Œæ¶ˆæ¯å…ƒæ•°æ®ç­‰é«˜çº§èŠå¤©åŠŸèƒ½
---

# èŠå¤©è¿›é˜¶

> åŸºç¡€èŠå¤©æœºå™¨äººåªèƒ½å¤„ç†å•æ¬¡ä¼šè¯ã€‚ç”Ÿäº§çº§åº”ç”¨è¿˜éœ€è¦ **æ¶ˆæ¯æŒä¹…åŒ–**ï¼ˆåˆ·æ–°ä¸ä¸¢å¤±å†å²ï¼‰ã€**æµæ¢å¤**ï¼ˆæ–­çº¿è‡ªåŠ¨é‡è¿ï¼‰ã€**å·¥å…·è°ƒç”¨å±•ç¤º**ï¼ˆå°† AI å·¥å…·è°ƒç”¨æ¸²æŸ“ä¸ºå¯è§†åŒ–ç»„ä»¶ï¼‰ç­‰èƒ½åŠ›ã€‚æœ¬ç¯‡é€ä¸€è®²è§£è¿™äº›è¿›é˜¶åœºæ™¯ã€‚

## 1. æ¶ˆæ¯æŒä¹…åŒ–

[ğŸ”— æ¶ˆæ¯æŒä¹…åŒ–æ–‡æ¡£](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence){target="_blank" rel="noopener"} çš„æ ¸å¿ƒæ€è·¯æ˜¯ï¼š**å®¢æˆ·ç«¯åªå‘é€æœ€æ–°ä¸€æ¡æ¶ˆæ¯ï¼ŒæœåŠ¡ç«¯è´Ÿè´£åŠ è½½å’Œä¿å­˜å®Œæ•´å¯¹è¯å†å²**ã€‚

### 1.1 æ¶æ„è®¾è®¡

```mermaid
sequenceDiagram
    participant Client as å®¢æˆ·ç«¯<br/>(useChat)
    participant Server as API Route
    participant DB as æ•°æ®åº“<br/>(Redis/Postgres)
    participant LLM as LLM

    Client->>Server: POST { id, message }
    Server->>DB: loadChat(id)
    DB-->>Server: previousMessages
    Server->>Server: æ‹¼æ¥ [...previous, message]
    Server->>LLM: streamText(allMessages)
    LLM-->>Server: æµå¼å“åº”
    Server-->>Client: UIMessage Stream
    Server->>DB: saveChat(id, finalMessages)
```

### 1.2 å®¢æˆ·ç«¯å®ç°

å®¢æˆ·ç«¯ä½¿ç”¨ `prepareSendMessagesRequest` åªå‘é€æœ€æ–°æ¶ˆæ¯å’ŒèŠå¤© IDï¼š

```tsx
'use client'

import { UIMessage, useChat } from '@ai-sdk/react'
import { DefaultChatTransport } from 'ai'
import { useState } from 'react'

export default function Chat({
  id,
  initialMessages,
}: {
  id?: string
  initialMessages?: UIMessage[]
}) {
  const [input, setInput] = useState('')
  const { sendMessage, messages } = useChat({
    id,
    messages: initialMessages,
    transport: new DefaultChatTransport({
      api: '/api/chat',
      // åªå‘é€æœ€æ–°æ¶ˆæ¯ + èŠå¤© IDï¼Œå‡å°‘ä¼ è¾“é‡
      prepareSendMessagesRequest: ({ id, messages }) => ({
        body: {
          id,
          message: messages[messages.length - 1],
        },
      }),
    }),
  })

  return (
    <div>
      {messages.map(m => (
        <div key={m.id}>
          {m.role === 'user' ? 'ä½ ï¼š' : 'AIï¼š'}
          {m.parts
            .map(part => (part.type === 'text' ? part.text : ''))
            .join('')}
        </div>
      ))}

      <form
        onSubmit={e => {
          e.preventDefault()
          if (input.trim()) {
            sendMessage({ text: input })
            setInput('')
          }
        }}
      >
        <input value={input} onChange={e => setInput(e.target.value)} />
        <button type="submit">å‘é€</button>
      </form>
    </div>
  )
}
```

### 1.3 æœåŠ¡ç«¯å®ç°

æœåŠ¡ç«¯è´Ÿè´£åŠ è½½å†å²ã€éªŒè¯æ¶ˆæ¯ã€ç”Ÿæˆå“åº”ã€ä¿å­˜ç»“æœï¼š

```typescript
// app/api/chat/route.ts
import {
  convertToModelMessages,
  streamText,
  UIMessage,
  validateUIMessages,
} from 'ai'
import { openai } from '@ai-sdk/openai'
import { loadChat, saveChat } from '@/utils/chat-store'

export async function POST(req: Request) {
  const { message, id } = await req.json()

  // 1. ä»æ•°æ®åº“åŠ è½½å†å²æ¶ˆæ¯
  const previousMessages = await loadChat(id)

  // 2. æ‹¼æ¥æ–°æ¶ˆæ¯
  const messages = [...previousMessages, message]

  // 3. éªŒè¯æ¶ˆæ¯ï¼ˆç¡®ä¿å·¥å…·è°ƒç”¨ç­‰æ•°æ®ç»“æ„æ­£ç¡®ï¼‰
  const validatedMessages = await validateUIMessages({ messages })

  // 4. è°ƒç”¨ LLM
  const result = streamText({
    model: openai('gpt-4o'),
    messages: convertToModelMessages(validatedMessages),
  })

  // 5. è¿”å›æµå¼å“åº”ï¼Œå¹¶åœ¨å®Œæˆæ—¶ä¿å­˜
  return result.toUIMessageStreamResponse({
    originalMessages: validatedMessages,
    onFinish: ({ messages }) => {
      saveChat({ chatId: id, messages })
    },
  })
}
```

### 1.4 é¡µé¢åŠ è½½æ—¶æ¢å¤èŠå¤©

åœ¨ Next.js çš„ Server Component ä¸­åŠ è½½å†å²æ¶ˆæ¯å¹¶ä¼ ç»™å®¢æˆ·ç«¯ç»„ä»¶ï¼š

```tsx
// app/chat/[id]/page.tsx
import { loadChat } from '@/utils/chat-store'
import Chat from '@/components/chat'

export default async function ChatPage({
  params,
}: {
  params: Promise<{ id: string }>
}) {
  const { id } = await params
  const messages = await loadChat(id)
  return <Chat id={id} initialMessages={messages} />
}
```

## 2. æµæ¢å¤ï¼ˆResume Streamsï¼‰

[ğŸ”— æµæ¢å¤æ–‡æ¡£](https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-resume-streams){target="_blank" rel="noopener"} è§£å†³äº†ä¸€ä¸ªå¸¸è§é—®é¢˜ï¼š**ç”¨æˆ·åœ¨ AI ç”Ÿæˆè¿‡ç¨‹ä¸­åˆ·æ–°é¡µé¢ï¼Œå“åº”ä¸ä¼šä¸¢å¤±**ã€‚

### 2.1 å·¥ä½œåŸç†

æµæ¢å¤çš„æ ¸å¿ƒæœºåˆ¶ï¼š

1. **æµåˆ›å»º**ï¼šPOST è¯·æ±‚æ—¶ï¼Œå°† SSE æµå­˜å‚¨ä¸ºå¯æ¢å¤æµï¼ˆResumable Streamï¼‰ï¼Œç”Ÿæˆ `streamId`
2. **æµè¿½è¸ª**ï¼šå°† `activeStreamId` ä¿å­˜åˆ°èŠå¤©æ•°æ®ä¸­
3. **å®¢æˆ·ç«¯é‡è¿**ï¼šé¡µé¢åˆ·æ–°åï¼Œ`useChat` è‡ªåŠ¨å‘èµ· GET è¯·æ±‚æ¢å¤æµ
4. **æµæ¢å¤**ï¼šæœåŠ¡ç«¯æ ¹æ® `activeStreamId` æ‰¾åˆ°ç¼“å­˜çš„æµå¹¶ç»§ç»­æ¨é€
5. **æ¸…ç†**ï¼šæµå®Œæˆåï¼Œæ¸…é™¤ `activeStreamId`

### 2.2 å®¢æˆ·ç«¯å¯ç”¨æµæ¢å¤

åªéœ€è®¾ç½® `resume: true`ï¼š

```tsx
'use client'

import { useChat } from '@ai-sdk/react'
import { DefaultChatTransport, type UIMessage } from 'ai'

export function Chat({
  chatData,
  resume = false,
}: {
  chatData: { id: string; messages: UIMessage[] }
  resume?: boolean
}) {
  const { messages, sendMessage, status } = useChat({
    id: chatData.id,
    messages: chatData.messages,
    resume, // å¯ç”¨è‡ªåŠ¨æµæ¢å¤
    transport: new DefaultChatTransport({
      prepareSendMessagesRequest: ({ id, messages }) => ({
        body: {
          id,
          message: messages[messages.length - 1],
        },
      }),
    }),
  })

  return <div>{/* èŠå¤© UI */}</div>
}
```

### 2.3 æœåŠ¡ç«¯ POST å¤„ç†å™¨

ä½¿ç”¨ `consumeSseStream` å›è°ƒåˆ›å»ºå¯æ¢å¤æµï¼š

```typescript
// app/api/chat/[id]/route.ts
import { openai } from '@ai-sdk/openai'
import { readChat, saveChat } from '@/utils/chat-store'
import {
  convertToModelMessages,
  generateId,
  streamText,
  type UIMessage,
} from 'ai'
import { after } from 'next/server'
import { createResumableStreamContext } from 'resumable-stream'

export async function POST(req: Request) {
  const { message, id }: { message: UIMessage; id: string } = await req.json()

  const chat = await readChat(id)
  const messages = [...chat.messages, message]

  // æ¸…é™¤ä¹‹å‰çš„æ´»è·ƒæµ
  saveChat({ id, messages, activeStreamId: null })

  const result = streamText({
    model: openai('gpt-4o'),
    messages: convertToModelMessages(messages),
  })

  return result.toUIMessageStreamResponse({
    originalMessages: messages,
    generateMessageId: generateId,
    onFinish: ({ messages }) => {
      // æµå®Œæˆæ—¶æ¸…é™¤æ´»è·ƒæµ ID
      saveChat({ id, messages, activeStreamId: null })
    },
    async consumeSseStream({ stream }) {
      const streamId = generateId()

      // åˆ›å»ºå¯æ¢å¤æµ
      const streamContext = createResumableStreamContext({ waitUntil: after })
      await streamContext.createNewResumableStream(streamId, () => stream)

      // è®°å½•æ´»è·ƒæµ ID
      saveChat({ id, activeStreamId: streamId })
    },
  })
}
```

### 2.4 æœåŠ¡ç«¯ GET å¤„ç†å™¨

GET ç«¯ç‚¹ç”¨äºå®¢æˆ·ç«¯é‡è¿æ—¶æ¢å¤æµï¼š

```typescript
// app/api/chat/[id]/stream/route.ts
import { readChat } from '@/utils/chat-store'
import { UI_MESSAGE_STREAM_HEADERS } from 'ai'
import { after } from 'next/server'
import { createResumableStreamContext } from 'resumable-stream'

export async function GET(
  _: Request,
  { params }: { params: Promise<{ id: string }> },
) {
  const { id } = await params
  const chat = await readChat(id)

  if (chat.activeStreamId == null) {
    // æ²¡æœ‰æ´»è·ƒçš„æµï¼Œè¿”å› 204
    return new Response(null, { status: 204 })
  }

  const streamContext = createResumableStreamContext({ waitUntil: after })

  return new Response(
    await streamContext.resumeExistingStream(chat.activeStreamId),
    { headers: UI_MESSAGE_STREAM_HEADERS },
  )
}
```

## 3. å·¥å…·è°ƒç”¨å±•ç¤º

å½“ AI è°ƒç”¨å·¥å…·æ—¶ï¼Œæ¶ˆæ¯çš„ `parts` ä¸­ä¼šå‡ºç° `tool-call` å’Œ `tool-result` ç±»å‹çš„éƒ¨åˆ†ã€‚ä½ å¯ä»¥ä¸ºä¸åŒå·¥å…·æ¸²æŸ“ä¸åŒçš„ UI ç»„ä»¶ã€‚

### 3.1 æ¸²æŸ“å·¥å…·è°ƒç”¨çŠ¶æ€

AI SDK ä¸ºå·¥å…·è°ƒç”¨çš„ part æä¾›äº† `state` å±æ€§ï¼Œåæ˜ å·¥å…·çš„æ‰§è¡Œé˜¶æ®µï¼š

```tsx
function ToolCallDisplay({ part }: { part: any }) {
  // å·¥å…·çš„ part.type æ ¼å¼ä¸º "tool-<toolName>"
  switch (part.state) {
    case 'input-available':
      // å·¥å…·æ­£åœ¨æ‰§è¡Œï¼Œå±•ç¤º loading
      return (
        <div className="flex items-center gap-2 text-gray-500">
          <span className="animate-spin">â³</span>
          æ­£åœ¨è·å–æ•°æ®...
        </div>
      )

    case 'output-available':
      // å·¥å…·æ‰§è¡Œå®Œæˆï¼Œå±•ç¤ºç»“æœ
      return <div>{JSON.stringify(part.output)}</div>

    case 'output-error':
      // å·¥å…·æ‰§è¡Œå‡ºé”™
      return (
        <div className="text-red-500">
          å·¥å…·è°ƒç”¨å¤±è´¥: {part.errorText}
        </div>
      )

    default:
      return null
  }
}
```

### 3.2 ä¸ºä¸åŒå·¥å…·æ³¨å†Œç»„ä»¶

```tsx
import { Weather } from '@/components/weather'
import { StockChart } from '@/components/stock-chart'

function MessageContent({ message }: { message: UIMessage }) {
  return (
    <div>
      {message.parts.map((part, index) => {
        // æ–‡æœ¬éƒ¨åˆ†
        if (part.type === 'text') {
          return <p key={index}>{part.text}</p>
        }

        // å¤©æ°”å·¥å…·
        if (part.type === 'tool-displayWeather') {
          switch (part.state) {
            case 'input-available':
              return <div key={index}>æ­£åœ¨æŸ¥è¯¢å¤©æ°”...</div>
            case 'output-available':
              return <Weather key={index} {...part.output} />
            case 'output-error':
              return <div key={index}>å¤©æ°”æŸ¥è¯¢å¤±è´¥</div>
            default:
              return null
          }
        }

        // è‚¡ç¥¨å·¥å…·
        if (part.type === 'tool-showStock') {
          switch (part.state) {
            case 'input-available':
              return <div key={index}>æ­£åœ¨åŠ è½½è‚¡ç¥¨æ•°æ®...</div>
            case 'output-available':
              return <StockChart key={index} data={part.output} />
            default:
              return null
          }
        }

        return null
      })}
    </div>
  )
}
```

## 4. æ¶ˆæ¯å…ƒæ•°æ®

ä½ å¯ä»¥åœ¨ `UIMessage` ä¸Šé™„åŠ è‡ªå®šä¹‰å…ƒæ•°æ®ï¼ˆå¦‚æ¶ˆæ¯æ¥æºã€ç½®ä¿¡åº¦è¯„åˆ†ç­‰ï¼‰ï¼Œå®ç°æ›´ä¸°å¯Œçš„æ¶ˆæ¯å±•ç¤ºã€‚

### 4.1 å®šä¹‰å…ƒæ•°æ® Schema

```typescript
// utils/schemas.ts
import { z } from 'zod'

export const metadataSchema = z.object({
  confidence: z.number().optional(),
  model: z.string().optional(),
  sources: z
    .array(
      z.object({
        url: z.string(),
        title: z.string(),
      }),
    )
    .optional(),
})

export type MessageMetadata = z.infer<typeof metadataSchema>
```

### 4.2 æœåŠ¡ç«¯é™„åŠ å…ƒæ•°æ®

åœ¨ `toUIMessageStreamResponse` ä¸­é€šè¿‡ `messageMetadata` é€‰é¡¹é™„åŠ ï¼š

```typescript
// app/api/chat/route.ts
const result = streamText({
  model: openai('gpt-4o'),
  messages: convertToModelMessages(messages),
})

return result.toUIMessageStreamResponse({
  messageMetadata: {
    model: 'gpt-4o',
    confidence: 0.95,
  },
})
```

### 4.3 å®¢æˆ·ç«¯è¯»å–å…ƒæ•°æ®

```tsx
function MessageWithMeta({ message }: { message: UIMessage }) {
  const metadata = message.metadata as MessageMetadata | undefined

  return (
    <div>
      {/* æ¶ˆæ¯å†…å®¹ */}
      {message.parts.map((part, i) =>
        part.type === 'text' ? <p key={i}>{part.text}</p> : null,
      )}

      {/* å…ƒæ•°æ®å±•ç¤º */}
      {metadata && message.role === 'assistant' && (
        <div className="text-xs text-gray-400 mt-2 flex gap-4">
          {metadata.model && <span>æ¨¡å‹: {metadata.model}</span>}
          {metadata.confidence && (
            <span>ç½®ä¿¡åº¦: {(metadata.confidence * 100).toFixed(0)}%</span>
          )}
        </div>
      )}
    </div>
  )
}
```

## 5. æ¶ˆæ¯éªŒè¯

å½“èŠå¤©å†å²ä¸­åŒ…å«å·¥å…·è°ƒç”¨ã€è‡ªå®šä¹‰æ•°æ®éƒ¨åˆ†æˆ–å…ƒæ•°æ®æ—¶ï¼Œä»æ•°æ®åº“åŠ è½½åéœ€è¦éªŒè¯å…¶ç»“æ„å®Œæ•´æ€§ï¼š

```typescript
import { validateUIMessages } from 'ai'

const validatedMessages = await validateUIMessages({
  messages,
  tools,           // ç¡®ä¿å·¥å…·è°ƒç”¨åŒ¹é…å½“å‰ schema
  metadataSchema,  // éªŒè¯å…ƒæ•°æ®ç»“æ„
  dataSchemas,     // éªŒè¯è‡ªå®šä¹‰æ•°æ®éƒ¨åˆ†
})
```

::: tip AI æ¦‚å¿µè¯´æ˜
**æ¶ˆæ¯éªŒè¯** æ˜¯ç”Ÿäº§ç¯å¢ƒçš„å…³é”®ç¯èŠ‚ã€‚å·¥å…·å®šä¹‰å¯èƒ½éšç‰ˆæœ¬è¿­ä»£è€Œå˜åŒ–ï¼ˆå‚æ•°æ–°å¢/åˆ é™¤ï¼‰ï¼Œ`validateUIMessages` ç¡®ä¿å†å²æ¶ˆæ¯ä¸­çš„å·¥å…·è°ƒç”¨ä»ç„¶ç¬¦åˆå½“å‰çš„ Schemaï¼Œé¿å…è¿è¡Œæ—¶é”™è¯¯ã€‚
:::

## ä¸‹ä¸€æ­¥

- [ç”Ÿæˆå¼ UI](/ai/vercel-ai-sdk/guide/generative-ui) â€” è®© LLM å†³å®šæ¸²æŸ“ UI ç»„ä»¶
- [æµå¼è‡ªå®šä¹‰æ•°æ®](/ai/vercel-ai-sdk/guide/streaming-data) â€” åœ¨ AI å“åº”ä¸­ä¼ è¾“è‡ªå®šä¹‰ç»“æ„åŒ–æ•°æ®
- [æµåè®®è¯¦è§£](/ai/vercel-ai-sdk/guide/stream-protocol) â€” ç†è§£åº•å±‚ UIMessage Stream åè®®
