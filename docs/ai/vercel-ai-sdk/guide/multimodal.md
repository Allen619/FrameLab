---
title: å¤šæ¨¡æ€
description: å­¦ä¹ å¦‚ä½•ä½¿ç”¨ AI SDK å¤„ç†å¤šæ¨¡æ€å†…å®¹ï¼ŒåŒ…æ‹¬å›¾ç‰‡ç”Ÿæˆã€è¯­éŸ³åˆæˆã€è¯­éŸ³è½¬æ–‡å­—ï¼Œä»¥åŠåœ¨æç¤ºè¯ä¸­å‘é€å›¾ç‰‡
---

# å¤šæ¨¡æ€

## æ¦‚è¿°

å¤šæ¨¡æ€ï¼ˆMultimodalï¼‰æ˜¯æŒ‡ AI æ¨¡å‹èƒ½å¤ŸåŒæ—¶å¤„ç†å’Œç”Ÿæˆå¤šç§ç±»å‹çš„åª’ä½“â€”â€”æ–‡æœ¬ã€å›¾ç‰‡ã€éŸ³é¢‘ç­‰ã€‚AI SDK æä¾›äº†ä¸€æ•´å¥—å¤šæ¨¡æ€ APIï¼š`generateImage` ç”¨äºç”Ÿæˆå›¾ç‰‡ï¼Œ`generateSpeech` ç”¨äºæ–‡æœ¬è½¬è¯­éŸ³ï¼Œ`transcribe` ç”¨äºè¯­éŸ³è½¬æ–‡å­—ï¼ŒåŒæ—¶è¿˜æ”¯æŒåœ¨å¯¹è¯æ¶ˆæ¯ä¸­å‘é€å›¾ç‰‡å’ŒéŸ³é¢‘è®©æ¨¡å‹ç†è§£åˆ†æã€‚

::: tip å‰ç«¯ç±»æ¯”
å¤šæ¨¡æ€ API ç±»ä¼¼äºå‰ç«¯ä¸­çš„ **Media API å®¶æ—**ã€‚å°±åƒæµè§ˆå™¨æä¾› `Canvas API` å¤„ç†å›¾ç‰‡ã€`Web Audio API` å¤„ç†éŸ³é¢‘ã€`MediaRecorder` å½•åˆ¶åª’ä½“ä¸€æ ·ï¼ŒAI SDK é€šè¿‡ç»Ÿä¸€çš„å‡½æ•°ç­¾åæä¾›äº†å¯¹å›¾ç‰‡ã€è¯­éŸ³çš„ç”Ÿæˆå’Œåˆ†æèƒ½åŠ›ã€‚æ¯ä¸ªå‡½æ•°éƒ½éµå¾ªç›¸åŒçš„æ¨¡å¼ï¼šä¼ å…¥ `model` + è¾“å…¥å†…å®¹ï¼Œè¿”å›ç»“æœã€‚

**AI SDK åŸç”Ÿè¯­ä¹‰**ï¼š`generateSpeech` å’Œ `transcribe` ç›®å‰è¿˜æ˜¯å®éªŒæ€§ APIï¼ˆéœ€è¦é€šè¿‡ `experimental_generateSpeech` å’Œ `experimental_transcribe` å¯¼å…¥ï¼‰ï¼Œæ¥å£å¯èƒ½åœ¨æœªæ¥ç‰ˆæœ¬ä¸­è°ƒæ•´ã€‚`generateImage` å·²ç»æ˜¯ç¨³å®š APIã€‚
:::

## å›¾ç‰‡ç”Ÿæˆï¼šgenerateImage

ä½¿ç”¨ `generateImage` æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆå›¾ç‰‡ï¼š

[ğŸ”— generateImage API å‚è€ƒ](https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-image){target="_blank" rel="noopener"}

```typescript
import { generateImage } from 'ai'
import { openai } from '@ai-sdk/openai'

const { images } = await generateImage({
  model: openai.image('dall-e-3'),
  prompt: 'ä¸€åªæˆ´ç€ç¨‹åºå‘˜çœ¼é•œåœ¨ç”µè„‘å‰å†™ä»£ç çš„çŒ«ï¼Œåƒç´ é£æ ¼',
  size: '1024x1024',
})

// images æ˜¯ä¸€ä¸ªæ•°ç»„ï¼ŒåŒ…å«ç”Ÿæˆçš„å›¾ç‰‡æ•°æ®
const image = images[0]
console.log(image) // åŒ…å«å›¾ç‰‡çš„ base64 æ•°æ®æˆ– URL
```

### ç”Ÿæˆå¤šå¼ å›¾ç‰‡

```typescript
import { generateImage } from 'ai'
import { openai } from '@ai-sdk/openai'

const { images } = await generateImage({
  model: openai.image('dall-e-3'),
  prompt: 'æ—¥è½æ—¶åˆ†çš„åŸå¸‚å¤©é™…çº¿ï¼Œèµ›åšæœ‹å…‹é£æ ¼',
  n: 3, // ç”Ÿæˆ 3 å¼ 
  size: '1024x1024',
})

images.forEach((image, index) => {
  console.log(`å›¾ç‰‡ ${index + 1} å·²ç”Ÿæˆ`)
})
```

### å¸¸ç”¨å‚æ•°

| å‚æ•° | ç±»å‹ | è¯´æ˜ |
| --- | --- | --- |
| `model` | `ImageModelV3` | å›¾ç‰‡æ¨¡å‹ï¼Œå¦‚ `openai.image('dall-e-3')` |
| `prompt` | `string` | å›¾ç‰‡æè¿°æç¤ºè¯ |
| `n` | `number` | ç”Ÿæˆå›¾ç‰‡æ•°é‡ |
| `size` | `string` | å›¾ç‰‡å°ºå¯¸ï¼Œå¦‚ `'1024x1024'` |
| `aspectRatio` | `string` | å®½é«˜æ¯”ï¼Œå¦‚ `'16:9'` |
| `seed` | `number` | éšæœºç§å­ï¼Œç”¨äºå¯å¤ç°çš„ç”Ÿæˆ |

## è¯­éŸ³åˆæˆï¼šgenerateSpeech

ä½¿ç”¨ `generateSpeech`ï¼ˆå®éªŒæ€§ APIï¼‰å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯­éŸ³éŸ³é¢‘ï¼š

[ğŸ”— generateSpeech API å‚è€ƒ](https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-speech){target="_blank" rel="noopener"}

```typescript
import { experimental_generateSpeech as generateSpeech } from 'ai'
import { openai } from '@ai-sdk/openai'

const { audio } = await generateSpeech({
  model: openai.speech('tts-1'),
  text: 'ä½ å¥½ï¼Œæ¬¢è¿ä½¿ç”¨ AI SDKï¼è¿™æ˜¯ä¸€æ®µç”± AI ç”Ÿæˆçš„è¯­éŸ³ã€‚',
  voice: 'alloy',
})

// audio æ˜¯ Uint8Array æ ¼å¼çš„éŸ³é¢‘æ•°æ®
console.log(`éŸ³é¢‘å¤§å°: ${audio.length} bytes`)
```

### å¯ç”¨å£°éŸ³å’Œå‚æ•°

```typescript
import { experimental_generateSpeech as generateSpeech } from 'ai'
import { openai } from '@ai-sdk/openai'
import { writeFile } from 'fs/promises'

const { audio } = await generateSpeech({
  model: openai.speech('tts-1'),
  text: 'è¿™æ˜¯ä¸€æ®µæ¼”ç¤ºæ–‡æœ¬ã€‚',
  voice: 'nova', // å¯é€‰: alloy, echo, fable, onyx, nova, shimmer
  outputFormat: 'mp3', // è¾“å‡ºæ ¼å¼: mp3, wav ç­‰
  speed: 1.0, // è¯­é€Ÿ: 0.25 - 4.0
})

// ä¿å­˜ä¸ºéŸ³é¢‘æ–‡ä»¶
await writeFile('output.mp3', Buffer.from(audio))
console.log('éŸ³é¢‘å·²ä¿å­˜ä¸º output.mp3')
```

## è¯­éŸ³è½¬æ–‡å­—ï¼štranscribe

ä½¿ç”¨ `transcribe`ï¼ˆå®éªŒæ€§ APIï¼‰å°†éŸ³é¢‘æ–‡ä»¶è½¬æ¢ä¸ºæ–‡æœ¬ï¼š

[ğŸ”— transcribe API å‚è€ƒ](https://ai-sdk.dev/docs/reference/ai-sdk-core/transcribe){target="_blank" rel="noopener"}

```typescript
import { experimental_transcribe as transcribe } from 'ai'
import { openai } from '@ai-sdk/openai'
import { readFile } from 'fs/promises'

const { text, segments, language, durationInSeconds } = await transcribe({
  model: openai.transcription('whisper-1'),
  audio: await readFile('meeting-recording.mp3'),
})

console.log(`è¯­è¨€: ${language}`) // "zh"
console.log(`æ—¶é•¿: ${durationInSeconds} ç§’`)
console.log(`è½¬å½•æ–‡æœ¬: ${text}`)

// segments åŒ…å«å¸¦æ—¶é—´æˆ³çš„åˆ†æ®µä¿¡æ¯
segments?.forEach((segment) => {
  console.log(
    `[${segment.startSecond}s - ${segment.endSecond}s] ${segment.text}`
  )
})
```

### ä» URL è½¬å½•

```typescript
import { experimental_transcribe as transcribe } from 'ai'
import { openai } from '@ai-sdk/openai'

const { text } = await transcribe({
  model: openai.transcription('whisper-1'),
  audio: new URL('https://example.com/audio/recording.mp3'),
})

console.log(text)
```

## å¤šæ¨¡æ€æç¤ºï¼šåœ¨æ¶ˆæ¯ä¸­å‘é€å›¾ç‰‡

è®¸å¤šæ¨¡å‹ï¼ˆå¦‚ GPT-4oã€Claude 3.5ï¼‰æ”¯æŒç›´æ¥åœ¨å¯¹è¯ä¸­å‘é€å›¾ç‰‡ï¼Œè®©æ¨¡å‹"çœ‹æ‡‚"å›¾ç‰‡å†…å®¹ï¼š

### é€šè¿‡ URL å‘é€å›¾ç‰‡

```typescript
import { generateText } from 'ai'
import { openai } from '@ai-sdk/openai'

const { text } = await generateText({
  model: openai('gpt-4o'),
  messages: [
    {
      role: 'user',
      content: [
        { type: 'text', text: 'æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚' },
        {
          type: 'file',
          mediaType: 'image/png',
          url: 'https://example.com/photo.png',
        },
      ],
    },
  ],
})

console.log(text) // æ¨¡å‹å¯¹å›¾ç‰‡å†…å®¹çš„æè¿°
```

### é€šè¿‡ Base64 å‘é€å›¾ç‰‡

```typescript
import { generateText } from 'ai'
import { openai } from '@ai-sdk/openai'
import { readFileSync } from 'fs'

const imageBuffer = readFileSync('./screenshot.png')

const { text } = await generateText({
  model: openai('gpt-4o'),
  messages: [
    {
      role: 'user',
      content: [
        { type: 'text', text: 'è¿™ä¸ªæˆªå›¾ä¸­æœ‰ä»€ä¹ˆ UI é—®é¢˜ï¼Ÿ' },
        {
          type: 'file',
          mediaType: 'image/png',
          data: imageBuffer,
        },
      ],
    },
  ],
})

console.log(text)
```

### å‘é€éŸ³é¢‘æ–‡ä»¶

éƒ¨åˆ†æ¨¡å‹ï¼ˆå¦‚ `gpt-4o-audio-preview`ï¼‰è¿˜æ”¯æŒç›´æ¥åˆ†æéŸ³é¢‘å†…å®¹ï¼š

```typescript
import { generateText } from 'ai'
import { openai } from '@ai-sdk/openai'
import { readFileSync } from 'fs'

const { text } = await generateText({
  model: openai('gpt-4o-audio-preview'),
  messages: [
    {
      role: 'user',
      content: [
        { type: 'text', text: 'è¿™æ®µéŸ³é¢‘åœ¨è¯´ä»€ä¹ˆï¼Ÿ' },
        {
          type: 'file',
          mediaType: 'audio/mpeg',
          data: readFileSync('./recording.mp3'),
        },
      ],
    },
  ],
})

console.log(text)
```

## å¤šæ¨¡æ€èƒ½åŠ›ä¸€è§ˆ

| èƒ½åŠ› | å‡½æ•° | è¾“å…¥ | è¾“å‡º | çŠ¶æ€ |
| --- | --- | --- | --- | --- |
| æ–‡æœ¬ â†’ å›¾ç‰‡ | `generateImage` | æ–‡æœ¬æç¤º | å›¾ç‰‡æ•°æ® | ç¨³å®š |
| æ–‡æœ¬ â†’ è¯­éŸ³ | `generateSpeech` | æ–‡æœ¬ | éŸ³é¢‘æ•°æ® | å®éªŒæ€§ |
| è¯­éŸ³ â†’ æ–‡æœ¬ | `transcribe` | éŸ³é¢‘æ–‡ä»¶ | æ–‡æœ¬ + æ—¶é—´æˆ³ | å®éªŒæ€§ |
| å›¾ç‰‡ â†’ æ–‡æœ¬ | `generateText` (å¤šæ¨¡æ€æ¶ˆæ¯) | å›¾ç‰‡ + æç¤º | æ–‡æœ¬æè¿° | ç¨³å®š |
| éŸ³é¢‘ â†’ æ–‡æœ¬ | `generateText` (å¤šæ¨¡æ€æ¶ˆæ¯) | éŸ³é¢‘ + æç¤º | æ–‡æœ¬åˆ†æ | ç¨³å®š |

## åœ¨ Next.js ä¸­å¤„ç†å¤šæ¨¡æ€

### å›¾ç‰‡åˆ†æèŠå¤©ç•Œé¢

å‰ç«¯ç»„ä»¶å¯ä»¥é€šè¿‡ `useChat` å‘é€åŒ…å«å›¾ç‰‡çš„æ¶ˆæ¯ï¼š

```typescript
// app/api/chat/route.ts
import { streamText } from 'ai'
import { openai } from '@ai-sdk/openai'

export async function POST(req: Request) {
  const { messages } = await req.json()

  const result = streamText({
    model: openai('gpt-4o'),
    messages,
  })

  return result.toUIMessageStreamResponse()
}
```

```typescript
// app/page.tsx
'use client'

import { useChat } from '@ai-sdk/react'
import { useState } from 'react'

export default function Chat() {
  const [imageUrl, setImageUrl] = useState('')
  const { messages, input, setInput, sendMessage } = useChat()

  const handleSubmit = (e: React.FormEvent) => {
    e.preventDefault()
    sendMessage({
      role: 'user',
      parts: [
        ...(imageUrl
          ? [{ type: 'file' as const, mediaType: 'image/png', url: imageUrl }]
          : []),
        { type: 'text' as const, text: input },
      ],
    })
    setInput('')
    setImageUrl('')
  }

  return (
    <div>
      {messages.map((m) => (
        <div key={m.id}>
          <strong>{m.role === 'user' ? 'ä½ ' : 'AI'}:</strong>
          {m.parts.map((part, i) => {
            if (part.type === 'text') return <p key={i}>{part.text}</p>
            if (part.type === 'file')
              return <img key={i} src={part.url} alt="uploaded" />
            return null
          })}
        </div>
      ))}
      <form onSubmit={handleSubmit}>
        <input
          value={imageUrl}
          onChange={(e) => setImageUrl(e.target.value)}
          placeholder="å›¾ç‰‡ URLï¼ˆå¯é€‰ï¼‰"
        />
        <input
          value={input}
          onChange={(e) => setInput(e.target.value)}
          placeholder="æè¿°ä½ æƒ³äº†è§£çš„å†…å®¹..."
        />
        <button type="submit">å‘é€</button>
      </form>
    </div>
  )
}
```

## æœ€ä½³å®è·µ

1. **å›¾ç‰‡æç¤ºè¯è¦å…·ä½“**ï¼š`generateImage` çš„æç¤ºè¯è¶Šè¯¦ç»†ï¼Œç”Ÿæˆæ•ˆæœè¶Šå¥½ã€‚åŒ…å«é£æ ¼ã€è‰²è°ƒã€æ„å›¾ç­‰æè¿°
2. **éŸ³é¢‘æ ¼å¼åŒ¹é…**ï¼šä¸åŒæ¨¡å‹æ”¯æŒçš„éŸ³é¢‘æ ¼å¼ä¸åŒï¼ŒOpenAI Whisper æ”¯æŒ mp3ã€mp4ã€wav ç­‰
3. **å›¾ç‰‡å¤§å°æ§åˆ¶**ï¼šå‘é€å›¾ç‰‡ç»™æ¨¡å‹æ—¶æ³¨æ„æ–‡ä»¶å¤§å°ï¼Œè¿‡å¤§çš„å›¾ç‰‡ä¼šå¢åŠ  token æ¶ˆè€—å’Œå»¶è¿Ÿ
4. **å®éªŒæ€§ API æ³¨æ„äº‹é¡¹**ï¼š`generateSpeech` å’Œ `transcribe` æ˜¯å®éªŒæ€§ APIï¼Œéœ€è¦é€šè¿‡ `experimental_` å‰ç¼€å¯¼å…¥ï¼Œæ¥å£å¯èƒ½åœ¨æœªæ¥ç‰ˆæœ¬å˜æ›´
5. **å¤šæ¨¡æ€æ¶ˆæ¯çš„ content æ ¼å¼**ï¼šå‘é€å¤šæ¨¡æ€æ¶ˆæ¯æ—¶ï¼Œ`content` å¿…é¡»æ˜¯æ•°ç»„æ ¼å¼ï¼ˆåŒ…å« `type: 'text'` å’Œ `type: 'file'` ç­‰ partï¼‰

## ä¸‹ä¸€æ­¥

- [æ–‡æœ¬ç”Ÿæˆ](/ai/vercel-ai-sdk/guide/generating-text) â€” å›é¡¾æ–‡æœ¬ç”Ÿæˆçš„åŸºç¡€ç”¨æ³•
- [å·¥å…·è°ƒç”¨](/ai/vercel-ai-sdk/guide/tool-calling) â€” å°†å¤šæ¨¡æ€èƒ½åŠ›å°è£…ä¸ºå·¥å…·ä¾›æ¨¡å‹è°ƒç”¨
- [å‘é‡åµŒå…¥](/ai/vercel-ai-sdk/guide/embeddings) â€” å­¦ä¹ è¯­ä¹‰æœç´¢å’Œ RAG æ£€ç´¢
