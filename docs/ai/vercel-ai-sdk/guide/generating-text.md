---
title: æ–‡æœ¬ç”Ÿæˆ
description: å­¦ä¹ å¦‚ä½•ä½¿ç”¨ AI SDK çš„ generateText å’Œ streamText å‡½æ•°è¿›è¡Œä¸€æ¬¡æ€§æ–‡æœ¬ç”Ÿæˆä¸æµå¼æ–‡æœ¬ç”Ÿæˆ
---

# æ–‡æœ¬ç”Ÿæˆ

## æ¦‚è¿°

æ–‡æœ¬ç”Ÿæˆæ˜¯ AI åº”ç”¨æœ€æ ¸å¿ƒçš„èƒ½åŠ›ã€‚AI SDK æä¾›ä¸¤ä¸ªæ ¸å¿ƒå‡½æ•°æ¥å®ç°æ–‡æœ¬ç”Ÿæˆï¼š`generateText` ç”¨äºä¸€æ¬¡æ€§ç”Ÿæˆå®Œæ•´æ–‡æœ¬ï¼Œ`streamText` ç”¨äºæµå¼é€å­—è¾“å‡ºã€‚ä¸¤è€…å…±äº«ç›¸åŒçš„å‚æ•°æ¥å£ï¼Œä½†åœ¨å“åº”æ¨¡å¼ä¸Šæœ‰æœ¬è´¨åŒºåˆ«ã€‚

[ğŸ”— generateText API å‚è€ƒ](https://ai-sdk.dev/docs/reference/ai-sdk-core/generate-text){target="_blank" rel="noopener"} | [ğŸ”— streamText API å‚è€ƒ](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text){target="_blank" rel="noopener"}

::: tip å‰ç«¯ç±»æ¯”
`generateText` ç±»ä¼¼äº `fetch` ç­‰å¾…å®Œæ•´å“åº”åè¿”å›ï¼Œè€Œ `streamText` ç±»ä¼¼äº `ReadableStream` çš„æµå¼æ¶ˆè´¹æ¨¡å¼ã€‚å¦‚æœä½ ç”¨è¿‡ `EventSource` æˆ– Server-Sent Eventsï¼Œé‚£ä¹ˆ `streamText` çš„ä½“éªŒå‡ ä¹ä¸€æ ·â€”â€”æ•°æ®åˆ°è¾¾ä¸€éƒ¨åˆ†å°±æ¸²æŸ“ä¸€éƒ¨åˆ†ï¼Œç”¨æˆ·æ— éœ€ç­‰å¾…å…¨éƒ¨å†…å®¹ç”Ÿæˆå®Œæ¯•ã€‚

**AI SDK åŸç”Ÿè¯­ä¹‰**ï¼š`streamText` è¿”å›çš„ `textStream` åŒæ—¶å®ç°äº† `AsyncIterable` å’Œ `ReadableStream` æ¥å£ï¼Œä½ å¯ä»¥ç”¨ `for await...of` è¿­ä»£ï¼Œä¹Ÿå¯ä»¥ç›´æ¥å½“ä½œ Web Stream ä½¿ç”¨ã€‚
:::

## æ ¸å¿ƒå‚æ•°

`generateText` å’Œ `streamText` å…±äº«ä»¥ä¸‹å…³é”®å‚æ•°ï¼š

| å‚æ•° | ç±»å‹ | è¯´æ˜ |
| --- | --- | --- |
| `model` | `LanguageModel` | ä½¿ç”¨çš„æ¨¡å‹ï¼Œå¦‚ `openai('gpt-4o')` |
| `prompt` | `string` | ç®€å•æ–‡æœ¬æç¤ºï¼ˆä¸ `messages` äºŒé€‰ä¸€ï¼‰ |
| `messages` | `ModelMessage[]` | å¤šè½®å¯¹è¯æ¶ˆæ¯æ•°ç»„ï¼ˆä¸ `prompt` äºŒé€‰ä¸€ï¼‰ |
| `system` | `string` | ç³»ç»Ÿæç¤ºè¯ï¼Œå®šä¹‰ AI çš„è§’è‰²å’Œè¡Œä¸ºçº¦æŸ |
| `maxTokens` | `number` | æœ€å¤§ç”Ÿæˆ token æ•°é‡ |
| `temperature` | `number` | æ¸©åº¦å‚æ•°ï¼ˆ0-2ï¼‰ï¼Œè¶Šé«˜è¾“å‡ºè¶Šéšæœº |

## generateTextï¼šä¸€æ¬¡æ€§ç”Ÿæˆ

é€‚ç”¨äºåç«¯å¤„ç†ã€æ‰¹é‡ä»»åŠ¡ã€ä¸éœ€è¦å®æ—¶å±•ç¤ºçš„åœºæ™¯ï¼š

```typescript
import { generateText } from 'ai'
import { openai } from '@ai-sdk/openai'

const { text, usage, finishReason } = await generateText({
  model: openai('gpt-4o'),
  system: 'ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å‰ç«¯æŠ€æœ¯ä¸“å®¶ã€‚',
  prompt: 'è§£é‡Š React Server Components çš„æ ¸å¿ƒåŸç†ã€‚',
  maxTokens: 1000,
  temperature: 0.7,
})

console.log(text) // å®Œæ•´çš„ç”Ÿæˆæ–‡æœ¬
console.log(usage) // { promptTokens: 23, completionTokens: 456, totalTokens: 479 }
console.log(finishReason) // 'stop' | 'length' | 'tool-calls' | ...
```

### ä½¿ç”¨ messages è¿›è¡Œå¤šè½®å¯¹è¯

```typescript
import { generateText } from 'ai'
import { openai } from '@ai-sdk/openai'

const { text } = await generateText({
  model: openai('gpt-4o'),
  system: 'ä½ æ˜¯ä¸€ä½ TypeScript ç¼–ç¨‹åŠ©æ‰‹ã€‚',
  messages: [
    { role: 'user', content: 'ä»€ä¹ˆæ˜¯æ³›å‹ï¼Ÿ' },
    {
      role: 'assistant',
      content: 'æ³›å‹æ˜¯ TypeScript ä¸­å®ç°ç±»å‹å‚æ•°åŒ–çš„æœºåˆ¶â€¦â€¦',
    },
    { role: 'user', content: 'èƒ½ä¸¾ä¸€ä¸ªå®é™…çš„ä½¿ç”¨ä¾‹å­å—ï¼Ÿ' },
  ],
})
```

## streamTextï¼šæµå¼ç”Ÿæˆ

é€‚ç”¨äºèŠå¤©ç•Œé¢ã€å®æ—¶äº¤äº’ç­‰éœ€è¦é€æ­¥å±•ç¤ºå†…å®¹çš„åœºæ™¯ï¼š

```typescript
import { streamText } from 'ai'
import { openai } from '@ai-sdk/openai'

const result = streamText({
  model: openai('gpt-4o'),
  prompt: 'å‘æ˜ä¸€ä¸ªæ–°èŠ‚æ—¥å¹¶æè¿°å®ƒçš„ä¼ ç»Ÿã€‚',
})

// æ–¹å¼ 1ï¼šä½¿ç”¨ async iterable è¿­ä»£
for await (const textPart of result.textStream) {
  process.stdout.write(textPart)
}
```

### åœ¨ Next.js API è·¯ç”±ä¸­ä½¿ç”¨

`streamText` å¯ä»¥ç›´æ¥è½¬æ¢ä¸ºå¤šç§å“åº”æ ¼å¼ï¼Œé€‚é…ä¸åŒçš„å‰ç«¯æ¶ˆè´¹æ–¹å¼ï¼š

```typescript
import { streamText } from 'ai'
import { openai } from '@ai-sdk/openai'

export async function POST(req: Request) {
  const { prompt }: { prompt: string } = await req.json()

  const result = streamText({
    model: openai('gpt-4o'),
    system: 'ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚',
    prompt,
  })

  // è½¬æ¢ä¸º UI æ¶ˆæ¯æµå“åº”ï¼Œå‰ç«¯å¯ç”¨ useChat æ¶ˆè´¹
  return result.toUIMessageStreamResponse()
}
```

### è·å–æµå¼ç”Ÿæˆçš„å…ƒæ•°æ®

æµå¼ç”Ÿæˆå®Œæˆåï¼Œå¯ä»¥é€šè¿‡ `await` è·å–å…ƒæ•°æ®ï¼š

```typescript
const result = streamText({
  model: openai('gpt-4o'),
  prompt: 'å†™ä¸€é¦–å…³äºç¼–ç¨‹çš„è¯—ã€‚',
})

// å…ˆæ¶ˆè´¹æµ
for await (const textPart of result.textStream) {
  process.stdout.write(textPart)
}

// æµç»“æŸåè·å–å…ƒæ•°æ®
console.log('\nUsage:', await result.usage)
console.log('Finish reason:', await result.finishReason)
```

## å¤šæ­¥ç”Ÿæˆï¼ˆMulti-stepï¼‰

é€šè¿‡ `stopWhen` é…åˆ `stepCountIs`ï¼Œå¯ä»¥è®©æ¨¡å‹åœ¨å¤šä¸ªæ­¥éª¤ä¸­å®Œæˆå¤æ‚ä»»åŠ¡ã€‚æœ€å¸¸è§çš„åœºæ™¯æ˜¯ç»“åˆå·¥å…·è°ƒç”¨â€”â€”æ¨¡å‹å…ˆè°ƒç”¨å·¥å…·è·å–æ•°æ®ï¼Œå†åŸºäºå·¥å…·ç»“æœç”Ÿæˆæœ€ç»ˆå›ç­”ï¼š

```typescript
import { generateText, tool, stepCountIs } from 'ai'
import { openai } from '@ai-sdk/openai'
import { z } from 'zod'

const { text, steps } = await generateText({
  model: openai('gpt-4o'),
  tools: {
    weather: tool({
      description: 'è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯',
      inputSchema: z.object({
        location: z.string().describe('åŸå¸‚åç§°'),
      }),
      execute: async ({ location }) => ({
        location,
        temperature: 72 + Math.floor(Math.random() * 21) - 10,
        condition: 'æ™´å¤©',
      }),
    }),
  },
  stopWhen: stepCountIs(5), // æœ€å¤šæ‰§è¡Œ 5 æ­¥
  prompt: 'ä¸Šæµ·ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ',
})

// text: æ¨¡å‹åŸºäºå·¥å…·ç»“æœç”Ÿæˆçš„æœ€ç»ˆå›ç­”
// steps: æ¯ä¸€æ­¥çš„è¯¦ç»†ä¿¡æ¯ï¼ˆå·¥å…·è°ƒç”¨ã€ä¸­é—´ç»“æœç­‰ï¼‰
console.log(text)
console.log(`æ€»å…±æ‰§è¡Œäº† ${steps.length} æ­¥`)
```

### å¤šæ­¥ç”Ÿæˆæµç¨‹

```mermaid
sequenceDiagram
    participant U as ç”¨æˆ·
    participant S as AI SDK
    participant M as LLM
    participant T as å·¥å…·

    U->>S: prompt + tools + stopWhen
    loop æ¯ä¸€æ­¥ï¼ˆç›´åˆ° stopWhen æ»¡è¶³ï¼‰
        S->>M: å‘é€æ¶ˆæ¯
        M->>S: è¿”å›æ–‡æœ¬æˆ–å·¥å…·è°ƒç”¨
        alt æ¨¡å‹è°ƒç”¨å·¥å…·
            S->>T: æ‰§è¡Œå·¥å…·
            T->>S: è¿”å›ç»“æœ
            Note over S: å°†ç»“æœåŠ å…¥æ¶ˆæ¯ï¼Œç»§ç»­ä¸‹ä¸€æ­¥
        else æ¨¡å‹è¿”å›æ–‡æœ¬
            Note over S: ç”Ÿæˆå®Œæˆï¼Œé€€å‡ºå¾ªç¯
        end
    end
    S->>U: è¿”å›æœ€ç»ˆæ–‡æœ¬ + æ­¥éª¤è¯¦æƒ…
```

`streamText` åŒæ ·æ”¯æŒå¤šæ­¥ç”Ÿæˆï¼š

```typescript
import { streamText, tool, stepCountIs } from 'ai'
import { openai } from '@ai-sdk/openai'
import { z } from 'zod'

const result = streamText({
  model: openai('gpt-4o'),
  tools: {
    weather: tool({
      description: 'è·å–å¤©æ°”ä¿¡æ¯',
      inputSchema: z.object({
        location: z.string().describe('åŸå¸‚åç§°'),
      }),
      execute: async ({ location }) => ({
        location,
        temperature: 25,
      }),
    }),
  },
  stopWhen: stepCountIs(5),
  prompt: 'åŒ—äº¬å’Œä¸Šæµ·ä»Šå¤©å“ªä¸ªæ›´çƒ­ï¼Ÿ',
})

for await (const textPart of result.textStream) {
  process.stdout.write(textPart)
}
```

## æœ€ä½³å®è·µ

### é€‰æ‹© generateText è¿˜æ˜¯ streamText

| åœºæ™¯ | æ¨èå‡½æ•° | åŸå›  |
| --- | --- | --- |
| èŠå¤©ç•Œé¢ | `streamText` | ç”¨æˆ·å¯ä»¥å®æ—¶çœ‹åˆ°ç”Ÿæˆè¿‡ç¨‹ï¼Œä½“éªŒæ›´å¥½ |
| åç«¯æ•°æ®å¤„ç† | `generateText` | æ— éœ€æµå¼è¾“å‡ºï¼Œä»£ç æ›´ç®€å• |
| API ç½‘å…³ | `streamText` | é¿å…é•¿æ—¶é—´ç­‰å¾…å¯¼è‡´è¶…æ—¶ |
| æ‰¹é‡ä»»åŠ¡ | `generateText` | é€»è¾‘ç®€å•ï¼Œä¾¿äºé”™è¯¯å¤„ç† |

### æç¤ºè¯æŠ€å·§

- **system** ç”¨äºå®šä¹‰è§’è‰²å’Œé€šç”¨è§„åˆ™ï¼Œåœ¨å¤šè½®å¯¹è¯ä¸­å§‹ç»ˆç”Ÿæ•ˆ
- **temperature** è®¾ä¸º 0 è·å–ç¡®å®šæ€§è¾“å‡ºï¼Œè®¾ä¸º 0.7-1.0 è·å–åˆ›æ„æ€§è¾“å‡º
- **maxTokens** å»ºè®®å§‹ç»ˆè®¾ç½®ä¸Šé™ï¼Œé¿å…æ„å¤–çš„é•¿è¾“å‡ºæ¶ˆè€—è¿‡å¤š token

## ä¸‹ä¸€æ­¥

- [ç»“æ„åŒ–è¾“å‡º](/ai/vercel-ai-sdk/guide/structured-output) â€” è®©æ¨¡å‹è¿”å›ç¬¦åˆ Schema çš„ JSON å¯¹è±¡
- [å·¥å…·è°ƒç”¨](/ai/vercel-ai-sdk/guide/tool-calling) â€” è®©æ¨¡å‹è°ƒç”¨å¤–éƒ¨å‡½æ•°è·å–å®æ—¶æ•°æ®
- [å¤šæ¨¡æ€](/ai/vercel-ai-sdk/guide/multimodal) â€” å¤„ç†å›¾ç‰‡ã€è¯­éŸ³ç­‰å¤šç§åª’ä½“ç±»å‹
