---
title: ä¸­é—´ä»¶ç³»ç»Ÿ
description: æŒæ¡ AI SDK çš„ Language Model Middleware æœºåˆ¶ï¼Œå­¦ä¹ ä½¿ç”¨å†…ç½®ä¸­é—´ä»¶å’Œç¼–å†™è‡ªå®šä¹‰ä¸­é—´ä»¶æ¥å¢å¼ºæ¨¡å‹èƒ½åŠ›ã€‚
---

# ä¸­é—´ä»¶ç³»ç»Ÿ

AI SDK çš„ä¸­é—´ä»¶ç³»ç»Ÿå…è®¸ä½ åœ¨ä¸ä¿®æ”¹æ¨¡å‹è°ƒç”¨ä»£ç çš„æƒ…å†µä¸‹ï¼Œæ‹¦æˆªå’Œå¢å¼ºæ¨¡å‹çš„è¾“å…¥è¾“å‡ºã€‚è¿™æ˜¯ä¸€ç§å¼ºå¤§çš„æ¨ªåˆ‡å…³æ³¨ç‚¹ï¼ˆcross-cutting concernï¼‰å¤„ç†æ–¹å¼ã€‚

[ğŸ”— AI SDK ä¸­é—´ä»¶å®˜æ–¹æ–‡æ¡£](https://ai-sdk.dev/docs/ai-sdk-core/middleware){target="_blank" rel="noopener"}

## æ ¸å¿ƒæ¦‚å¿µ

::: tip å‰ç«¯ç±»æ¯”
AI SDK çš„ä¸­é—´ä»¶ä¸ Express/Koa ä¸­é—´ä»¶æˆ– Redux Middleware éå¸¸ç±»ä¼¼ã€‚å®ƒå¯ä»¥æ‹¦æˆªè¯·æ±‚ï¼ˆæ¨¡å‹è°ƒç”¨å‚æ•°ï¼‰ï¼Œä¿®æ”¹å“åº”ï¼ˆæ¨¡å‹è¾“å‡ºï¼‰ï¼Œç”šè‡³çŸ­è·¯æ‰§è¡Œï¼ˆè¿”å›ç¼“å­˜ç»“æœï¼‰ã€‚
:::

ä¸­é—´ä»¶é€šè¿‡ `wrapLanguageModel` å‡½æ•°åŒ…è£¹ä¸€ä¸ªæ¨¡å‹ï¼Œè¿”å›ä¸€ä¸ªå¢å¼ºåçš„æ–°æ¨¡å‹ï¼š

```typescript
import { wrapLanguageModel } from 'ai'

const enhancedModel = wrapLanguageModel({
  model: yourModel,
  middleware: yourMiddleware,
})

// ä½¿ç”¨å¢å¼ºåçš„æ¨¡å‹ï¼Œä¸æ™®é€šæ¨¡å‹ç”¨æ³•å®Œå…¨ä¸€è‡´
const { text } = await generateText({
  model: enhancedModel,
  prompt: '...',
})
```

## å†…ç½®ä¸­é—´ä»¶

### extractReasoningMiddleware

ä»æ¨¡å‹è¾“å‡ºä¸­æå–æ¨ç†è¿‡ç¨‹ã€‚ä¸€äº›æ¨¡å‹ä¼šåœ¨ç‰¹æ®Šæ ‡ç­¾ï¼ˆå¦‚ `<think>`ï¼‰ä¸­è¾“å‡ºæ¨ç†è¿‡ç¨‹ï¼Œè¿™ä¸ªä¸­é—´ä»¶ä¼šè‡ªåŠ¨æå–å¹¶æš´éœ²ä¸º `reasoning` å±æ€§ï¼š

```typescript
import { wrapLanguageModel, extractReasoningMiddleware } from 'ai'

const model = wrapLanguageModel({
  model: yourModel,
  middleware: extractReasoningMiddleware({ tagName: 'think' }),
})

// ä½¿ç”¨åï¼Œç»“æœä¸­ä¼šæœ‰ reasoning å­—æ®µ
const result = await generateText({
  model,
  prompt: 'è§£é‡Šé‡å­çº ç¼ ',
})

console.log(result.reasoning) // æ¨ç†è¿‡ç¨‹
console.log(result.text)      // æœ€ç»ˆå›ç­”
```

### simulateStreamingMiddleware

ä¸ºä¸æ”¯æŒæµå¼è¾“å‡ºçš„æ¨¡å‹æ¨¡æ‹Ÿæµå¼è¡Œä¸ºï¼š

```typescript
import { wrapLanguageModel, simulateStreamingMiddleware } from 'ai'

const model = wrapLanguageModel({
  model: nonStreamingModel, // ä¸æ”¯æŒæµå¼çš„æ¨¡å‹
  middleware: simulateStreamingMiddleware(),
})

// ç°åœ¨å¯ä»¥åƒæµå¼æ¨¡å‹ä¸€æ ·ä½¿ç”¨
const result = streamText({
  model,
  prompt: '...',
})
```

### defaultSettingsMiddleware

ä¸ºæ¨¡å‹è®¾ç½®é»˜è®¤å‚æ•°ï¼Œé¿å…æ¯æ¬¡è°ƒç”¨éƒ½é‡å¤é…ç½®ï¼š

```typescript
import { wrapLanguageModel, defaultSettingsMiddleware } from 'ai'

const model = wrapLanguageModel({
  model: yourModel,
  middleware: defaultSettingsMiddleware({
    settings: {
      temperature: 0.5,
      maxOutputTokens: 800,
      providerOptions: { openai: { store: false } },
    },
  }),
})

// ä½¿ç”¨æ—¶æ— éœ€å†æŒ‡å®š temperature å’Œ maxOutputTokens
const { text } = await generateText({
  model,
  prompt: '...',
})
```

### extractJsonMiddleware

ä»æ¨¡å‹è¾“å‡ºä¸­æå– JSONï¼Œè‡ªåŠ¨ç§»é™¤ Markdown ä»£ç å—æ ‡è®°ï¼ˆ`` ```json ```)ï¼š

```typescript
import {
  wrapLanguageModel,
  extractJsonMiddleware,
  Output,
  generateText,
} from 'ai'
import { z } from 'zod'

const model = wrapLanguageModel({
  model: yourModel,
  middleware: extractJsonMiddleware(),
})

const result = await generateText({
  model,
  output: Output.object({
    schema: z.object({
      name: z.string(),
      ingredients: z.array(z.string()),
    }),
  }),
  prompt: 'ç”Ÿæˆä¸€ä¸ªèœè°±ã€‚',
})
```

### addToolInputExamplesMiddleware

å°†å·¥å…·è¾“å…¥ç¤ºä¾‹æ·»åŠ åˆ°å·¥å…·æè¿°ä¸­ï¼Œå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°ç†è§£å¦‚ä½•è°ƒç”¨å·¥å…·ã€‚

## ç¼–å†™è‡ªå®šä¹‰ä¸­é—´ä»¶

`LanguageModelV3Middleware` æ¥å£æä¾›äº†ä¸¤ä¸ªæ ¸å¿ƒé’©å­ï¼š

- **`wrapGenerate`** â€” æ‹¦æˆªéæµå¼è°ƒç”¨
- **`wrapStream`** â€” æ‹¦æˆªæµå¼è°ƒç”¨

### ç®€å•ç¼“å­˜ä¸­é—´ä»¶

```typescript
import type { LanguageModelV3Middleware } from '@ai-sdk/provider'

const cache = new Map<string, any>()

export const simpleCacheMiddleware: LanguageModelV3Middleware = {
  wrapGenerate: async ({ doGenerate, params }) => {
    const cacheKey = JSON.stringify(params)

    // å‘½ä¸­ç¼“å­˜ï¼Œç›´æ¥è¿”å›
    if (cache.has(cacheKey)) {
      return cache.get(cacheKey)
    }

    // æœªå‘½ä¸­ï¼Œè°ƒç”¨æ¨¡å‹å¹¶ç¼“å­˜ç»“æœ
    const result = await doGenerate()
    cache.set(cacheKey, result)

    return result
  },

  // æµå¼è°ƒç”¨çš„ç¼“å­˜éœ€è¦é¢å¤–å¤„ç†
  // wrapStream: async ({ doStream, params }) => { ... }
}
```

### æ—¥å¿—ä¸­é—´ä»¶

è®°å½•æ‰€æœ‰æ¨¡å‹è°ƒç”¨çš„å‚æ•°å’Œç»“æœï¼š

```typescript
import type { LanguageModelV3Middleware } from '@ai-sdk/provider'

export const loggingMiddleware: LanguageModelV3Middleware = {
  wrapGenerate: async ({ doGenerate, params, model }) => {
    const startTime = Date.now()

    console.log(`[LLM] è°ƒç”¨æ¨¡å‹: ${model.modelId}`)
    console.log(`[LLM] å‚æ•°:`, JSON.stringify(params.prompt).slice(0, 200))

    const result = await doGenerate()

    const duration = Date.now() - startTime
    console.log(`[LLM] è€—æ—¶: ${duration}ms`)
    console.log(`[LLM] Token ä½¿ç”¨:`, result.usage)

    return result
  },

  wrapStream: async ({ doStream, params, model }) => {
    console.log(`[LLM-Stream] è°ƒç”¨æ¨¡å‹: ${model.modelId}`)

    const { stream, ...rest } = await doStream()

    // ä½¿ç”¨ TransformStream æ‹¦æˆªæµå¼æ•°æ®
    let totalChunks = 0
    const transformStream = new TransformStream({
      transform(chunk, controller) {
        totalChunks++
        controller.enqueue(chunk)
      },
      flush() {
        console.log(`[LLM-Stream] å…± ${totalChunks} ä¸ª chunk`)
      },
    })

    return {
      stream: stream.pipeThrough(transformStream),
      ...rest,
    }
  },
}
```

### ç»„åˆå¤šä¸ªä¸­é—´ä»¶

é€šè¿‡å¤šå±‚ `wrapLanguageModel` ç»„åˆä¸­é—´ä»¶ï¼ˆå¤–å±‚å…ˆæ‰§è¡Œï¼‰ï¼š

```typescript
import { wrapLanguageModel, defaultSettingsMiddleware } from 'ai'

// ä»å†…åˆ°å¤–ä¾æ¬¡åŒ…è£¹
let model = yourBaseModel

// ç¬¬ä¸€å±‚ï¼šé»˜è®¤è®¾ç½®
model = wrapLanguageModel({
  model,
  middleware: defaultSettingsMiddleware({
    settings: { temperature: 0.7 },
  }),
})

// ç¬¬äºŒå±‚ï¼šæ—¥å¿—
model = wrapLanguageModel({
  model,
  middleware: loggingMiddleware,
})

// ç¬¬ä¸‰å±‚ï¼šç¼“å­˜
model = wrapLanguageModel({
  model,
  middleware: simpleCacheMiddleware,
})

// æœ€ç»ˆçš„ model åŒæ—¶å…·å¤‡é»˜è®¤è®¾ç½®ã€æ—¥å¿—å’Œç¼“å­˜èƒ½åŠ›
```

## ä¸­é—´ä»¶ä½¿ç”¨åœºæ™¯

| åœºæ™¯ | ä¸­é—´ä»¶ | è¯´æ˜ |
|------|--------|------|
| æå–æ¨ç†è¿‡ç¨‹ | `extractReasoningMiddleware` | DeepSeek ç­‰æ¨¡å‹çš„ thinking æ ‡ç­¾ |
| éæµå¼è½¬æµå¼ | `simulateStreamingMiddleware` | å…¼å®¹ä¸æ”¯æŒæµå¼çš„æ¨¡å‹ |
| ç»Ÿä¸€æ¨¡å‹å‚æ•° | `defaultSettingsMiddleware` | é¿å…é‡å¤é…ç½® |
| å“åº”ç¼“å­˜ | è‡ªå®šä¹‰ç¼“å­˜ä¸­é—´ä»¶ | å‡å°‘ API è°ƒç”¨ã€é™ä½æˆæœ¬ |
| è°ƒç”¨æ—¥å¿— | è‡ªå®šä¹‰æ—¥å¿—ä¸­é—´ä»¶ | ç›‘æ§ã€è°ƒè¯•ã€å®¡è®¡ |
| å®‰å…¨è¿‡æ»¤ | è‡ªå®šä¹‰è¿‡æ»¤ä¸­é—´ä»¶ | è¾“å…¥/è¾“å‡ºå†…å®¹å®¡æ ¸ |
| é‡è¯•é€»è¾‘ | è‡ªå®šä¹‰é‡è¯•ä¸­é—´ä»¶ | å¤„ç†ç¬æ—¶é”™è¯¯ |

## ä¸‹ä¸€æ­¥

- [ç¼“å­˜ä¸é€Ÿç‡é™åˆ¶](/ai/vercel-ai-sdk/guide/caching-and-limits) â€” æ·±å…¥äº†è§£åŸºäºä¸­é—´ä»¶çš„ç¼“å­˜ç­–ç•¥
- [é”™è¯¯å¤„ç†ä¸æµ‹è¯•](/ai/vercel-ai-sdk/guide/error-handling) â€” å­¦ä¹ é”™è¯¯ç±»å‹å’Œæµ‹è¯•æ–¹æ³•
- [Provider é€‰å‹æŒ‡å—](/ai/vercel-ai-sdk/guide/providers) â€” äº†è§£ä¸åŒ Provider çš„ç‰¹æ€§
