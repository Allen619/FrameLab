---
title: ç¼“å­˜ä¸é€Ÿç‡é™åˆ¶
description: å­¦ä¹  AI SDK ä¸­çš„å“åº”ç¼“å­˜ç­–ç•¥ã€é€Ÿç‡é™åˆ¶å®ç°å’ŒèƒŒå‹å¤„ç†ï¼Œæœ‰æ•ˆæ§åˆ¶æˆæœ¬å¹¶æå‡åº”ç”¨æ€§èƒ½ã€‚
---

# ç¼“å­˜ä¸é€Ÿç‡é™åˆ¶

åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼ŒLLM è°ƒç”¨çš„æˆæœ¬å’Œé€Ÿç‡é™åˆ¶æ˜¯å¿…é¡»é¢å¯¹çš„é—®é¢˜ã€‚æœ¬ç« ä»‹ç»å¦‚ä½•é€šè¿‡ç¼“å­˜å‡å°‘é‡å¤è°ƒç”¨ã€å®æ–½é€Ÿç‡é™åˆ¶é˜²æ­¢è¶…é¢ä½¿ç”¨ã€ä»¥åŠå¤„ç†èƒŒå‹ã€‚

[ğŸ”— AI SDK ç¼“å­˜å®˜æ–¹æ–‡æ¡£](https://ai-sdk.dev/docs/advanced/caching){target="_blank" rel="noopener"}

## å“åº”ç¼“å­˜

### æ–¹å¼ä¸€ï¼šonFinish å›è°ƒç¼“å­˜

æœ€ç®€å•çš„ç¼“å­˜æ–¹å¼æ˜¯åœ¨ `onFinish` å›è°ƒä¸­å­˜å‚¨å“åº”ï¼š

```typescript
import { convertToModelMessages, streamText, type UIMessage } from 'ai'
import { Redis } from '@upstash/redis'

export const maxDuration = 30

const redis = new Redis({
  url: process.env.KV_URL,
  token: process.env.KV_TOKEN,
})

export async function POST(req: Request) {
  const { messages }: { messages: UIMessage[] } = await req.json()

  // åŸºäºæ¶ˆæ¯å†…å®¹ç”Ÿæˆç¼“å­˜ key
  const key = JSON.stringify(messages)

  // æ£€æŸ¥ç¼“å­˜
  const cached = (await redis.get(key)) as string | null
  if (cached != null) {
    return new Response(cached, {
      status: 200,
      headers: { 'Content-Type': 'text/plain' },
    })
  }

  // è°ƒç”¨æ¨¡å‹
  const result = streamText({
    model: 'openai/gpt-4o',
    messages: await convertToModelMessages(messages),
    async onFinish({ text }) {
      // ç¼“å­˜å“åº”ï¼Œ1 å°æ—¶è¿‡æœŸ
      await redis.set(key, text)
      await redis.expire(key, 60 * 60)
    },
  })

  return result.toUIMessageStreamResponse()
}
```

### æ–¹å¼äºŒï¼šä¸­é—´ä»¶å±‚ç¼“å­˜

ä½¿ç”¨ Language Model Middleware åœ¨æ¨¡å‹è°ƒç”¨å±‚å®ç°ç¼“å­˜ï¼Œå¯¹ä¸Šå±‚ä»£ç å®Œå…¨é€æ˜ï¼š

```typescript
import { Redis } from '@upstash/redis'
import {
  type LanguageModelV3Middleware,
  type LanguageModelV3StreamPart,
  simulateReadableStream,
} from 'ai'

const redis = new Redis({
  url: process.env.KV_URL,
  token: process.env.KV_TOKEN,
})

export const cacheMiddleware: LanguageModelV3Middleware = {
  // éæµå¼è°ƒç”¨ç¼“å­˜
  wrapGenerate: async ({ doGenerate, params }) => {
    const cacheKey = JSON.stringify(params)

    const cached = await redis.get(cacheKey)
    if (cached !== null) {
      return {
        ...(cached as any),
        response: {
          ...(cached as any).response,
          timestamp: (cached as any)?.response?.timestamp
            ? new Date((cached as any).response.timestamp)
            : undefined,
        },
      }
    }

    const result = await doGenerate()
    redis.set(cacheKey, result)
    return result
  },

  // æµå¼è°ƒç”¨ç¼“å­˜
  wrapStream: async ({ doStream, params }) => {
    const cacheKey = JSON.stringify(params)

    const cached = await redis.get(cacheKey)
    if (cached !== null) {
      const formattedChunks = (cached as LanguageModelV3StreamPart[]).map(
        (p) => {
          if (p.type === 'response-metadata' && (p as any).timestamp) {
            return { ...p, timestamp: new Date((p as any).timestamp) }
          }
          return p
        },
      )
      return {
        stream: simulateReadableStream({
          initialDelayInMs: 0,
          chunkDelayInMs: 10,
          chunks: formattedChunks,
        }),
      }
    }

    const { stream, ...rest } = await doStream()

    // æ”¶é›†æ‰€æœ‰ chunk ç”¨äºç¼“å­˜
    const fullResponse: LanguageModelV3StreamPart[] = []
    const transformStream = new TransformStream<
      LanguageModelV3StreamPart,
      LanguageModelV3StreamPart
    >({
      transform(chunk, controller) {
        fullResponse.push(chunk)
        controller.enqueue(chunk)
      },
      flush() {
        redis.set(cacheKey, fullResponse)
      },
    })

    return {
      stream: stream.pipeThrough(transformStream),
      ...rest,
    }
  },
}
```

ä½¿ç”¨ç¼“å­˜ä¸­é—´ä»¶ï¼š

```typescript
import { wrapLanguageModel, generateText } from 'ai'
import { cacheMiddleware } from './cache-middleware'

const cachedModel = wrapLanguageModel({
  model: 'openai/gpt-4o',
  middleware: cacheMiddleware,
})

// ç¬¬ä¸€æ¬¡è°ƒç”¨ â€” è°ƒç”¨æ¨¡å‹ï¼Œç¼“å­˜ç»“æœ
const result1 = await generateText({
  model: cachedModel,
  prompt: 'ä»€ä¹ˆæ˜¯ TypeScriptï¼Ÿ',
})

// ç¬¬äºŒæ¬¡ç›¸åŒè°ƒç”¨ â€” ç›´æ¥è¿”å›ç¼“å­˜
const result2 = await generateText({
  model: cachedModel,
  prompt: 'ä»€ä¹ˆæ˜¯ TypeScriptï¼Ÿ',
})
```

### ç¼“å­˜ç­–ç•¥é€‰æ‹©

| ç­–ç•¥ | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| onFinish å›è°ƒ | å®ç°ç®€å• | åªç¼“å­˜æœ€ç»ˆæ–‡æœ¬ | ç®€å•çš„æ–‡æœ¬ç¼“å­˜ |
| ä¸­é—´ä»¶å±‚ | å®Œæ•´ç¼“å­˜ï¼ˆå« metadataï¼‰ | å®ç°å¤æ‚ | éœ€è¦ç²¾ç¡®ç¼“å­˜çš„åœºæ™¯ |
| æœ¬åœ°å†…å­˜ | æœ€å¿« | é‡å¯ä¸¢å¤±ã€ä¸è·¨è¿›ç¨‹ | å¼€å‘ç¯å¢ƒã€å•è¿›ç¨‹ |
| Redis/KV | æŒä¹…åŒ–ã€è·¨è¿›ç¨‹ | éœ€è¦å¤–éƒ¨æœåŠ¡ | ç”Ÿäº§ç¯å¢ƒ |

## é€Ÿç‡é™åˆ¶

### åŸºäºç”¨æˆ·çš„é€Ÿç‡é™åˆ¶

```typescript
import { Redis } from '@upstash/redis'
import { Ratelimit } from '@upstash/ratelimit'

const ratelimit = new Ratelimit({
  redis: new Redis({
    url: process.env.KV_URL,
    token: process.env.KV_TOKEN,
  }),
  // æ¯ä¸ªç”¨æˆ·æ¯åˆ†é’Ÿæœ€å¤š 10 æ¬¡è¯·æ±‚
  limiter: Ratelimit.slidingWindow(10, '1m'),
})

export async function POST(req: Request) {
  // è·å–ç”¨æˆ·æ ‡è¯†
  const userId = getUserId(req)

  // æ£€æŸ¥é€Ÿç‡é™åˆ¶
  const { success, limit, remaining, reset } = await ratelimit.limit(userId)

  if (!success) {
    return new Response('è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åå†è¯•', {
      status: 429,
      headers: {
        'X-RateLimit-Limit': limit.toString(),
        'X-RateLimit-Remaining': remaining.toString(),
        'X-RateLimit-Reset': reset.toString(),
      },
    })
  }

  // æ­£å¸¸å¤„ç†è¯·æ±‚
  const result = streamText({
    model: 'openai/gpt-4o',
    prompt: '...',
  })

  return result.toUIMessageStreamResponse()
}
```

### åŸºäº Token çš„é¢„ç®—æ§åˆ¶

```typescript
import { generateText } from 'ai'

// Token é¢„ç®—ç®¡ç†å™¨
class TokenBudget {
  private used = 0

  constructor(private readonly maxTokens: number) {}

  canSpend(estimated: number): boolean {
    return this.used + estimated <= this.maxTokens
  }

  record(usage: { inputTokens: number; outputTokens: number }) {
    this.used += usage.inputTokens + usage.outputTokens
  }

  get remaining() {
    return this.maxTokens - this.used
  }
}

const dailyBudget = new TokenBudget(1_000_000) // æ¯æ—¥ 100 ä¸‡ token

export async function handleRequest(prompt: string) {
  // é¢„ä¼°æ¶ˆè€—ï¼ˆç®€å•æŒ‰å­—ç¬¦ä¼°ç®—ï¼‰
  const estimatedTokens = prompt.length / 2 + 500

  if (!dailyBudget.canSpend(estimatedTokens)) {
    throw new Error('ä»Šæ—¥ Token é¢„ç®—å·²ç”¨å®Œ')
  }

  const result = await generateText({
    model: 'openai/gpt-4o',
    prompt,
  })

  // è®°å½•å®é™…ä½¿ç”¨é‡
  dailyBudget.record({
    inputTokens: result.usage.inputTokens,
    outputTokens: result.usage.outputTokens,
  })

  return result.text
}
```

## èƒŒå‹å¤„ç†

å½“ LLM Provider è¿”å› 429ï¼ˆToo Many Requestsï¼‰é”™è¯¯æ—¶ï¼Œéœ€è¦å®æ–½é€€é¿é‡è¯•ç­–ç•¥ï¼š

```typescript
import { generateText } from 'ai'

async function generateWithRetry(
  prompt: string,
  maxRetries = 3,
): Promise<string> {
  for (let attempt = 0; attempt < maxRetries; attempt++) {
    try {
      const { text } = await generateText({
        model: 'openai/gpt-4o',
        prompt,
      })
      return text
    } catch (error: any) {
      // æ£€æŸ¥æ˜¯å¦æ˜¯é€Ÿç‡é™åˆ¶é”™è¯¯
      if (error?.statusCode === 429 && attempt < maxRetries - 1) {
        // æŒ‡æ•°é€€é¿
        const delay = Math.pow(2, attempt) * 1000 + Math.random() * 1000
        console.log(`é€Ÿç‡é™åˆ¶ï¼Œ${delay}ms åé‡è¯• (${attempt + 1}/${maxRetries})`)
        await new Promise((resolve) => setTimeout(resolve, delay))
        continue
      }
      throw error
    }
  }
  throw new Error('é‡è¯•æ¬¡æ•°å·²è¾¾ä¸Šé™')
}
```

### è¯·æ±‚é˜Ÿåˆ—

å¯¹äºé«˜å¹¶å‘åœºæ™¯ï¼Œä½¿ç”¨é˜Ÿåˆ—æ§åˆ¶å¹¶å‘æ•°ï¼š

```typescript
class RequestQueue {
  private queue: Array<() => Promise<void>> = []
  private running = 0

  constructor(private readonly concurrency: number) {}

  async add<T>(fn: () => Promise<T>): Promise<T> {
    return new Promise((resolve, reject) => {
      this.queue.push(async () => {
        try {
          resolve(await fn())
        } catch (e) {
          reject(e)
        }
      })
      this.process()
    })
  }

  private async process() {
    if (this.running >= this.concurrency || this.queue.length === 0) return

    this.running++
    const fn = this.queue.shift()!
    await fn()
    this.running--
    this.process()
  }
}

// æœ€å¤šåŒæ—¶ 3 ä¸ª LLM è¯·æ±‚
const llmQueue = new RequestQueue(3)

// ä½¿ç”¨
const result = await llmQueue.add(() =>
  generateText({ model: 'openai/gpt-4o', prompt: '...' }),
)
```

## æœ€ä½³å®è·µ

1. **åˆ†å±‚ç¼“å­˜** â€” çƒ­æ•°æ®æ”¾å†…å­˜ï¼Œå†·æ•°æ®æ”¾ Redis
2. **ç¼“å­˜ Key è®¾è®¡** â€” åŸºäº prompt + model + parameters ç”Ÿæˆï¼Œæ’é™¤æ—¶é—´æˆ³ç­‰å˜åŒ–å› ç´ 
3. **åˆç†çš„ TTL** â€” æ ¹æ®æ•°æ®æ—¶æ•ˆæ€§è®¾ç½®è¿‡æœŸæ—¶é—´
4. **ç›‘æ§å‘Šè­¦** â€” è¿½è¸ªç¼“å­˜å‘½ä¸­ç‡ã€API è°ƒç”¨é‡ã€Token æ¶ˆè€—
5. **ä¼˜é›…é™çº§** â€” ç¼“å­˜æœåŠ¡ä¸å¯ç”¨æ—¶ç›´æ¥è°ƒç”¨æ¨¡å‹ï¼Œä¸åº”é˜»æ–­ä¸»æµç¨‹

## ä¸‹ä¸€æ­¥

- [é”™è¯¯å¤„ç†ä¸æµ‹è¯•](/ai/vercel-ai-sdk/guide/error-handling) â€” å¤„ç† API è°ƒç”¨ä¸­çš„å„ç±»é”™è¯¯
- [ä¸­é—´ä»¶ç³»ç»Ÿ](/ai/vercel-ai-sdk/guide/middleware) â€” äº†è§£ä¸­é—´ä»¶çš„æ›´å¤šç”¨æ³•
- [éƒ¨ç½²æŒ‡å—](/ai/vercel-ai-sdk/guide/deployment) â€” ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²é…ç½®
