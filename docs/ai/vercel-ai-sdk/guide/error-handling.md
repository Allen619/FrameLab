---
title: é”™è¯¯å¤„ç†ä¸æµ‹è¯•
description: æŒæ¡ AI SDK çš„é”™è¯¯ç±»å‹ä½“ç³»å’Œå¤„ç†æ¨¡å¼ï¼Œå­¦ä¹ ä½¿ç”¨ Mock å·¥å…·è¿›è¡Œå•å…ƒæµ‹è¯•ã€‚
---

# é”™è¯¯å¤„ç†ä¸æµ‹è¯•

AI åº”ç”¨çš„é”™è¯¯å¤„ç†å’Œæµ‹è¯•ä¸ä¼ ç»Ÿ Web åº”ç”¨æœ‰æ˜¾è‘—ä¸åŒ â€”â€” LLM è°ƒç”¨å¯èƒ½è¶…æ—¶ã€è¿”å›æ— æ•ˆå†…å®¹ã€æˆ–è§¦å‘é€Ÿç‡é™åˆ¶ã€‚æœ¬ç« ä»‹ç» AI SDK æä¾›çš„é”™è¯¯ç±»å‹ä½“ç³»å’Œæµ‹è¯•å·¥å…·ã€‚

[ğŸ”— AI SDK é”™è¯¯å¤„ç†å®˜æ–¹æ–‡æ¡£](https://ai-sdk.dev/docs/ai-sdk-core/error-handling){target="_blank" rel="noopener"}

## é”™è¯¯ç±»å‹ä½“ç³»

AI SDK æä¾›äº†ä¸€å¥—æ ‡å‡†åŒ–çš„é”™è¯¯ç±»ï¼Œæ¯ç§é”™è¯¯éƒ½æœ‰ `isInstance` é™æ€æ–¹æ³•ç”¨äºç±»å‹åˆ¤æ–­ï¼š

### APICallError

API è°ƒç”¨å¤±è´¥æ—¶æŠ›å‡ºï¼ŒåŒ…å« HTTP çŠ¶æ€ç å’Œé”™è¯¯ä¿¡æ¯ï¼š

```typescript
import { APICallError, generateText } from 'ai'

try {
  const { text } = await generateText({
    model: 'openai/gpt-4o',
    prompt: '...',
  })
} catch (error) {
  if (APICallError.isInstance(error)) {
    console.error('API è°ƒç”¨å¤±è´¥')
    console.error('çŠ¶æ€ç :', error.statusCode)
    console.error('é”™è¯¯ä¿¡æ¯:', error.message)
    console.error('æ˜¯å¦å¯é‡è¯•:', error.isRetryable)

    // é’ˆå¯¹ä¸åŒçŠ¶æ€ç å¤„ç†
    if (error.statusCode === 429) {
      // é€Ÿç‡é™åˆ¶ï¼Œéœ€è¦é€€é¿é‡è¯•
    } else if (error.statusCode >= 500) {
      // æœåŠ¡ç«¯é”™è¯¯ï¼Œå¯ä»¥é‡è¯•
    }
  }
}
```

### TooManyRequestsError

é€Ÿç‡é™åˆ¶é”™è¯¯ï¼ˆHTTP 429ï¼‰çš„ä¸“ç”¨ç±»å‹ï¼š

```typescript
import { TooManyRequestsError } from '@ai-sdk/provider'

try {
  // LLM è°ƒç”¨
} catch (error) {
  if (error instanceof TooManyRequestsError) {
    const retryAfter = error.retryAfter // é‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    console.log(`é€Ÿç‡é™åˆ¶ï¼Œ${retryAfter} ç§’åé‡è¯•`)
  }
}
```

### InvalidPromptError

æç¤ºè¯æ ¼å¼é”™è¯¯æ—¶æŠ›å‡ºï¼š

```typescript
import { InvalidPromptError } from 'ai'

try {
  const { text } = await generateText({
    model: 'openai/gpt-4o',
    prompt: '', // ç©ºæç¤ºè¯
  })
} catch (error) {
  if (InvalidPromptError.isInstance(error)) {
    console.error('æç¤ºè¯æ— æ•ˆ:', error.message)
  }
}
```

### å…¶ä»–é”™è¯¯ç±»å‹

| é”™è¯¯ç±» | è¯´æ˜ | å¸¸è§åŸå›  |
|--------|------|----------|
| `APICallError` | API è°ƒç”¨å¤±è´¥ | ç½‘ç»œé”™è¯¯ã€é‰´æƒå¤±è´¥ã€æœåŠ¡ä¸å¯ç”¨ |
| `TooManyRequestsError` | é€Ÿç‡é™åˆ¶ | è¯·æ±‚é¢‘ç‡è¿‡é«˜ |
| `InvalidPromptError` | æç¤ºè¯æ— æ•ˆ | æ ¼å¼é”™è¯¯ã€è¶…é•¿ |
| `InvalidResponseDataError` | å“åº”æ•°æ®æ— æ•ˆ | æ¨¡å‹è¿”å›æ ¼å¼ä¸ç¬¦é¢„æœŸ |
| `NoSuchModelError` | æ¨¡å‹ä¸å­˜åœ¨ | æ¨¡å‹åæ‹¼å†™é”™è¯¯ |

## é”™è¯¯å¤„ç†æ¨¡å¼

### ç»Ÿä¸€é”™è¯¯å¤„ç†

```typescript
import { APICallError, InvalidPromptError, generateText } from 'ai'

async function safeGenerate(prompt: string) {
  try {
    const { text } = await generateText({
      model: 'openai/gpt-4o',
      prompt,
    })
    return { success: true, text }
  } catch (error) {
    if (InvalidPromptError.isInstance(error)) {
      return { success: false, error: 'è¾“å…¥æ ¼å¼ä¸æ­£ç¡®ï¼Œè¯·æ£€æŸ¥åé‡è¯•' }
    }

    if (APICallError.isInstance(error)) {
      if (error.statusCode === 429) {
        return { success: false, error: 'è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åå†è¯•' }
      }
      if (error.isRetryable) {
        return { success: false, error: 'æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•' }
      }
      return { success: false, error: 'æœåŠ¡è°ƒç”¨å¤±è´¥' }
    }

    // æœªçŸ¥é”™è¯¯
    console.error('æœªçŸ¥é”™è¯¯:', error)
    return { success: false, error: 'ç³»ç»Ÿå¼‚å¸¸ï¼Œè¯·è”ç³»ç®¡ç†å‘˜' }
  }
}
```

### æµå¼é”™è¯¯å¤„ç†

```typescript
import { streamText } from 'ai'

export async function POST(req: Request) {
  try {
    const { messages } = await req.json()

    const result = streamText({
      model: 'openai/gpt-4o',
      messages,
    })

    return result.toUIMessageStreamResponse()
  } catch (error) {
    // æµå¼å“åº”å¼€å§‹å‰çš„é”™è¯¯
    return new Response(
      JSON.stringify({ error: 'å¤„ç†è¯·æ±‚å¤±è´¥' }),
      { status: 500, headers: { 'Content-Type': 'application/json' } },
    )
  }
}
```

### å¸¦é‡è¯•çš„é”™è¯¯å¤„ç†

```typescript
import { APICallError, generateText } from 'ai'

async function generateWithRetry(
  prompt: string,
  maxRetries = 3,
) {
  let lastError: Error | undefined

  for (let i = 0; i < maxRetries; i++) {
    try {
      return await generateText({
        model: 'openai/gpt-4o',
        prompt,
      })
    } catch (error) {
      lastError = error as Error

      if (APICallError.isInstance(error)) {
        if (!error.isRetryable) {
          throw error // ä¸å¯é‡è¯•çš„é”™è¯¯ç›´æ¥æŠ›å‡º
        }
        // æŒ‡æ•°é€€é¿
        const delay = Math.pow(2, i) * 1000
        await new Promise((r) => setTimeout(r, delay))
      } else {
        throw error // é API é”™è¯¯ç›´æ¥æŠ›å‡º
      }
    }
  }

  throw lastError
}
```

## æµ‹è¯•

[ğŸ”— AI SDK æµ‹è¯•å®˜æ–¹æ–‡æ¡£](https://ai-sdk.dev/docs/ai-sdk-core/testing){target="_blank" rel="noopener"}

AI SDK æä¾›äº† Mock å·¥å…·ï¼Œè®©ä½ æ— éœ€çœŸå® API è°ƒç”¨å³å¯æµ‹è¯• AI åŠŸèƒ½ã€‚

### MockLanguageModelV3

æ¨¡æ‹Ÿæ¨¡å‹çš„éæµå¼å’Œæµå¼è¡Œä¸ºï¼š

```typescript
import { generateText } from 'ai'
import { MockLanguageModelV3 } from 'ai/test'

// æµ‹è¯• generateText
const result = await generateText({
  model: new MockLanguageModelV3({
    doGenerate: async () => ({
      text: 'è¿™æ˜¯æ¨¡æ‹Ÿçš„å›ç­”',
      usage: {
        inputTokens: { total: 10, noCache: 10 },
        outputTokens: { total: 20, text: 20 },
      },
      finishReason: { unified: 'stop', raw: undefined },
      response: {
        id: 'mock-id',
        timestamp: new Date(),
        modelId: 'mock-model',
      },
    }),
  }),
  prompt: 'æµ‹è¯•æç¤º',
})

expect(result.text).toBe('è¿™æ˜¯æ¨¡æ‹Ÿçš„å›ç­”')
```

### simulateReadableStream

æ¨¡æ‹Ÿæµå¼è¾“å‡ºï¼Œç”¨äºæµ‹è¯• `streamText`ï¼š

```typescript
import { streamText, simulateReadableStream } from 'ai'
import { MockLanguageModelV3 } from 'ai/test'

const result = streamText({
  model: new MockLanguageModelV3({
    doStream: async () => ({
      stream: simulateReadableStream({
        chunks: [
          { type: 'text-start', id: 'text-1' },
          { type: 'text-delta', id: 'text-1', delta: 'ä½ å¥½' },
          { type: 'text-delta', id: 'text-1', delta: 'ï¼Œ' },
          { type: 'text-delta', id: 'text-1', delta: 'ä¸–ç•Œï¼' },
          { type: 'text-end', id: 'text-1' },
          {
            type: 'finish',
            finishReason: { unified: 'stop', raw: undefined },
            logprobs: undefined,
            usage: {
              inputTokens: {
                total: 3,
                noCache: 3,
                cacheRead: undefined,
                cacheWrite: undefined,
              },
              outputTokens: {
                total: 10,
                text: 10,
                reasoning: undefined,
              },
            },
          },
        ],
      }),
    }),
  }),
  prompt: 'ä½ å¥½',
})

// éªŒè¯æµå¼è¾“å‡º
const chunks: string[] = []
for await (const chunk of result.textStream) {
  chunks.push(chunk)
}
expect(chunks.join('')).toBe('ä½ å¥½ï¼Œä¸–ç•Œï¼')
```

### æµ‹è¯•å·¥å…·è°ƒç”¨

```typescript
import { generateText, tool } from 'ai'
import { MockLanguageModelV3 } from 'ai/test'
import { z } from 'zod'

// å®šä¹‰è¢«æµ‹è¯•çš„å·¥å…·
const weatherTool = tool({
  description: 'è·å–å¤©æ°”',
  inputSchema: z.object({ city: z.string() }),
  execute: async ({ city }) => ({ city, temp: 25 }),
})

// æµ‹è¯•å·¥å…·çš„ execute å‡½æ•°
const toolResult = await weatherTool.execute(
  { city: 'åŒ—äº¬' },
  {} as any, // ç®€åŒ–çš„ä¸Šä¸‹æ–‡
)
expect(toolResult).toEqual({ city: 'åŒ—äº¬', temp: 25 })
```

### æµ‹è¯•é”™è¯¯åœºæ™¯

```typescript
import { generateText } from 'ai'
import { MockLanguageModelV3 } from 'ai/test'

// æ¨¡æ‹Ÿ API é”™è¯¯
const errorModel = new MockLanguageModelV3({
  doGenerate: async () => {
    throw new Error('æ¨¡æ‹Ÿçš„ API é”™è¯¯')
  },
})

await expect(
  generateText({ model: errorModel, prompt: '...' }),
).rejects.toThrow('æ¨¡æ‹Ÿçš„ API é”™è¯¯')
```

## æµ‹è¯•æœ€ä½³å®è·µ

1. **éš”ç¦» AI è°ƒç”¨** â€” å°† AI è°ƒç”¨å°è£…åœ¨ç‹¬ç«‹å‡½æ•°ä¸­ï¼Œä¾¿äº Mock
2. **æµ‹è¯•è¾¹ç•Œæ¡ä»¶** â€” ç©ºå“åº”ã€è¶…é•¿è¾“å‡ºã€å·¥å…·è°ƒç”¨å¤±è´¥
3. **å¿«ç…§æµ‹è¯•** â€” å¯¹ç¨³å®šçš„æç¤ºè¯æ¨¡æ¿ä½¿ç”¨å¿«ç…§æµ‹è¯•
4. **E2E æµ‹è¯•åˆ†ç¦»** â€” çœŸå® API è°ƒç”¨çš„æµ‹è¯•æ ‡è®°ä¸ºé›†æˆæµ‹è¯•ï¼Œä¸åœ¨ CI ä¸­é¢‘ç¹è¿è¡Œ
5. **Token ç”¨é‡æ–­è¨€** â€” ç¡®ä¿ Mock çš„ usage æ•°æ®åˆç†

## ä¸‹ä¸€æ­¥

- [éƒ¨ç½²æŒ‡å—](/ai/vercel-ai-sdk/guide/deployment) â€” ç”Ÿäº§ç¯å¢ƒé”™è¯¯å¤„ç†å’Œç›‘æ§
- [ç¼“å­˜ä¸é€Ÿç‡é™åˆ¶](/ai/vercel-ai-sdk/guide/caching-and-limits) â€” é˜²æ­¢é”™è¯¯çš„é¢„é˜²æ€§æªæ–½
- [ä¸­é—´ä»¶ç³»ç»Ÿ](/ai/vercel-ai-sdk/guide/middleware) â€” é€šè¿‡ä¸­é—´ä»¶ç»Ÿä¸€å¤„ç†é”™è¯¯
