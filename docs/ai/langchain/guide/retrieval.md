---
title: æ£€ç´¢å¢å¼º RAG
description: æŒæ¡ LangChain 1.0 çš„ RAGï¼ˆRetrieval Augmented Generationï¼‰å…¨æµç¨‹â€”â€”æ–‡æ¡£åŠ è½½ã€åˆ‡åˆ†ã€å‘é‡åŒ–ã€æ£€ç´¢ä¸ Agent é›†æˆ
---

# æ£€ç´¢å¢å¼º RAG

## æ¦‚è¿°

å¤§è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†æ¥æºäºè®­ç»ƒæ•°æ®ï¼Œå­˜åœ¨**æ—¶æ•ˆæ€§å·®**å’Œ**ç¼ºä¹ç§æœ‰æ•°æ®**ä¸¤å¤§å±€é™ã€‚RAGï¼ˆRetrieval Augmented Generationï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰é€šè¿‡åœ¨ç”Ÿæˆå‰å…ˆæ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼Œè®©æ¨¡å‹åŸºäºçœŸå®æ•°æ®å›ç­”é—®é¢˜ï¼Œè€Œéä»…ä¾èµ–å‚æ•°åŒ–è®°å¿†ã€‚

RAG çš„æ ¸å¿ƒç†å¿µå¾ˆç®€å•ï¼š**å…ˆæ‰¾åˆ°ç›¸å…³å†…å®¹ï¼Œå†è®© LLM åŸºäºè¿™äº›å†…å®¹å›ç­”**ã€‚è¿™å°† LLM ä»"å‡­è®°å¿†å›ç­”"å‡çº§ä¸º"æŸ¥èµ„æ–™åå›ç­”"ï¼Œæ˜¾è‘—æé«˜äº†å›ç­”çš„å‡†ç¡®æ€§å’Œå¯ä¿¡åº¦ã€‚

[ğŸ”— LangChain RAG å®˜æ–¹æ•™ç¨‹](https://python.langchain.com/docs/tutorials/rag/){target="_blank" rel="noopener"} Â· [ğŸ”— Retrievers æ¦‚å¿µæ–‡æ¡£](https://python.langchain.com/docs/concepts/retrievers/){target="_blank" rel="noopener"}

::: tip å‰ç«¯ç±»æ¯”
RAG ç±»ä¼¼æœç´¢å¼•æ“çš„å·¥ä½œæ–¹å¼â€”â€”Google å…ˆä¸ºç½‘é¡µå»ºç«‹å€’æ’ç´¢å¼•ï¼ˆå¯¹åº” Embedding + Vector Storeï¼‰ï¼Œç”¨æˆ·æœç´¢æ—¶å¿«é€Ÿæ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼ˆRetrievalï¼‰ï¼Œæœ€åå°†ç»“æœå‘ˆç°ç»™ç”¨æˆ·ï¼ˆGenerationï¼‰ã€‚ä½ å¯ä»¥æŠŠå‘é‡æ•°æ®åº“çœ‹ä½œ Elasticsearchï¼ŒæŠŠ Embedding çœ‹ä½œå»ºç«‹ç´¢å¼•çš„è¿‡ç¨‹ï¼Œåªä¸è¿‡è¿™é‡Œçš„"ç´¢å¼•"æ˜¯åŸºäºè¯­ä¹‰è€Œéå…³é”®è¯ã€‚
:::

### å…ˆä¿®çŸ¥è¯†

- å·²å®Œæˆ [ç¯å¢ƒæ­å»º](/ai/langchain/guide/install)
- äº†è§£ [Agent å®æˆ˜æŒ‡å—](/ai/langchain/guide/agents) å’Œ [å·¥å…· Tools](/ai/langchain/guide/tools) çš„åŸºæœ¬ç”¨æ³•

## RAG Pipeline å…¨æ™¯

ä¸€ä¸ªå®Œæ•´çš„ RAG ç³»ç»ŸåŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼š**ç´¢å¼•é˜¶æ®µ**ï¼ˆç¦»çº¿ï¼Œä¸€æ¬¡æ€§æˆ–å®šæœŸæ‰§è¡Œï¼‰å’Œ**æŸ¥è¯¢é˜¶æ®µ**ï¼ˆåœ¨çº¿ï¼Œæ¯æ¬¡ç”¨æˆ·æé—®æ—¶æ‰§è¡Œï¼‰ã€‚

```mermaid
flowchart LR
    subgraph ç´¢å¼•é˜¶æ®µ["ç´¢å¼•é˜¶æ®µï¼ˆç¦»çº¿ï¼‰"]
        A["åŸå§‹æ–‡æ¡£<br/>PDF / HTML / DB"] --> B["Document Loader<br/>æ–‡æ¡£åŠ è½½"]
        B --> C["Text Splitter<br/>æ–‡æœ¬åˆ‡åˆ†"]
        C --> D["Embedding Model<br/>å‘é‡åŒ–"]
        D --> E[("Vector Store<br/>å‘é‡æ•°æ®åº“")]
    end

    subgraph æŸ¥è¯¢é˜¶æ®µ["æŸ¥è¯¢é˜¶æ®µï¼ˆåœ¨çº¿ï¼‰"]
        F["ç”¨æˆ·æé—®"] --> G["Query Embedding<br/>é—®é¢˜å‘é‡åŒ–"]
        G --> H["Retriever<br/>ç›¸ä¼¼åº¦æ£€ç´¢"]
        E -.-> H
        H --> I["ç›¸å…³æ–‡æ¡£ç‰‡æ®µ"]
        I --> J["LLM Agent<br/>åŸºäºæ–‡æ¡£ç”Ÿæˆå›ç­”"]
        J --> K["æœ€ç»ˆå›ç­”"]
    end

    style A fill:#e1f5ff
    style E fill:#fff9c4
    style K fill:#c8e6c9
```

**å…³é”®ç†è§£ï¼š**

1. ç´¢å¼•é˜¶æ®µå°†æ–‡æ¡£è½¬æ¢ä¸ºå‘é‡å¹¶å­˜å‚¨ï¼Œè¿™æ˜¯ä¸€æ¬¡æ€§çš„å‰ç½®å·¥ä½œ
2. æŸ¥è¯¢é˜¶æ®µå°†ç”¨æˆ·é—®é¢˜ä¹Ÿè½¬æ¢ä¸ºå‘é‡ï¼Œåœ¨å‘é‡ç©ºé—´ä¸­æ‰¾åˆ°æœ€ç›¸ä¼¼çš„æ–‡æ¡£ç‰‡æ®µ
3. æ‰¾åˆ°çš„æ–‡æ¡£ç‰‡æ®µä½œä¸ºä¸Šä¸‹æ–‡æ³¨å…¥ LLM çš„æç¤ºè¯ä¸­ï¼ŒLLM æ®æ­¤ç”Ÿæˆå›ç­”
4. æ•´ä¸ªè¿‡ç¨‹ä¸­ LLM ä¸éœ€è¦"è®°ä½"æ–‡æ¡£å†…å®¹ï¼Œåªéœ€è¦ç†è§£å¹¶å›ç­”

## Document Loaders æ–‡æ¡£åŠ è½½

LangChain æä¾›äº†ä¸°å¯Œçš„ Document Loaderï¼Œæ”¯æŒä»å„ç§æ•°æ®æºåŠ è½½æ–‡æ¡£ï¼š

```python
from langchain_community.document_loaders import (
    PyPDFLoader,
    TextLoader,
    WebBaseLoader,
    CSVLoader,
    DirectoryLoader,
)

# 1. åŠ è½½ PDF æ–‡ä»¶
pdf_loader = PyPDFLoader("knowledge_base/product_manual.pdf")
pdf_docs = pdf_loader.load()
# æ¯é¡µä½œä¸ºä¸€ä¸ª Documentï¼ŒåŒ…å« page_content å’Œ metadata

# 2. åŠ è½½çº¯æ–‡æœ¬æ–‡ä»¶
text_loader = TextLoader("knowledge_base/faq.txt", encoding="utf-8")
text_docs = text_loader.load()

# 3. åŠ è½½ç½‘é¡µå†…å®¹
web_loader = WebBaseLoader("https://docs.example.com/api-guide")
web_docs = web_loader.load()

# 4. åŠ è½½ CSV æ•°æ®
csv_loader = CSVLoader("knowledge_base/products.csv")
csv_docs = csv_loader.load()
# æ¯è¡Œä½œä¸ºä¸€ä¸ª Document

# 5. æ‰¹é‡åŠ è½½ç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶
dir_loader = DirectoryLoader(
    "knowledge_base/",
    glob="**/*.md",          # åŒ¹é…æ¨¡å¼
    loader_cls=TextLoader,   # ä½¿ç”¨çš„åŠ è½½å™¨
    loader_kwargs={"encoding": "utf-8"},
)
all_docs = dir_loader.load()

# Document æ•°æ®ç»“æ„
print(pdf_docs[0].page_content[:100])  # æ–‡æ¡£æ–‡æœ¬å†…å®¹
print(pdf_docs[0].metadata)            # {"source": "...", "page": 0}
```

**Document ç»“æ„è¯´æ˜ï¼š**

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `page_content` | `str` | æ–‡æ¡£çš„æ–‡æœ¬å†…å®¹ |
| `metadata` | `dict` | å…ƒæ•°æ®ï¼ˆæ¥æºã€é¡µç ã€æ—¥æœŸç­‰ï¼‰ï¼Œå¯ç”¨äºåç»­è¿‡æ»¤ |

## Text Splitters æ–‡æœ¬åˆ‡åˆ†

æ–‡æ¡£é€šå¸¸å¾ˆé•¿ï¼Œè€Œ LLM çš„ä¸Šä¸‹æ–‡çª—å£æœ‰é™ï¼Œä¸”è¿‡é•¿çš„ä¸Šä¸‹æ–‡ä¼šé™ä½æ£€ç´¢ç²¾åº¦ã€‚å› æ­¤éœ€è¦å°†æ–‡æ¡£åˆ‡åˆ†ä¸ºåˆç†å¤§å°çš„ç‰‡æ®µï¼ˆchunksï¼‰ã€‚

### RecursiveCharacterTextSplitterï¼ˆæ¨èï¼‰

é€’å½’å­—ç¬¦åˆ‡åˆ†å™¨æŒ‰ä¼˜å…ˆçº§ä¾æ¬¡å°è¯• `\n\n` â†’ `\n` â†’ ` ` â†’ `""` è¿›è¡Œåˆ‡åˆ†ï¼Œå°½å¯èƒ½ä¿æŒæ®µè½å’Œå¥å­çš„å®Œæ•´æ€§ï¼š

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,       # æ¯ä¸ªç‰‡æ®µçš„æœ€å¤§å­—ç¬¦æ•°
    chunk_overlap=200,     # ç›¸é‚»ç‰‡æ®µçš„é‡å å­—ç¬¦æ•°ï¼ˆä¿æŒä¸Šä¸‹æ–‡è¿è´¯ï¼‰
    separators=["\n\n", "\n", "ã€‚", "ï¼Œ", " ", ""],  # ä¸­æ–‡å‹å¥½çš„åˆ†éš”ç¬¦
    length_function=len,
)

chunks = splitter.split_documents(all_docs)
print(f"åŸå§‹æ–‡æ¡£: {len(all_docs)} ä¸ª â†’ åˆ‡åˆ†å: {len(chunks)} ä¸ªç‰‡æ®µ")
```

### Token-based åˆ‡åˆ†

å½“éœ€è¦ç²¾ç¡®æ§åˆ¶ token æ•°æ—¶ï¼ˆå¦‚ç¡®ä¿æ¯ä¸ªç‰‡æ®µä¸è¶…è¿‡æ¨¡å‹ token é™åˆ¶ï¼‰ï¼š

```python
from langchain_text_splitters import TokenTextSplitter

token_splitter = TokenTextSplitter(
    chunk_size=500,        # æ¯ä¸ªç‰‡æ®µçš„æœ€å¤§ token æ•°
    chunk_overlap=50,      # é‡å  token æ•°
    encoding_name="cl100k_base",  # OpenAI æ¨¡å‹çš„ tokenizer
)

token_chunks = token_splitter.split_documents(all_docs)
```

### åˆ‡åˆ†ç­–ç•¥é€‰æ‹©

| åˆ‡åˆ†å™¨ | é€‚ç”¨åœºæ™¯ | ç‰¹ç‚¹ |
|--------|---------|------|
| `RecursiveCharacterTextSplitter` | é€šç”¨åœºæ™¯ï¼ˆæ¨èï¼‰ | ä¿æŒæ®µè½å®Œæ•´æ€§ï¼Œä¸­æ–‡å‹å¥½ |
| `TokenTextSplitter` | éœ€è¦ç²¾ç¡® token æ§åˆ¶ | æŒ‰ token åˆ‡åˆ†ï¼Œé€‚é…ç‰¹å®šæ¨¡å‹ |
| `MarkdownHeaderTextSplitter` | Markdown æ–‡æ¡£ | æŒ‰æ ‡é¢˜å±‚çº§åˆ‡åˆ†ï¼Œä¿ç•™ç»“æ„ |
| `HTMLSectionSplitter` | HTML æ–‡æ¡£ | æŒ‰ HTML æ ‡ç­¾ç»“æ„åˆ‡åˆ† |

## Embedding Models å‘é‡åµŒå…¥

Embedding æ¨¡å‹å°†æ–‡æœ¬è½¬æ¢ä¸ºé«˜ç»´å‘é‡ï¼Œä½¿å¾—è¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬åœ¨å‘é‡ç©ºé—´ä¸­è·ç¦»æ›´è¿‘ï¼š

```python
from langchain.embeddings import init_embeddings

# ä½¿ç”¨ç»Ÿä¸€æ¥å£åˆå§‹åŒ– Embedding æ¨¡å‹
embeddings = init_embeddings("openai:text-embedding-3-small")

# æˆ–ä½¿ç”¨å…¶ä»–æä¾›å•†
# embeddings = init_embeddings("anthropic:voyage-3")
# embeddings = init_embeddings("huggingface:BAAI/bge-small-zh-v1.5")

# åµŒå…¥å•æ¡æ–‡æœ¬
vector = embeddings.embed_query("LangChain æ˜¯ä»€ä¹ˆï¼Ÿ")
print(f"å‘é‡ç»´åº¦: {len(vector)}")  # 1536ï¼ˆtext-embedding-3-smallï¼‰

# æ‰¹é‡åµŒå…¥æ–‡æ¡£
doc_vectors = embeddings.embed_documents([
    "LangChain æ˜¯ä¸€ä¸ª LLM åº”ç”¨å¼€å‘æ¡†æ¶",
    "RAG é€šè¿‡æ£€ç´¢å¢å¼ºæ¨¡å‹çš„å›ç­”è´¨é‡",
])
```

**é€‰å‹å»ºè®®ï¼š**

| æ¨¡å‹ | ç»´åº¦ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|---------|
| `openai:text-embedding-3-small` | 1536 | æ€§ä»·æ¯”é«˜ï¼Œè‹±æ–‡ä¼˜ç§€ | é€šç”¨è‹±æ–‡åœºæ™¯ |
| `openai:text-embedding-3-large` | 3072 | é«˜ç²¾åº¦ï¼Œæ”¯æŒç»´åº¦è£å‰ª | é«˜ç²¾åº¦éœ€æ±‚ |
| `huggingface:BAAI/bge-small-zh-v1.5` | 512 | ä¸­æ–‡æ•ˆæœå¥½ï¼Œå¯æœ¬åœ°éƒ¨ç½² | ä¸­æ–‡åœºæ™¯ã€ç§æœ‰åŒ–éƒ¨ç½² |

## Vector Stores å‘é‡æ•°æ®åº“

å‘é‡æ•°æ®åº“å­˜å‚¨ Embedding å‘é‡å¹¶æä¾›é«˜æ•ˆçš„ç›¸ä¼¼åº¦æœç´¢ï¼š

### Chromaï¼ˆæœ¬åœ°å¼€å‘æ¨èï¼‰

```python
from langchain_chroma import Chroma

# ä»æ–‡æ¡£åˆ›å»ºå‘é‡åº“ï¼ˆè‡ªåŠ¨åµŒå…¥ï¼‰
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    collection_name="knowledge_base",
    persist_directory="./chroma_db",  # æŒä¹…åŒ–åˆ°æœ¬åœ°
)

# æˆ–åŠ è½½å·²æœ‰çš„å‘é‡åº“
vectorstore = Chroma(
    collection_name="knowledge_base",
    embedding_function=embeddings,
    persist_directory="./chroma_db",
)
```

### FAISSï¼ˆé«˜æ€§èƒ½æœ¬åœ°æ–¹æ¡ˆï¼‰

```python
from langchain_community.vectorstores import FAISS

# åˆ›å»º FAISS å‘é‡åº“
vectorstore = FAISS.from_documents(chunks, embeddings)

# ä¿å­˜ä¸åŠ è½½
vectorstore.save_local("./faiss_index")
vectorstore = FAISS.load_local(
    "./faiss_index", embeddings, allow_dangerous_deserialization=True
)
```

### Pineconeï¼ˆç”Ÿäº§çº§äº‘æœåŠ¡ï¼‰

```python
from langchain_pinecone import PineconeVectorStore

vectorstore = PineconeVectorStore.from_documents(
    documents=chunks,
    embedding=embeddings,
    index_name="knowledge-base",
)
```

### å‘é‡æ•°æ®åº“å¯¹æ¯”

| å‘é‡åº“ | éƒ¨ç½²æ–¹å¼ | é€‚ç”¨åœºæ™¯ | å®‰è£… |
|--------|---------|---------|------|
| Chroma | æœ¬åœ°/åµŒå…¥å¼ | å¼€å‘æµ‹è¯•ã€å°è§„æ¨¡æ•°æ® | `pip install langchain-chroma` |
| FAISS | æœ¬åœ° | é«˜æ€§èƒ½æœ¬åœ°æœç´¢ | `pip install faiss-cpu` |
| Pinecone | äº‘æœåŠ¡ | ç”Ÿäº§ç¯å¢ƒã€å¤§è§„æ¨¡æ•°æ® | `pip install langchain-pinecone` |
| Milvus | è‡ªéƒ¨ç½²/äº‘ | ä¼ä¸šçº§ã€é«˜å¯ç”¨ | `pip install langchain-milvus` |

## Retriever æ£€ç´¢å™¨

Retriever æ˜¯ LangChain çš„æ£€ç´¢æŠ½è±¡å±‚ï¼Œä»å‘é‡åº“ä¸­æ£€ç´¢ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µï¼š

```python
# ä»å‘é‡åº“åˆ›å»ºæ£€ç´¢å™¨
retriever = vectorstore.as_retriever(
    search_type="similarity",     # ç›¸ä¼¼åº¦æœç´¢
    search_kwargs={
        "k": 4,                   # è¿”å› top-4 ç»“æœ
    },
)

# æ‰§è¡Œæ£€ç´¢
docs = retriever.invoke("LangChain çš„æ ¸å¿ƒç»„ä»¶æœ‰å“ªäº›ï¼Ÿ")
for doc in docs:
    print(f"[æ¥æº: {doc.metadata.get('source', 'æœªçŸ¥')}]")
    print(doc.page_content[:200])
    print("---")
```

### æ£€ç´¢ç­–ç•¥

```python
# ç­–ç•¥ 1ï¼šç›¸ä¼¼åº¦æœç´¢ï¼ˆé»˜è®¤ï¼‰
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 4})

# ç­–ç•¥ 2ï¼šç›¸ä¼¼åº¦ + åˆ†æ•°é˜ˆå€¼
retriever = vectorstore.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={"score_threshold": 0.7, "k": 10},
)

# ç­–ç•¥ 3ï¼šMMRï¼ˆæœ€å¤§è¾¹é™…ç›¸å…³æ€§ï¼‰â€” å…¼é¡¾ç›¸å…³æ€§ä¸å¤šæ ·æ€§
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 4, "fetch_k": 20, "lambda_mult": 0.5},
)
```

| ç­–ç•¥ | è¯´æ˜ | é€‚ç”¨åœºæ™¯ |
|------|------|---------|
| `similarity` | çº¯ç›¸ä¼¼åº¦æ’åº | ç²¾ç¡®åŒ¹é…éœ€æ±‚ |
| `similarity_score_threshold` | è¿‡æ»¤ä½åˆ†ç»“æœ | é¿å…å¼•å…¥å™ªéŸ³ |
| `mmr` | å…¼é¡¾ç›¸å…³æ€§ä¸ç»“æœå¤šæ ·æ€§ | å¹¿æ³›ä¿¡æ¯æ”¶é›† |

## å°†æ£€ç´¢é›†æˆåˆ° Agent

LangChain 1.0 ä¸­ï¼Œæœ€ä½³å®è·µæ˜¯å°†æ£€ç´¢å™¨å°è£…ä¸ºå·¥å…·ï¼Œè®© Agent è‡ªä¸»å†³å®šä½•æ—¶æ£€ç´¢ï¼š

```python
from langchain.agents import create_agent
from langchain.tools import tool
from langchain_chroma import Chroma
from langchain.embeddings import init_embeddings

# åˆå§‹åŒ–å‘é‡åº“ï¼ˆå‡è®¾å·²å»ºå¥½ç´¢å¼•ï¼‰
embeddings = init_embeddings("openai:text-embedding-3-small")
vectorstore = Chroma(
    collection_name="knowledge_base",
    embedding_function=embeddings,
    persist_directory="./chroma_db",
)
retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

@tool
def search_knowledge_base(query: str) -> str:
    """åœ¨äº§å“çŸ¥è¯†åº“ä¸­æœç´¢ç›¸å…³æ–‡æ¡£ã€‚å½“ç”¨æˆ·è¯¢é—®äº§å“åŠŸèƒ½ã€ä½¿ç”¨æ–¹æ³•ã€
    å¸¸è§é—®é¢˜ç­‰ä¸äº§å“çŸ¥è¯†ç›¸å…³çš„é—®é¢˜æ—¶ä½¿ç”¨æ­¤å·¥å…·ã€‚

    Args:
        query: æœç´¢å…³é”®è¯æˆ–é—®é¢˜æè¿°
    """
    docs = retriever.invoke(query)
    if not docs:
        return "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚"
    results = []
    for i, doc in enumerate(docs, 1):
        source = doc.metadata.get("source", "æœªçŸ¥æ¥æº")
        results.append(f"[æ–‡æ¡£ {i} | æ¥æº: {source}]\n{doc.page_content}")
    return "\n\n---\n\n".join(results)

agent = create_agent(
    model="anthropic:claude-sonnet-4-5-20250929",
    tools=[search_knowledge_base],
    prompt="""ä½ æ˜¯ä¸€ä¸ªäº§å“çŸ¥è¯†åŠ©æ‰‹ã€‚å›ç­”ç”¨æˆ·é—®é¢˜æ—¶ï¼š
1. ä¼˜å…ˆä½¿ç”¨çŸ¥è¯†åº“æœç´¢å·¥å…·è·å–ç›¸å…³æ–‡æ¡£
2. åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹å›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
3. å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œå¦‚å®å‘ŠçŸ¥ç”¨æˆ·
4. å›ç­”æ—¶å¼•ç”¨æ–‡æ¡£æ¥æº""",
)
```

## å®Œæ•´ RAG ç¤ºä¾‹ï¼šæ–‡æ¡£é—®ç­” Agent

ä»¥ä¸‹æ˜¯ä¸€ä¸ªä»é›¶å¼€å§‹çš„å®Œæ•´ RAG åº”ç”¨ï¼ŒåŒ…å«ç´¢å¼•æ„å»ºå’Œé—®ç­”äº¤äº’ï¼š

```python
from langchain.agents import create_agent
from langchain.tools import tool
from langchain.embeddings import init_embeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_chroma import Chroma
from langgraph.checkpoint.memory import InMemorySaver

# ========== Step 1: ç´¢å¼•æ„å»ºï¼ˆç¦»çº¿æ‰§è¡Œä¸€æ¬¡ï¼‰ ==========

# åŠ è½½æ–‡æ¡£
loader = DirectoryLoader(
    "knowledge_base/",
    glob="**/*.md",
    loader_cls=TextLoader,
    loader_kwargs={"encoding": "utf-8"},
)
docs = loader.load()
print(f"åŠ è½½ {len(docs)} ä¸ªæ–‡æ¡£")

# åˆ‡åˆ†
splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,
    chunk_overlap=150,
    separators=["\n\n", "\n", "ã€‚", "ï¼Œ", " ", ""],
)
chunks = splitter.split_documents(docs)
print(f"åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªç‰‡æ®µ")

# å‘é‡åŒ–å¹¶å­˜å‚¨
embeddings = init_embeddings("openai:text-embedding-3-small")
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    collection_name="docs_qa",
    persist_directory="./chroma_db",
)

# ========== Step 2: æ„å»ºé—®ç­” Agent ==========

retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 5, "fetch_k": 20},
)

@tool
def search_docs(query: str) -> str:
    """åœ¨æ–‡æ¡£åº“ä¸­æœç´¢ä¸é—®é¢˜ç›¸å…³çš„å†…å®¹ã€‚
    å¯¹äºä»»ä½•å…³äºé¡¹ç›®æ–‡æ¡£ã€API è¯´æ˜ã€ä½¿ç”¨æŒ‡å—çš„é—®é¢˜éƒ½åº”è¯¥ä½¿ç”¨æ­¤å·¥å…·ã€‚

    Args:
        query: æœç´¢å…³é”®è¯æˆ–é—®é¢˜ï¼Œå°½é‡ä½¿ç”¨æ ¸å¿ƒæ¦‚å¿µè¯
    """
    docs = retriever.invoke(query)
    if not docs:
        return "æœªåœ¨æ–‡æ¡£åº“ä¸­æ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚"
    results = []
    for i, doc in enumerate(docs, 1):
        source = doc.metadata.get("source", "æœªçŸ¥")
        results.append(f"[{i}] æ¥æº: {source}\n{doc.page_content}")
    return "\n\n".join(results)

agent = create_agent(
    model="anthropic:claude-sonnet-4-5-20250929",
    tools=[search_docs],
    prompt="""ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚è¯·éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š
1. å›ç­”é—®é¢˜å‰ï¼Œå…ˆé€šè¿‡æœç´¢å·¥å…·æŸ¥æ‰¾ç›¸å…³æ–‡æ¡£
2. ä¸¥æ ¼åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹å›ç­”ï¼Œä¸è¦å‡­ç©ºç¼–é€ 
3. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·"æ–‡æ¡£ä¸­æš‚æ— æ­¤ä¿¡æ¯"
4. å›ç­”æ—¶æ ‡æ³¨ä¿¡æ¯æ¥æºï¼Œæ–¹ä¾¿ç”¨æˆ·æŸ¥è¯
5. å¯¹äºå¤æ‚é—®é¢˜ï¼Œå¯ä»¥å¤šæ¬¡æœç´¢ä¸åŒå…³é”®è¯""",
    checkpointer=InMemorySaver(),
)

# ========== Step 3: é—®ç­”äº¤äº’ ==========

config = {"configurable": {"thread_id": "qa-session-1"}}

result = agent.invoke(
    {"messages": [{"role": "user", "content": "é¡¹ç›®çš„è®¤è¯æœºåˆ¶æ˜¯æ€ä¹ˆå®ç°çš„ï¼Ÿ"}]},
    config,
)
print(result["messages"][-1].content)
```

## è´¨é‡ä¼˜åŒ–æŠ€å·§

### 1. Chunk Size è°ƒä¼˜

```python
# å° chunkï¼ˆ300-500 å­—ç¬¦ï¼‰ï¼šç²¾ç¡®åŒ¹é…ï¼Œä½†å¯èƒ½ä¸¢å¤±ä¸Šä¸‹æ–‡
splitter_precise = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)

# å¤§ chunkï¼ˆ1000-1500 å­—ç¬¦ï¼‰ï¼šä¸Šä¸‹æ–‡ä¸°å¯Œï¼Œä½†å¯èƒ½å¼•å…¥å™ªéŸ³
splitter_broad = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=200)

# ç»éªŒå€¼ï¼š800-1000 å­—ç¬¦ã€150-200 é‡å æ˜¯å¤šæ•°åœºæ™¯çš„æœ€ä½³èµ·ç‚¹
```

### 2. Metadata è¿‡æ»¤

åˆ©ç”¨æ–‡æ¡£å…ƒæ•°æ®ç¼©å°æ£€ç´¢èŒƒå›´ï¼Œæé«˜ç²¾åº¦ï¼š

```python
from langchain_core.documents import Document

# åœ¨åŠ è½½æ—¶æ·»åŠ å…ƒæ•°æ®
docs_with_metadata = [
    Document(
        page_content="API è®¤è¯é‡‡ç”¨ JWT Token æ–¹æ¡ˆ...",
        metadata={"source": "api-guide.md", "category": "auth", "version": "2.0"},
    ),
]

# æ£€ç´¢æ—¶æŒ‰å…ƒæ•°æ®è¿‡æ»¤
retriever = vectorstore.as_retriever(
    search_kwargs={
        "k": 4,
        "filter": {"category": "auth"},  # ä»…æœç´¢è®¤è¯ç›¸å…³æ–‡æ¡£
    },
)
```

### 3. æŸ¥è¯¢ä¼˜åŒ–

```python
@tool
def smart_search(query: str) -> str:
    """æ™ºèƒ½æœç´¢ï¼šå…ˆæœç´¢åŸå§‹é—®é¢˜ï¼Œè‹¥ç»“æœä¸è¶³ï¼Œè‡ªåŠ¨æ‰©å±•å…³é”®è¯ã€‚

    Args:
        query: ç”¨æˆ·çš„åŸå§‹é—®é¢˜
    """
    # ç¬¬ä¸€è½®ï¼šç›´æ¥æœç´¢
    docs = retriever.invoke(query)

    # å¦‚æœç»“æœä¸è¶³ï¼Œå°è¯•æå–å…³é”®è¯é‡æ–°æœç´¢
    if len(docs) < 2:
        # å¯ä»¥ç”¨ LLM æå–å…³é”®è¯
        keywords = extract_keywords(query)
        docs.extend(retriever.invoke(keywords))

    # å»é‡
    seen = set()
    unique_docs = []
    for doc in docs:
        content_hash = hash(doc.page_content[:100])
        if content_hash not in seen:
            seen.add(content_hash)
            unique_docs.append(doc)

    return format_results(unique_docs)
```

### 4. ä¼˜åŒ–æ£€æŸ¥æ¸…å•

| ä¼˜åŒ–æ–¹å‘ | æ–¹æ³• | æ•ˆæœ |
|---------|------|------|
| chunk_size | æ ¹æ®æ–‡æ¡£ç±»å‹è°ƒæ•´ï¼ˆFAQ ç”¨å° chunkï¼Œæ–‡ç« ç”¨å¤§ chunkï¼‰ | æé«˜æ£€ç´¢ç²¾åº¦ |
| chunk_overlap | è®¾ç½® 15-20% çš„é‡å ç‡ | é˜²æ­¢ä¿¡æ¯åœ¨åˆ‡åˆ†è¾¹ç•Œä¸¢å¤± |
| metadata | æ·»åŠ  categoryã€dateã€version ç­‰æ ‡ç­¾ | æ”¯æŒç²¾ç¡®è¿‡æ»¤ |
| embedding æ¨¡å‹ | ä¸­æ–‡åœºæ™¯é€‰ç”¨ bge ç³»åˆ— | æå‡ä¸­æ–‡è¯­ä¹‰ç†è§£ |
| æ£€ç´¢ç­–ç•¥ | ä½¿ç”¨ MMR æ›¿ä»£çº¯ç›¸ä¼¼åº¦ | å¢åŠ ç»“æœå¤šæ ·æ€§ |
| å¤šè½®æ£€ç´¢ | é¦–è½®ç»“æœä¸è¶³æ—¶æ‰©å±•å…³é”®è¯é‡æœ | æé«˜å¬å›ç‡ |

## å¸¸è§é—®é¢˜

**Q: å‘é‡æ•°æ®åº“åº”è¯¥å¤šä¹…æ›´æ–°ä¸€æ¬¡ï¼Ÿ**

A: å–å†³äºæ•°æ®æºçš„å˜åŒ–é¢‘ç‡ã€‚å¯¹äºé™æ€æ–‡æ¡£ï¼ˆå¦‚äº§å“æ‰‹å†Œï¼‰ï¼Œå»ºç´¢å¼•ä¸€æ¬¡å³å¯ï¼›å¯¹äºåŠ¨æ€æ•°æ®ï¼ˆå¦‚æ–°é—»ã€å·¥å•ï¼‰ï¼Œå»ºè®®è®¾ç½®å®šæ—¶ä»»åŠ¡å¢é‡æ›´æ–°ã€‚Chroma å’Œ Pinecone éƒ½æ”¯æŒå¢é‡æ·»åŠ æ–‡æ¡£ã€‚

**Q: chunk_size è®¾å¤šå¤§åˆé€‚ï¼Ÿ**

A: æ²¡æœ‰ä¸‡èƒ½å€¼ï¼Œéœ€è¦æ ¹æ®æ–‡æ¡£ç±»å‹å®éªŒã€‚ä¸€èˆ¬å»ºè®®ï¼šFAQ ç±» 300-500 å­—ç¬¦ï¼ŒæŠ€æœ¯æ–‡æ¡£ 800-1000 å­—ç¬¦ï¼Œé•¿æ–‡åˆ†æ 1200-1500 å­—ç¬¦ã€‚overlap è®¾ä¸º chunk_size çš„ 15-20%ã€‚

**Q: æ£€ç´¢å‡ºçš„æ–‡æ¡£ä¸ç›¸å…³æ€ä¹ˆåŠï¼Ÿ**

A: æ’æŸ¥æ­¥éª¤ï¼š
1. æ£€æŸ¥ embedding æ¨¡å‹æ˜¯å¦é€‚åˆä½ çš„è¯­è¨€ï¼ˆä¸­æ–‡ç”¨ bgeï¼Œè‹±æ–‡ç”¨ OpenAIï¼‰
2. è°ƒæ•´ chunk_sizeï¼Œè¿‡å¤§æˆ–è¿‡å°éƒ½ä¼šå½±å“ç²¾åº¦
3. æ·»åŠ  metadata è¿‡æ»¤ï¼Œç¼©å°æ£€ç´¢èŒƒå›´
4. å°è¯• MMR ç­–ç•¥å¢åŠ ç»“æœå¤šæ ·æ€§
5. åœ¨å·¥å…·æè¿°ä¸­å¼•å¯¼ LLM ä½¿ç”¨æ›´ç²¾ç¡®çš„æœç´¢å…³é”®è¯

**Q: RAG å’Œ Fine-tuning å¦‚ä½•é€‰æ‹©ï¼Ÿ**

A: RAG é€‚åˆçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ï¼ˆé—®ç­”ã€æ–‡æ¡£æ£€ç´¢ï¼‰ï¼Œä¼˜ç‚¹æ˜¯æ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ã€çŸ¥è¯†å¯å®æ—¶æ›´æ–°ã€‚Fine-tuning é€‚åˆè°ƒæ•´æ¨¡å‹çš„è¡Œä¸ºæ¨¡å¼ï¼ˆé£æ ¼ã€æ ¼å¼ï¼‰ã€‚ä¸¤è€…å¯ä»¥ç»„åˆä½¿ç”¨ï¼šFine-tune æ¨¡å‹çš„å›ç­”é£æ ¼ï¼ŒRAG æä¾›çŸ¥è¯†æ¥æºã€‚

## ä¸‹ä¸€æ­¥

- [Agent å®æˆ˜æŒ‡å—](/ai/langchain/guide/agents) -- äº†è§£ Agent å¦‚ä½•ç¼–æ’å·¥å…·è°ƒç”¨
- [å·¥å…· Tools](/ai/langchain/guide/tools) -- æ·±å…¥å·¥å…·å®šä¹‰ä¸è¿è¡Œæ—¶æ³¨å…¥
- [é•¿æœŸè®°å¿†](/ai/langchain/guide/long-term-memory) -- è·¨å¯¹è¯æŒä¹…åŒ–ç”¨æˆ·ä¿¡æ¯
- [æµå¼å“åº” Streaming](/ai/langchain/guide/streaming) -- åœ¨æ£€ç´¢è¿‡ç¨‹ä¸­æä¾›å®æ—¶è¿›åº¦

## å‚è€ƒèµ„æº

- [LangChain RAG å®˜æ–¹æ•™ç¨‹](https://python.langchain.com/docs/tutorials/rag/)
- [LangChain Retrievers æ¦‚å¿µæ–‡æ¡£](https://python.langchain.com/docs/concepts/retrievers/)
- [LangChain Text Splitters æ–‡æ¡£](https://python.langchain.com/docs/concepts/text_splitters/)
- [Chroma å®˜æ–¹æ–‡æ¡£](https://docs.trychroma.com/)
- [FAISS å®˜æ–¹æ–‡æ¡£](https://faiss.ai/)
