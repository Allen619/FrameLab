---
title: æ¨¡å‹ï¼ˆModelsï¼‰
description: æŒæ¡ LangChain çš„ç»Ÿä¸€æ¨¡å‹æ¥å£ï¼Œå­¦ä¹ å¦‚ä½•åˆå§‹åŒ–ã€é…ç½®å’Œåˆ‡æ¢ä¸åŒ Provider çš„èŠå¤©æ¨¡å‹
---

# æ¨¡å‹ï¼ˆModelsï¼‰

## æ¦‚è¿°

æ¨¡å‹æ˜¯ LangChain çš„åŸºç¡€å±‚â€”â€”æ‰€æœ‰ Agentã€Chainã€å·¥å…·è°ƒç”¨éƒ½å»ºç«‹åœ¨æ¨¡å‹ä¹‹ä¸Šã€‚LangChain æä¾›äº†ç»Ÿä¸€çš„æ¨¡å‹æ¥å£ï¼Œè®©ä½ å¯ä»¥ç”¨ç›¸åŒçš„ä»£ç ä¸ Anthropic Claudeã€OpenAI GPTã€Google Gemini ç­‰ä¸åŒ Provider äº¤äº’ï¼Œå¹¶åœ¨å®ƒä»¬ä¹‹é—´æ— ç¼åˆ‡æ¢ã€‚

::: tip å‰ç«¯ç±»æ¯”
æ¨¡å‹æ¥å£ç±»ä¼¼äºå‰ç«¯çš„ HTTP è¯·æ±‚é€‚é…å™¨æ¨¡å¼ã€‚å°±åƒ axios å°è£…äº† `fetch` å’Œ `XMLHttpRequest` çš„å·®å¼‚ï¼Œè®©ä½ ç”¨åŒä¸€å¥— API å‘èµ·è¯·æ±‚ä¸€æ ·ï¼ŒLangChain çš„ `init_chat_model` å°è£…äº†ä¸åŒ LLM Provider çš„å·®å¼‚ï¼Œè®©ä½ ç”¨åŒä¸€å¥—æ¥å£è°ƒç”¨ä»»ä½•æ¨¡å‹ã€‚åˆ‡æ¢ Provider å°±åƒåˆ‡æ¢ axios çš„ `baseURL`ã€‚
:::

ä» LangChain çš„åŸç”Ÿè¯­ä¹‰æ¥çœ‹ï¼Œ`init_chat_model` æ˜¯ä¸€ä¸ªå·¥å‚å‡½æ•°ï¼Œå®ƒæ ¹æ®æ¨¡å‹åç§°æˆ– `model_provider` å‚æ•°è‡ªåŠ¨é€‰æ‹©å¯¹åº”çš„ Provider å®ç°ç±»ï¼ˆå¦‚ `ChatAnthropic`ã€`ChatOpenAI`ï¼‰ï¼Œè¿”å›ä¸€ä¸ªéµå¾ª `BaseChatModel` æ¥å£çš„å®ä¾‹ã€‚æ‰€æœ‰æ¨¡å‹å®ä¾‹éƒ½æ”¯æŒ `.invoke()`ã€`.stream()`ã€`.bind_tools()`ã€`.with_structured_output()` ç­‰æ ‡å‡†æ–¹æ³•ã€‚

[ğŸ”— init_chat_model API å‚è€ƒ](https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html){target="_blank" rel="noopener"} Â· [ğŸ”— Chat Model é›†æˆåˆ—è¡¨](https://python.langchain.com/docs/integrations/chat/){target="_blank" rel="noopener"}

## æ ¸å¿ƒæ¦‚å¿µ

### æ¨¡å‹åˆå§‹åŒ–çš„ä¸¤ç§æ–¹å¼

LangChain æä¾›ä¸¤ç§æ–¹å¼åˆå§‹åŒ–èŠå¤©æ¨¡å‹ï¼š

| æ–¹å¼ | é€‚ç”¨åœºæ™¯ | ä¼˜åŠ¿ |
|------|---------|------|
| `init_chat_model()` | éœ€è¦è¿è¡Œæ—¶åˆ‡æ¢ Provider | ç»Ÿä¸€æ¥å£ï¼Œé…ç½®é©±åŠ¨ |
| ç›´æ¥å®ä¾‹åŒ– Provider ç±» | å›ºå®š Providerï¼Œéœ€è¦ç»†ç²’åº¦æ§åˆ¶ | ç±»å‹æç¤ºæ›´å®Œæ•´ï¼ŒIDE æ”¯æŒæ›´å¥½ |

### Provider åŒ…ç”Ÿæ€

æ¯ä¸ª LLM Provider æœ‰ç‹¬ç«‹çš„é›†æˆåŒ…ï¼š

| Provider | åŒ…å | æ¨¡å‹ç±» |
|----------|------|--------|
| Anthropic | `langchain-anthropic` | `ChatAnthropic` |
| OpenAI | `langchain-openai` | `ChatOpenAI` |
| Google | `langchain-google-genai` | `ChatGoogleGenerativeAI` |
| HuggingFace | `langchain-huggingface` | `ChatHuggingFace` |

### æ¨¡å‹è°ƒç”¨æµç¨‹

```mermaid
flowchart LR
    A["init_chat_model()"] --> B{è§£æ Provider}
    B -->|anthropic| C[ChatAnthropic]
    B -->|openai| D[ChatOpenAI]
    B -->|google-genai| E[ChatGoogleGenerativeAI]
    C --> F["ç»Ÿä¸€æ¥å£<br/>.invoke() / .stream()<br/>.bind_tools()<br/>.with_structured_output()"]
    D --> F
    E --> F
    F --> G["AIMessage<br/>content_blocks"]

    style A fill:#e1f5ff
    style F fill:#fff9c4
    style G fill:#e8f5e9
```

## ä»£ç ç¤ºä¾‹ 1: ä½¿ç”¨ init_chat_model åˆå§‹åŒ–

`init_chat_model` æ˜¯æ¨èçš„æ¨¡å‹åˆå§‹åŒ–æ–¹å¼ï¼Œæ”¯æŒæ‰€æœ‰å·²å®‰è£…çš„ Providerï¼š

```python
from langchain.chat_models import init_chat_model

# æ–¹å¼ 1: ç›´æ¥æŒ‡å®šæ¨¡å‹åç§°ï¼ˆè‡ªåŠ¨æ¨æ–­ Providerï¼‰
model = init_chat_model("claude-sonnet-4-5-20250929")

# æ–¹å¼ 2: æ˜¾å¼æŒ‡å®š Provider
model = init_chat_model(
    "claude-sonnet-4-5-20250929",
    model_provider="anthropic",
)

# æ–¹å¼ 3: å¸¦å‚æ•°åˆå§‹åŒ–
model = init_chat_model(
    "claude-sonnet-4-5-20250929",
    temperature=0.7,
    max_tokens=1000,
    timeout=30,
)

# è°ƒç”¨æ¨¡å‹
response = model.invoke("ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯ LangChain")
print(response.content)
```

**è¯´æ˜**ï¼š

- å½“æ¨¡å‹åç§°èƒ½å”¯ä¸€æ˜ å°„åˆ° Provider æ—¶ï¼ˆå¦‚ `claude-*` æ˜ å°„åˆ° Anthropicï¼‰ï¼Œå¯çœç•¥ `model_provider`
- å¦‚æœåç§°æœ‰æ­§ä¹‰ï¼Œéœ€è¦æ˜¾å¼æŒ‡å®š `model_provider` å‚æ•°
- `temperature`ã€`max_tokens`ã€`timeout` ç­‰é€šç”¨å‚æ•°ç›´æ¥ä½œä¸ºå…³é”®å­—å‚æ•°ä¼ å…¥

## ä»£ç ç¤ºä¾‹ 2: ç›´æ¥å®ä¾‹åŒ– Provider ç±»

å½“ä½ ç¡®å®šä½¿ç”¨å“ªä¸ª Provider æ—¶ï¼Œå¯ä»¥ç›´æ¥å®ä¾‹åŒ–å¯¹åº”çš„æ¨¡å‹ç±»ï¼š

```python
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI

# Anthropic Claude
claude = ChatAnthropic(
    model="claude-sonnet-4-5-20250929",
    temperature=0.7,
    max_tokens=1024,
)

# OpenAI GPT
gpt = ChatOpenAI(
    model="gpt-4o",
    temperature=0.7,
    max_tokens=1024,
)

# Google Gemini
gemini = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    temperature=0.7,
    max_tokens=1024,
)

# æ‰€æœ‰æ¨¡å‹éƒ½éµå¾ªç›¸åŒçš„æ¥å£
for m in [claude, gpt, gemini]:
    response = m.invoke("ä½ å¥½ï¼Œè¯·åšä¸ªè‡ªæˆ‘ä»‹ç»")
    print(response.content)
    print("---")
```

**å®‰è£…å¯¹åº”çš„ Provider åŒ…**ï¼š

```bash
# æ ¹æ®éœ€è¦å®‰è£…
pip install langchain-anthropic   # Anthropic Claude
pip install langchain-openai      # OpenAI GPT
pip install langchain-google-genai # Google Gemini
```

æ¯ä¸ªåŒ…éœ€è¦å¯¹åº”çš„ API Key ç¯å¢ƒå˜é‡ï¼š

```bash
export ANTHROPIC_API_KEY="sk-ant-..."
export OPENAI_API_KEY="sk-..."
export GOOGLE_API_KEY="AI..."
```

## ä»£ç ç¤ºä¾‹ 3: æ¨¡å‹å‚æ•°è¯¦è§£

æ‰€æœ‰æ¨¡å‹éƒ½æ”¯æŒä¸€ç»„é€šç”¨å‚æ•°ï¼Œç”¨äºæ§åˆ¶ç”Ÿæˆè¡Œä¸ºï¼š

```python
from langchain.chat_models import init_chat_model

model = init_chat_model(
    "claude-sonnet-4-5-20250929",
    # æ§åˆ¶éšæœºæ€§ï¼š0 = ç¡®å®šæ€§è¾“å‡ºï¼Œ1 = é«˜éšæœºæ€§
    temperature=0.3,
    # æœ€å¤§ç”Ÿæˆ token æ•°
    max_tokens=2048,
    # æ ¸é‡‡æ ·ï¼šåªä»æ¦‚ç‡æœ€é«˜çš„ top_p æ¯”ä¾‹çš„ token ä¸­é‡‡æ ·
    top_p=0.9,
    # åœæ­¢åºåˆ—ï¼šé‡åˆ°è¿™äº›å­—ç¬¦ä¸²æ—¶åœæ­¢ç”Ÿæˆ
    stop=["\n\n---", "END"],
    # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    timeout=60,
)

response = model.invoke("å†™ä¸€ç¯‡å…³äº Python çš„ç®€çŸ­ä»‹ç»")
print(response.content)
```

**å‚æ•°è¯´æ˜**ï¼š

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `temperature` | float | 1.0 | æ§åˆ¶è¾“å‡ºéšæœºæ€§ã€‚0 è¡¨ç¤ºå‡ ä¹ç¡®å®šæ€§ï¼Œé€‚åˆä»£ç ç”Ÿæˆï¼›0.7-1.0 é€‚åˆåˆ›æ„å†™ä½œ |
| `max_tokens` | int | å› æ¨¡å‹è€Œå¼‚ | é™åˆ¶ç”Ÿæˆçš„æœ€å¤§ token æ•°é‡ |
| `top_p` | float | 1.0 | æ ¸é‡‡æ ·å‚æ•°ï¼Œä¸ temperature äº’è¡¥ä½¿ç”¨ |
| `stop` | list[str] | None | é‡åˆ°æŒ‡å®šå­—ç¬¦ä¸²æ—¶æå‰åœæ­¢ç”Ÿæˆ |
| `timeout` | int | None | API è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ |

> **æç¤º**ï¼šä¸€èˆ¬å»ºè®®è°ƒæ•´ `temperature` æˆ– `top_p` å…¶ä¸­ä¹‹ä¸€ï¼Œä¸è¦åŒæ—¶è°ƒæ•´ä¸¤è€…ã€‚

## ä»£ç ç¤ºä¾‹ 4: bind_tools ç»‘å®šå·¥å…·

`bind_tools` è®©æ¨¡å‹çŸ¥é“æœ‰å“ªäº›å·¥å…·å¯ä»¥è°ƒç”¨ï¼š

```python
from langchain.chat_models import init_chat_model
from langchain.tools import tool

@tool
def get_weather(city: str) -> str:
    """è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯"""
    return f"{city}ä»Šå¤©æ™´ï¼Œæ°”æ¸© 22Â°C"

@tool
def search_docs(query: str) -> str:
    """æœç´¢æŠ€æœ¯æ–‡æ¡£"""
    return f"æ‰¾åˆ°å…³äº {query} çš„ 5 ç¯‡æ–‡æ¡£"

model = init_chat_model("claude-sonnet-4-5-20250929")

# ç»‘å®šå·¥å…·åˆ°æ¨¡å‹
model_with_tools = model.bind_tools([get_weather, search_docs])

# æ¨¡å‹ä¼šæ ¹æ®è¾“å…¥å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·
response = model_with_tools.invoke("åŒ—äº¬ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ")

# æ£€æŸ¥å·¥å…·è°ƒç”¨
if response.tool_calls:
    for tc in response.tool_calls:
        print(f"å·¥å…·: {tc['name']}")
        print(f"å‚æ•°: {tc['args']}")
else:
    print(f"ç›´æ¥å›å¤: {response.content}")
```

**è¯´æ˜**ï¼š

- `bind_tools` è¿”å›ä¸€ä¸ªæ–°çš„æ¨¡å‹å®ä¾‹ï¼Œä¸ä¼šä¿®æ”¹åŸæ¨¡å‹
- æ¨¡å‹ä¸ä¼šè‡ªåŠ¨æ‰§è¡Œå·¥å…·ï¼Œåªæ˜¯ç”Ÿæˆå·¥å…·è°ƒç”¨æŒ‡ä»¤
- å®é™…çš„å·¥å…·æ‰§è¡Œç”± Agent æ¡†æ¶å®Œæˆï¼ˆå‚è§ [æ™ºèƒ½ä½“ Agent](/ai/langchain/guide/agents)ï¼‰

## ä»£ç ç¤ºä¾‹ 5: with_structured_output ç»“æ„åŒ–è¾“å‡º

`with_structured_output` è®©æ¨¡å‹è¿”å›ç¬¦åˆæŒ‡å®š Schema çš„ç»“æ„åŒ–æ•°æ®ï¼š

```python
from langchain.chat_models import init_chat_model
from pydantic import BaseModel, Field

# å®šä¹‰è¾“å‡ºç»“æ„
class BookReview(BaseModel):
    title: str = Field(description="ä¹¦å")
    rating: int = Field(description="è¯„åˆ† 1-5")
    summary: str = Field(description="ä¸€å¥è¯æ€»ç»“")
    recommend: bool = Field(description="æ˜¯å¦æ¨è")

model = init_chat_model("claude-sonnet-4-5-20250929")

# ç»‘å®šç»“æ„åŒ–è¾“å‡º
structured_model = model.with_structured_output(BookReview)

# è¿”å›å€¼ç›´æ¥æ˜¯ BookReview å®ä¾‹
review = structured_model.invoke("è¯„ä»·ä¸€ä¸‹ã€ŠPython Cookbookã€‹è¿™æœ¬ä¹¦")

print(f"ä¹¦å: {review.title}")
print(f"è¯„åˆ†: {review.rating}/5")
print(f"æ€»ç»“: {review.summary}")
print(f"æ¨è: {'æ˜¯' if review.recommend else 'å¦'}")
```

**è¯´æ˜**ï¼š

- æ”¯æŒ Pydantic BaseModelã€TypedDictã€JSON Schema ä½œä¸ºè¾“å‡ºæ ¼å¼å®šä¹‰
- æ¨¡å‹ä¼šè‡ªåŠ¨å°†å“åº”è§£æä¸ºæŒ‡å®šç±»å‹ï¼Œè§£æå¤±è´¥ä¼šæŠ›å‡ºå¼‚å¸¸
- é€‚åˆéœ€è¦ä»éç»“æ„åŒ–æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯çš„åœºæ™¯

## ä»£ç ç¤ºä¾‹ 6: è¿è¡Œæ—¶åˆ‡æ¢ Provider

`init_chat_model` æ”¯æŒåˆ›å»ºå¯é…ç½®çš„æ¨¡å‹å®ä¾‹ï¼Œåœ¨è¿è¡Œæ—¶é€šè¿‡ config åˆ‡æ¢ Providerï¼š

```python
from langchain.chat_models import init_chat_model

# ä¸æŒ‡å®šæ¨¡å‹åç§°ï¼Œåˆ›å»ºå¯é…ç½®å®ä¾‹
configurable_model = init_chat_model(temperature=0)

# è¿è¡Œæ—¶é€šè¿‡ config åˆ‡æ¢æ¨¡å‹
# ä½¿ç”¨ Claude
response_claude = configurable_model.invoke(
    "ä½ å¥½",
    config={"configurable": {"model": "claude-sonnet-4-5-20250929"}},
)
print(f"Claude: {response_claude.content}")

# ä½¿ç”¨ GPT
response_gpt = configurable_model.invoke(
    "ä½ å¥½",
    config={"configurable": {"model": "gpt-4o"}},
)
print(f"GPT: {response_gpt.content}")

# ä½¿ç”¨ Gemini
response_gemini = configurable_model.invoke(
    "ä½ å¥½",
    config={"configurable": {"model": "gemini-2.0-flash"}},
)
print(f"Gemini: {response_gemini.content}")
```

**ä½¿ç”¨åœºæ™¯**ï¼š

- A/B æµ‹è¯•ä¸åŒæ¨¡å‹çš„è¡¨ç°
- æ ¹æ®ä»»åŠ¡å¤æ‚åº¦åŠ¨æ€é€‰æ‹©æ¨¡å‹ï¼ˆç®€å•ä»»åŠ¡ç”¨å°æ¨¡å‹ï¼Œå¤æ‚ä»»åŠ¡ç”¨å¤§æ¨¡å‹ï¼‰
- ç”¨æˆ·å¯é€‰æ¨¡å‹çš„åº”ç”¨åœºæ™¯

## ä»£ç ç¤ºä¾‹ 7: Content Blocks ç»Ÿä¸€å“åº”æ ¼å¼

æ‰€æœ‰æ¨¡å‹è¿”å›çš„ `AIMessage` éƒ½æ”¯æŒ `content_blocks` å±æ€§ï¼Œæä¾›è·¨ Provider çš„ç»Ÿä¸€å“åº”æ ¼å¼ï¼š

```python
from langchain.chat_models import init_chat_model

model = init_chat_model("claude-sonnet-4-5-20250929")
response = model.invoke("è§£é‡Šä»€ä¹ˆæ˜¯é€’å½’")

# content_blocks æä¾›ç»“æ„åŒ–çš„å†…å®¹è®¿é—®
for block in response.content_blocks:
    if block["type"] == "text":
        print(f"æ–‡æœ¬: {block['text']}")
    elif block["type"] == "reasoning":
        print(f"æ¨ç†è¿‡ç¨‹: {block['reasoning']}")
    elif block["type"] == "tool_call":
        print(f"å·¥å…·è°ƒç”¨: {block['name']}({block['args']})")
```

Content Blocks æ”¯æŒ 7 ç§æ ‡å‡†ç±»å‹ï¼š

| ç±»å‹ | è¯´æ˜ | æ”¯æŒçš„ Provider |
|------|------|----------------|
| `text` | æ–‡æœ¬å†…å®¹ | æ‰€æœ‰ Provider |
| `reasoning` | æ¨¡å‹æ¨ç†è¿‡ç¨‹ | Anthropic Claude |
| `tool_call` | å·¥å…·è°ƒç”¨æŒ‡ä»¤ | å¤§éƒ¨åˆ† Provider |
| `image` | å›¾ç‰‡å†…å®¹ | æ”¯æŒå¤šæ¨¡æ€çš„ Provider |
| `audio` | éŸ³é¢‘å†…å®¹ | æ”¯æŒéŸ³é¢‘çš„ Provider |
| `video` | è§†é¢‘å†…å®¹ | æ”¯æŒè§†é¢‘çš„ Provider |
| `file` | æ–‡ä»¶å†…å®¹ | æ”¯æŒæ–‡ä»¶çš„ Provider |

> æ›´å¤š Content Blocks ç»†èŠ‚è¯·å‚é˜… [æ¶ˆæ¯ Messages](/ai/langchain/guide/messages) ä¸“é¢˜é¡µã€‚

## ä»£ç ç¤ºä¾‹ 8: å¤šæ¨¡æ€è¾“å…¥

æ”¯æŒå¤šæ¨¡æ€çš„æ¨¡å‹å¯ä»¥æ¥æ”¶å›¾ç‰‡ã€éŸ³é¢‘ç­‰è¾“å…¥ï¼š

```python
from langchain.chat_models import init_chat_model
from langchain.messages import HumanMessage

model = init_chat_model("claude-sonnet-4-5-20250929")

# æ–¹å¼ 1: é€šè¿‡ URL å‘é€å›¾ç‰‡
message_url = HumanMessage(content_blocks=[
    {"type": "text", "text": "æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚"},
    {"type": "image", "url": "https://example.com/photo.jpg"},
])

response = model.invoke([message_url])
print(response.content)

# æ–¹å¼ 2: é€šè¿‡ base64 å‘é€å›¾ç‰‡
import base64

with open("local_image.png", "rb") as f:
    image_data = base64.b64encode(f.read()).decode()

message_b64 = HumanMessage(content_blocks=[
    {"type": "text", "text": "è¿™å¼ å›¾é‡Œæœ‰ä»€ä¹ˆï¼Ÿ"},
    {
        "type": "image",
        "base64": image_data,
        "mime_type": "image/png",
    },
])

response = model.invoke([message_b64])
print(response.content)
```

**è¯´æ˜**ï¼š

- å¤šæ¨¡æ€è¾“å…¥ä½¿ç”¨ `content_blocks` æ„é€ æ¶ˆæ¯ï¼Œæ¯ä¸ªå—æŒ‡å®š `type`
- å›¾ç‰‡æ”¯æŒ URLã€base64ã€provider æ–‡ä»¶ ID ä¸‰ç§æ–¹å¼
- ä¸åŒ Provider å¯¹å¤šæ¨¡æ€çš„æ”¯æŒç¨‹åº¦ä¸åŒï¼Œä½¿ç”¨å‰è¯·ç¡®è®¤ç›®æ ‡æ¨¡å‹çš„èƒ½åŠ›

## æœ€ä½³å®è·µ

### 1. ä¼˜å…ˆä½¿ç”¨ init_chat_model

é™¤éæœ‰ç‰¹æ®Šéœ€æ±‚ï¼Œæ¨èä½¿ç”¨ `init_chat_model` è€Œéç›´æ¥å®ä¾‹åŒ– Provider ç±»ã€‚è¿™æ ·ä»£ç æ›´å®¹æ˜“ç»´æŠ¤å’Œè¿ç§»ï¼š

```python
# æ¨èï¼šç»Ÿä¸€æ¥å£
from langchain.chat_models import init_chat_model
model = init_chat_model("claude-sonnet-4-5-20250929", temperature=0)

# ä¹Ÿå¯ä»¥ï¼šç›´æ¥å®ä¾‹åŒ–ï¼ˆéœ€è¦ Provider ç‰¹æœ‰åŠŸèƒ½æ—¶ï¼‰
from langchain_anthropic import ChatAnthropic
model = ChatAnthropic(model="claude-sonnet-4-5-20250929", temperature=0)
```

### 2. ç¯å¢ƒå˜é‡ç®¡ç† API Key

æ°¸è¿œä¸è¦åœ¨ä»£ç ä¸­ç¡¬ç¼–ç  API Keyï¼š

```python
# é”™è¯¯ï¼šç¡¬ç¼–ç 
model = init_chat_model("claude-sonnet-4-5-20250929", api_key="sk-ant-xxx")

# æ­£ç¡®ï¼šé€šè¿‡ç¯å¢ƒå˜é‡ï¼ˆæ¨¡å‹è‡ªåŠ¨è¯»å–ï¼‰
# export ANTHROPIC_API_KEY="sk-ant-xxx"
model = init_chat_model("claude-sonnet-4-5-20250929")
```

### 3. æ ¹æ®ä»»åŠ¡é€‰æ‹©å‚æ•°

```python
# ä»£ç ç”Ÿæˆ / æ•°æ®æå– â†’ ä½ temperature
code_model = init_chat_model("claude-sonnet-4-5-20250929", temperature=0)

# åˆ›æ„å†™ä½œ / å¤´è„‘é£æš´ â†’ è¾ƒé«˜ temperature
creative_model = init_chat_model("claude-sonnet-4-5-20250929", temperature=0.9)

# é•¿æ–‡æœ¬ç”Ÿæˆ â†’ å¢å¤§ max_tokens
long_model = init_chat_model("claude-sonnet-4-5-20250929", max_tokens=4096)
```

### 4. ä½¿ç”¨ content_blocks è€Œé content

`content_blocks` æ˜¯ LangChain 1.0 æ¨èçš„å“åº”è®¿é—®æ–¹å¼ï¼Œè·¨ Provider å…¼å®¹æ€§æ›´å¥½ï¼š

```python
response = model.invoke("ä½ å¥½")

# æ¨èï¼šç»Ÿä¸€æ ¼å¼
for block in response.content_blocks:
    if block["type"] == "text":
        print(block["text"])

# ä¸æ¨èï¼šæ ¼å¼å›  Provider è€Œå¼‚
print(response.content)
```

## å¸¸è§é—®é¢˜

**Q: init_chat_model å’Œç›´æ¥å®ä¾‹åŒ– Provider ç±»æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ**

A: `init_chat_model` æ˜¯å·¥å‚å‡½æ•°ï¼Œæ ¹æ®æ¨¡å‹åç§°åŠ¨æ€é€‰æ‹© Provider ç±»ã€‚å®ƒçš„ä¼˜åŠ¿æ˜¯ä»£ç ä¸ Provider è§£è€¦ï¼Œä¾¿äºåˆ‡æ¢ã€‚ç›´æ¥å®ä¾‹åŒ– Provider ç±»çš„ä¼˜åŠ¿æ˜¯ IDE ç±»å‹æç¤ºæ›´å®Œæ•´ï¼Œå¯ä»¥ä½¿ç”¨ Provider ç‰¹æœ‰çš„å‚æ•°ã€‚ä¸¤è€…è¿”å›çš„å¯¹è±¡æ¥å£å®Œå…¨ä¸€è‡´ã€‚

**Q: å¦‚ä½•å¤„ç†ä¸åŒ Provider çš„ API Keyï¼Ÿ**

A: æ¯ä¸ª Provider æœ‰å›ºå®šçš„ç¯å¢ƒå˜é‡åç§°ï¼š
- Anthropic: `ANTHROPIC_API_KEY`
- OpenAI: `OPENAI_API_KEY`
- Google: `GOOGLE_API_KEY`

æ¨¡å‹åˆå§‹åŒ–æ—¶ä¼šè‡ªåŠ¨è¯»å–å¯¹åº”çš„ç¯å¢ƒå˜é‡ï¼Œä¹Ÿå¯ä»¥é€šè¿‡ `api_key` å‚æ•°æ˜¾å¼ä¼ å…¥ã€‚

**Q: temperature è®¾ä¸º 0 å°±å®Œå…¨ç¡®å®šæ€§äº†å—ï¼Ÿ**

A: å¤§å¤šæ•° Provider åœ¨ `temperature=0` æ—¶è¾“å‡ºæ¥è¿‘ç¡®å®šæ€§ï¼Œä½†ä¸ä¿è¯å®Œå…¨ä¸€è‡´ã€‚å¦‚æœéœ€è¦å¯å¤ç°çš„è¾“å‡ºï¼Œéƒ¨åˆ† Provider æ”¯æŒ `seed` å‚æ•°ã€‚

**Q: æ¨¡å‹ä¸æ”¯æŒæˆ‘ç»‘å®šçš„å·¥å…·æ€ä¹ˆåŠï¼Ÿ**

A: ä¸æ˜¯æ‰€æœ‰æ¨¡å‹éƒ½æ”¯æŒ tool callingã€‚å¦‚æœæ¨¡å‹ä¸æ”¯æŒï¼Œ`bind_tools` ä¼šåœ¨è°ƒç”¨æ—¶æŠ›å‡ºå¼‚å¸¸ã€‚å»ºè®®ä½¿ç”¨æ”¯æŒ function calling çš„æ¨¡å‹ï¼ˆå¦‚ Claude 3.5+ã€GPT-4+ã€Gemini 1.5+ï¼‰ã€‚

**Q: å¤šæ¨¡æ€è¾“å…¥æ”¯æŒå“ªäº›æ ¼å¼ï¼Ÿ**

A: å–å†³äº Providerã€‚Anthropic Claude æ”¯æŒå›¾ç‰‡ï¼ˆJPEGã€PNGã€GIFã€WebPï¼‰å’Œ PDFï¼›OpenAI GPT-4o æ”¯æŒå›¾ç‰‡å’ŒéŸ³é¢‘ï¼›Google Gemini æ”¯æŒå›¾ç‰‡ã€éŸ³é¢‘å’Œè§†é¢‘ã€‚å…·ä½“é™åˆ¶è¯·æŸ¥é˜…å„ Provider çš„å®˜æ–¹æ–‡æ¡£ã€‚

## ä¸‹ä¸€æ­¥

ç°åœ¨ä½ å·²ç»æŒæ¡äº† LangChain çš„æ¨¡å‹æ¥å£ï¼Œæ¥ä¸‹æ¥å¯ä»¥ï¼š

- å­¦ä¹  [æ¶ˆæ¯ï¼ˆMessagesï¼‰](/ai/langchain/guide/messages) - äº†è§£å¦‚ä½•æ„é€ å¯¹è¯æ¶ˆæ¯
- æ¢ç´¢ [å·¥å…·ï¼ˆToolsï¼‰](/ai/langchain/guide/tools) - è®©æ¨¡å‹è°ƒç”¨å¤–éƒ¨å·¥å…·
- æŒæ¡ [ç»“æ„åŒ–è¾“å‡º](/ai/langchain/guide/structured-output) - ä»æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–æ•°æ®
- äº†è§£ [æ¶ˆæ¯ Messages](/ai/langchain/guide/messages) - æ·±å…¥ç»Ÿä¸€å†…å®¹è®¿é—®æ¥å£
- å®è·µ [æµå¼å“åº”](/ai/langchain/guide/streaming) - å®æ—¶æ¥æ”¶æ¨¡å‹è¾“å‡º

## å‚è€ƒèµ„æº

- [LangChain Models å®˜æ–¹æ–‡æ¡£](https://docs.langchain.com/oss/python/langchain/models)
- [init_chat_model API å‚è€ƒ](https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html)
- [LangChain Provider é›†æˆåˆ—è¡¨](https://python.langchain.com/docs/integrations/chat/)
- [LangChain v1.0 è¿ç§»æŒ‡å—](https://docs.langchain.com/oss/python/migrate/langchain-v1)
