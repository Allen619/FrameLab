---
title: æµå¼å“åº” Streaming
description: æŒæ¡ LangChain æµå¼å“åº”çš„ä¸‰ç§æ¨¡å¼ï¼ˆupdates / messages / customï¼‰ï¼Œå­¦ä¹ å¤šæ¨¡å¼ç»„åˆã€å‰ç«¯é›†æˆã€é”™è¯¯å¤„ç†ä¸å–æ¶ˆæœºåˆ¶
---

# æµå¼å“åº” Streaming

> å‰ç½®é˜…è¯»ï¼š[æ¨¡å‹è°ƒç”¨ Models](/ai/langchain/guide/models) Â· [æ™ºèƒ½ä½“ Agent](/ai/langchain/guide/agents)

## ä¸ºä»€ä¹ˆéœ€è¦æµå¼å“åº”

å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†å¾€å¾€éœ€è¦æ•°ç§’ç”šè‡³æ•°åç§’ã€‚éæµå¼è°ƒç”¨è®©ç”¨æˆ·é¢å¯¹ç©ºç™½ç­‰å¾…ï¼Œè€Œæµå¼å“åº”é€šè¿‡**å¢é‡è¿”å›**éƒ¨åˆ†ç»“æœï¼Œä»æ ¹æœ¬ä¸Šæ”¹å–„ä½“éªŒï¼š

| ç»´åº¦ | éæµå¼ | æµå¼ |
|------|--------|------|
| é¦–å­—èŠ‚å»¶è¿Ÿ | ç­‰å¾…å…¨éƒ¨ç”Ÿæˆå®Œæ¯• | é¦–ä¸ª Token å³è¿”å› |
| ç”¨æˆ·æ„ŸçŸ¥ | "ç³»ç»Ÿå¡æ­»äº†ï¼Ÿ" | "AI æ­£åœ¨æ‰“å­—" |
| å·¥å…·åé¦ˆ | æ— ä¸­é—´çŠ¶æ€ | å®æ—¶è¿›åº¦æ›´æ–° |
| èµ„æºåˆ©ç”¨ | å¿…é¡»ç­‰å®Œæˆ | å¯éšæ—¶å–æ¶ˆ |

æµå¼æ˜¾ç¤ºå³ä½¿ä¸å‡å°‘å®é™…å“åº”æ—¶é—´ï¼Œä¹Ÿèƒ½å°†**ç”¨æˆ·æ„ŸçŸ¥å»¶è¿Ÿé™ä½ 50% ä»¥ä¸Š**â€”â€”æˆ‘ä»¬ä¹ æƒ¯çœ‹åˆ°å¯¹æ–¹é€å­—å›ç­”ï¼Œè€Œä¸æ˜¯æ²‰é»˜è®¸ä¹…åçªç„¶å‡ºç°ä¸€å¤§æ®µè¯ã€‚

[ğŸ”— LangChain Streaming æ¦‚å¿µæ–‡æ¡£](https://python.langchain.com/docs/concepts/streaming/){target="_blank" rel="noopener"} Â· [ğŸ”— LangGraph æµå¼ Token æŒ‡å—](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/){target="_blank" rel="noopener"}

::: tip å‰ç«¯ç±»æ¯”
æµå¼å“åº”å¯¹å‰ç«¯å¼€å‘è€…å¹¶ä¸é™Œç”Ÿï¼š

- **SSE (Server-Sent Events)**ï¼šæœåŠ¡å™¨å•å‘æ¨é€æ•°æ®æµï¼ŒLangChain çš„ HTTP æµå¼é€šå¸¸é€šè¿‡ SSE å®ç°
- **WebSocket**ï¼šåŒå‘å®æ—¶é€šä¿¡ï¼Œé€‚åˆå®¢æˆ·ç«¯éœ€è¦ä¸­é€”å‘é€å–æ¶ˆæŒ‡ä»¤çš„åœºæ™¯
- **React Server Components Streaming**ï¼šNext.js RSC é€æ­¥å°† UI ç‰‡æ®µé€è¾¾å®¢æˆ·ç«¯ï¼Œä¸ LLM Token æµå¼è¾“å‡ºç†å¿µä¸€è‡´â€”â€”"å‡†å¤‡å¥½ä¸€éƒ¨åˆ†å°±å…ˆå‘ä¸€éƒ¨åˆ†"
- **ReadableStream**ï¼šWeb Streams API çš„æµå¼åŸè¯­ï¼Œ`for await...of` æ¶ˆè´¹ ReadableStream å’Œ `agent.stream()` ä½“éªŒå®Œå…¨ä¸€æ ·
:::

## æµå¼æ•°æ®æµå…¨æ™¯

```mermaid
sequenceDiagram
    participant C as å®¢æˆ·ç«¯ (å‰ç«¯)
    participant S as æœåŠ¡ç«¯ (FastAPI)
    participant A as LangChain Agent
    participant M as LLM æ¨¡å‹
    participant T as å·¥å…·å‡½æ•°

    C->>S: HTTP è¯·æ±‚ (SSE)
    S->>A: agent.stream(input, stream_mode)
    A->>M: å‘é€ Prompt

    rect rgb(232, 245, 233)
        Note over M,C: messages æ¨¡å¼ â€” Token çº§åˆ«
        loop æ¯ä¸ª Token
            M-->>A: Token chunk
            A-->>S: yield (token, metadata)
            S-->>C: SSE data: {token}
        end
    end

    alt LLM å†³å®šè°ƒç”¨å·¥å…·
        rect rgb(255, 243, 224)
            Note over A,T: updates æ¨¡å¼ â€” æ­¥éª¤çº§åˆ«
            A-->>S: yield {step: "agent", tool_call}
            A->>T: æ‰§è¡Œå·¥å…·
        end
        rect rgb(227, 242, 253)
            Note over T,C: custom æ¨¡å¼ â€” è‡ªå®šä¹‰è¿›åº¦
            T-->>A: writer("è¿›åº¦ 50%")
            A-->>S: yield custom_data
            S-->>C: SSE data: {progress}
        end
        T-->>A: å·¥å…·ç»“æœ
        A->>M: ç»§ç»­ç”Ÿæˆ
        loop æœ€ç»ˆå›å¤
            M-->>A: Token chunk
            A-->>S: yield token
            S-->>C: SSE data: {token}
        end
    end

    A-->>S: æµç»“æŸ
    S-->>C: SSE [DONE]
```

## ä¸‰ç§æµå¼æ¨¡å¼

| stream_mode | ç²’åº¦ | è¿”å›å†…å®¹ | åœºæ™¯ |
|-------------|------|----------|------|
| `"updates"` | æ­¥éª¤çº§ | èŠ‚ç‚¹æ‰§è¡Œåçš„å®Œæ•´çŠ¶æ€ | è¿½è¸ª Agent å†³ç­–ã€è°ƒè¯• |
| `"messages"` | Token çº§ | æ¯ä¸ª Token å¢é‡ | èŠå¤©æ‰“å­—æ•ˆæœ |
| `"custom"` | è‡ªå®šä¹‰ | å·¥å…·å†…å‘å‡ºçš„ä»»æ„æ•°æ® | é•¿ä»»åŠ¡è¿›åº¦æ¡ |

### updates â€” æ­¥éª¤çº§åˆ«æ›´æ–°

æ¯ä¸ªæ‰§è¡Œæ­¥éª¤å®Œæˆåè¿”å›ä¸€æ¬¡ï¼ŒåŒ…å«è¯¥æ­¥éª¤çš„å®Œæ•´è¾“å‡ºï¼š

```python
from langchain.chat_models import init_chat_model
from langgraph.prebuilt import create_react_agent

def get_weather(city: str) -> str:
    """è·å–å¤©æ°”"""
    return f"{city}ï¼šæ™´ï¼Œ25Â°C"

model = init_chat_model("openai:gpt-4o")
agent = create_react_agent(model, tools=[get_weather])

for chunk in agent.stream(
    {"messages": [{"role": "user", "content": "åŒ—äº¬å¤©æ°”å¦‚ä½•ï¼Ÿ"}]},
    stream_mode="updates",
):
    for node_name, node_output in chunk.items():
        last_msg = node_output["messages"][-1]
        if hasattr(last_msg, "tool_calls") and last_msg.tool_calls:
            for tc in last_msg.tool_calls:
                print(f"[{node_name}] è°ƒç”¨å·¥å…·: {tc['name']}({tc['args']})")
        elif hasattr(last_msg, "content"):
            print(f"[{node_name}] {last_msg.content[:80]}")
```

è¾“å‡ºï¼š

```
[agent] è°ƒç”¨å·¥å…·: get_weather({'city': 'åŒ—äº¬'})
[tools] åŒ—äº¬ï¼šæ™´ï¼Œ25Â°C
[agent] åŒ—äº¬ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼æ™´æœ—ï¼Œæ°”æ¸© 25Â°Cï¼Œéå¸¸é€‚åˆå¤–å‡ºã€‚
```

### messages â€” Token çº§åˆ«æµå¼

æœ€ç»†ç²’åº¦â€”â€”æ¯ä¸ª Token ç”Ÿæˆåç«‹å³è¿”å›ï¼Œå®ç°"æ‰“å­—æœºæ•ˆæœ"ï¼š

```python
for event in agent.stream(
    {"messages": [{"role": "user", "content": "æœç´¢æ´»è·ƒç”¨æˆ·"}]},
    stream_mode="messages",
):
    msg_chunk, metadata = event  # å…ƒç»„ï¼š(æ¶ˆæ¯ç‰‡æ®µ, å…ƒæ•°æ®)
    node = metadata.get("langgraph_node", "")

    if node == "agent" and msg_chunk.content:
        print(msg_chunk.content, end="", flush=True)  # é€å­—è¾“å‡º

    if node == "agent" and msg_chunk.tool_call_chunks:
        for tc in msg_chunk.tool_call_chunks:
            if tc.get("name"):
                print(f"\n[è°ƒç”¨: {tc['name']}]")

    if node == "tools" and msg_chunk.content:
        print(f"\n[ç»“æœ: {msg_chunk.content}]")
```

å…³é”®æ•°æ®ç»“æ„ï¼š

```python
msg_chunk.content            # str â€” æ–‡æœ¬ç‰‡æ®µ
msg_chunk.tool_call_chunks   # list â€” å·¥å…·è°ƒç”¨å¢é‡
metadata["langgraph_node"]   # "agent" | "tools"
metadata["langgraph_step"]   # int â€” æ­¥éª¤åºå·
```

### custom â€” è‡ªå®šä¹‰æµå¼æ›´æ–°

å·¥å…·å‡½æ•°å†…éƒ¨é€šè¿‡ `get_stream_writer()` å‘é€ä»»æ„æ•°æ®ï¼Œé€‚åˆé•¿æ—¶é—´æ“ä½œçš„è¿›åº¦åé¦ˆï¼š

```python
import time
from langgraph.config import get_stream_writer

def analyze_dataset(name: str) -> str:
    """åˆ†ææ•°æ®é›†ï¼Œè¿‡ç¨‹ä¸­æŠ¥å‘Šè¿›åº¦"""
    writer = get_stream_writer()

    for pct in [0, 25, 50, 75, 100]:
        writer({"phase": "loading", "progress": pct})
        time.sleep(0.3)

    writer({"phase": "analyzing", "progress": 100, "message": "åˆ†æå®Œæˆ"})
    return f"{name} åˆ†æç»“æœï¼š50,000 æ¡è®°å½•ï¼Œæ—¥æ´» 12,350"

agent = create_react_agent(model, tools=[analyze_dataset])

for chunk in agent.stream(
    {"messages": [{"role": "user", "content": "åˆ†æç”¨æˆ·æ•°æ®"}]},
    stream_mode="custom",
):
    if isinstance(chunk, dict):
        print(f"[{chunk.get('phase')}] {chunk.get('progress', 0)}%")
```

`get_stream_writer()` è¦ç‚¹ï¼šå¿…é¡»åœ¨å·¥å…·å‡½æ•°**å†…éƒ¨**è°ƒç”¨ï¼›å†™å…¥ä»»æ„å¯åºåˆ—åŒ–å¯¹è±¡ï¼›ä»… `stream_mode` å« `"custom"` æ—¶æ‰è¢«æ¶ˆè´¹ã€‚

## å¤šæ¨¡å¼ç»„åˆ

å°† `stream_mode` è®¾ä¸ºåˆ—è¡¨å³å¯åŒæ—¶è·å–å¤šç§ç²’åº¦çš„æ•°æ®ï¼š

```python
for mode, chunk in agent.stream(
    {"messages": [{"role": "user", "content": "ç”Ÿæˆè¿è¥æŠ¥å‘Š"}]},
    stream_mode=["messages", "custom"],
):
    if mode == "messages":
        msg_chunk, metadata = chunk
        if metadata.get("langgraph_node") == "agent" and msg_chunk.content:
            print(msg_chunk.content, end="", flush=True)
    elif mode == "custom":
        print(f"\n[è¿›åº¦] {chunk}")
```

æ¯æ¬¡è¿­ä»£è¿”å› `(mode, chunk)` å…ƒç»„â€”â€”é€šè¿‡ `mode` åŒºåˆ†æ•°æ®æ¥æºï¼Œ`chunk` ç»“æ„å–å†³äºå¯¹åº”æ¨¡å¼ã€‚

::: warning æ³¨æ„
ç»„åˆæ¨¡å¼ä¸‹ä¸åŒæ¨¡å¼çš„æ•°æ®ä¼š**äº¤ç»‡**å‡ºç°ã€‚åŠ¡å¿…é€šè¿‡ `mode` å­—æ®µåŒºåˆ†å¤„ç†ã€‚
:::

## å‰ç«¯é›†æˆæ¨¡å¼

### SSE + FastAPIï¼ˆæ¨èï¼‰

```python
# server.py
import json
from fastapi import FastAPI
from fastapi.responses import StreamingResponse

app = FastAPI()

async def event_generator(query: str):
    for event in agent.stream(
        {"messages": [{"role": "user", "content": query}]},
        stream_mode="messages",
    ):
        msg_chunk, metadata = event
        node = metadata.get("langgraph_node", "")
        if node == "agent" and msg_chunk.content:
            yield f"data: {json.dumps({'type': 'token', 'content': msg_chunk.content}, ensure_ascii=False)}\n\n"
    yield "data: [DONE]\n\n"

@app.get("/api/chat/stream")
async def stream_chat(query: str):
    return StreamingResponse(event_generator(query), media_type="text/event-stream",
        headers={"Cache-Control": "no-cache", "X-Accel-Buffering": "no"})
```

```typescript
// React å‰ç«¯
function useChatStream(query: string) {
  const [text, setText] = useState('')

  const start = useCallback(() => {
    const es = new EventSource(`/api/chat/stream?query=${encodeURIComponent(query)}`)
    es.onmessage = (e) => {
      if (e.data === '[DONE]') return es.close()
      const d = JSON.parse(e.data)
      if (d.type === 'token') setText((p) => p + d.content)
    }
    es.onerror = () => es.close()
    return () => es.close()
  }, [query])

  return { text, start }
}
```

### React useStream Hookï¼ˆ@langchain/sdkï¼‰

LangChain å®˜æ–¹å°è£…ï¼Œè‡ªåŠ¨å¤„ç†è¿æ¥ç®¡ç†å’Œæ¶ˆæ¯çŠ¶æ€ï¼š

```typescript
import { useStream } from '@langchain/sdk/react'

function Chat() {
  const { messages, start, stop, isStreaming } = useStream({
    apiUrl: 'http://localhost:8000',
    assistantId: 'my-agent',
    messagesKey: 'messages',
  })

  return (
    <div>
      {messages.map((m, i) => <div key={i}>{m.content}</div>)}
      {isStreaming && <span>AI æ­£åœ¨è¾“å…¥...</span>}
      <button onClick={stop}>åœæ­¢</button>
    </div>
  )
}
```

ä¼˜åŠ¿ï¼šè‡ªåŠ¨ SSE é‡è¿ã€å†…ç½®çŠ¶æ€ç®¡ç†ã€`stop()` å–æ¶ˆã€ä¸ LangGraph Platform æ— ç¼é›†æˆã€‚

### WebSocket æ–¹æ¡ˆ

éœ€è¦åŒå‘é€šä¿¡ï¼ˆå¦‚å®¢æˆ·ç«¯å®æ—¶å–æ¶ˆï¼‰æ—¶ä½¿ç”¨ï¼š

```python
from fastapi import WebSocket

@app.websocket("/ws/chat")
async def ws_chat(ws: WebSocket):
    await ws.accept()
    try:
        while True:
            data = await ws.receive_json()
            if data.get("type") == "cancel":
                break
            for event in agent.stream(
                {"messages": [{"role": "user", "content": data["content"]}]},
                stream_mode="messages",
            ):
                msg_chunk, meta = event
                if meta.get("langgraph_node") == "agent" and msg_chunk.content:
                    await ws.send_json({"type": "token", "content": msg_chunk.content})
            await ws.send_json({"type": "done"})
    finally:
        await ws.close()
```

## é”™è¯¯å¤„ç†

æµå¼å“åº”ä¸­é”™è¯¯å¯èƒ½åœ¨ä»»æ„ä½ç½®å‘ç”Ÿï¼Œéœ€è¦ä¸“é—¨çš„å¤„ç†ç­–ç•¥ï¼š

```python
import time

def stream_with_retry(agent, query: str, max_retries: int = 3):
    """å¸¦æŒ‡æ•°é€€é¿é‡è¯• + éæµå¼å›é€€"""
    for attempt in range(max_retries):
        try:
            for chunk in agent.stream(
                {"messages": [{"role": "user", "content": query}]},
                stream_mode="messages",
            ):
                yield chunk
            return  # æˆåŠŸå®Œæˆ
        except (ConnectionError, TimeoutError) as e:
            wait = 2 ** attempt
            print(f"ç¬¬ {attempt + 1} æ¬¡å¤±è´¥ï¼Œ{wait}s åé‡è¯•: {e}")
            time.sleep(wait)

    # é‡è¯•è€—å°½ï¼Œå›é€€åˆ°éæµå¼
    result = agent.invoke({"messages": [{"role": "user", "content": query}]})
    yield result
```

SSE åœºæ™¯ä¸‹å°†é”™è¯¯ä¼ æ’­åˆ°å‰ç«¯ï¼š

```python
async def safe_event_generator(query: str):
    try:
        for event in agent.stream(..., stream_mode="messages"):
            msg_chunk, metadata = event
            if metadata.get("langgraph_node") == "agent" and msg_chunk.content:
                yield f"data: {json.dumps({'type': 'token', 'content': msg_chunk.content}, ensure_ascii=False)}\n\n"
    except Exception as e:
        yield f"data: {json.dumps({'type': 'error', 'message': str(e)}, ensure_ascii=False)}\n\n"
    finally:
        yield "data: [DONE]\n\n"
```

## æµå¼å–æ¶ˆ

### Python ç«¯ï¼šbreak å³å–æ¶ˆ

```python
for chunk in agent.stream(..., stream_mode="messages"):
    msg_chunk, metadata = chunk
    if metadata.get("langgraph_node") == "agent" and msg_chunk.content:
        print(msg_chunk.content, end="", flush=True)
    if should_cancel():
        break  # è·³å‡ºå¾ªç¯å³åœæ­¢æ¶ˆè´¹
```

### å‰ç«¯ï¼šAbortController

```typescript
const controller = new AbortController()

// å¯åŠ¨æµå¼
fetch(`/api/chat/stream?query=${query}`, { signal: controller.signal })
  .then(async (res) => {
    const reader = res.body!.getReader()
    while (true) {
      const { done, value } = await reader.read()
      if (done) break
      processChunk(new TextDecoder().decode(value))
    }
  })
  .catch((err) => {
    if (err.name === 'AbortError') console.log('å·²å–æ¶ˆ')
  })

// å–æ¶ˆ
controller.abort()
```

## å¸¸è§é—®é¢˜

**Qï¼šæµå¼å’Œéæµå¼çš„æ€»è€—æ—¶æœ‰åŒºåˆ«å—ï¼Ÿ**

æ²¡æœ‰ã€‚LLM æ¨ç†æ—¶é—´ç›¸åŒï¼Œæµå¼åªæ˜¯æŠŠ"ç­‰å®Œå†è¿”å›"æ”¹ä¸º"ç”Ÿæˆä¸€éƒ¨åˆ†å°±è¿”å›ä¸€éƒ¨åˆ†"ã€‚

**Qï¼šä¸‰ç§æ¨¡å¼æ€ä¹ˆé€‰ï¼Ÿ**

- èŠå¤©æ‰“å­—æ•ˆæœ â†’ `messages`
- æ‰§è¡Œæ­¥éª¤å¯è§†åŒ– â†’ `updates`
- å·¥å…·è¿›åº¦æ¡ â†’ `custom`
- ç»¼åˆéœ€æ±‚ â†’ `["messages", "custom"]`

**Qï¼šå‰ç«¯é€‰ SSE è¿˜æ˜¯ WebSocketï¼Ÿ**

å¤§å¤šæ•°åœºæ™¯é€‰ SSEâ€”â€”å®ç°ç®€å•ã€HTTP å…¼å®¹ã€è‡ªåŠ¨é‡è¿ã€‚åªæœ‰éœ€è¦å®¢æˆ·ç«¯**ä¸»åŠ¨å‘æ•°æ®**ï¼ˆå¦‚å®æ—¶å–æ¶ˆã€è¿½åŠ ä¸Šä¸‹æ–‡ï¼‰æ—¶æ‰ç”¨ WebSocketã€‚

**Qï¼š`get_stream_writer()` å¯ä»¥åœ¨å·¥å…·å¤–ä½¿ç”¨å—ï¼Ÿ**

ä¸å¯ä»¥ã€‚å®ƒä¾èµ– LangGraph è¿è¡Œæ—¶ä¸Šä¸‹æ–‡ï¼Œä»…åœ¨å·¥å…·å‡½æ•°æ‰§è¡ŒæœŸé—´å¯ç”¨ã€‚

## ä¸‹ä¸€æ­¥

- [æ™ºèƒ½ä½“ Agent](/ai/langchain/guide/agents) â€” Agent å¦‚ä½•è°ƒåº¦å·¥å…·ä¸æ¨¡å‹
- [æ¨¡å‹è°ƒç”¨ Models](/ai/langchain/guide/models) â€” ä¸åŒæ¨¡å‹å¯¹æµå¼çš„æ”¯æŒå·®å¼‚
- [ç”Ÿäº§éƒ¨ç½²](/ai/langchain/guide/deployment) â€” æµå¼ Agent çš„ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

## å‚è€ƒèµ„æº

- [LangChain Streaming å®˜æ–¹æ–‡æ¡£](https://python.langchain.com/docs/concepts/streaming/)
- [LangGraph æµå¼é…ç½®](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/)
- [Server-Sent Events MDN æ–‡æ¡£](https://developer.mozilla.org/zh-CN/docs/Web/API/Server-sent_events)
- [useStream React Hook](https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph_sdk_react.useStream.html)
